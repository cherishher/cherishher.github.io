<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Max&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/d02dec40f94389c9e1ed922fa6ad3ed2</icon>
  <subtitle>keep hungry, then you&#39;ll be really hungry</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-15T02:19:20.596Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Max Qi</name>
    <email>490949611@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图说mysql的四种join</title>
    <link href="http://yoursite.com/2018/09/14/%E5%9B%BE%E8%AF%B4mysql%E7%9A%84%E5%9B%9B%E7%A7%8Djoin/"/>
    <id>http://yoursite.com/2018/09/14/图说mysql的四种join/</id>
    <published>2018-09-14T02:58:13.621Z</published>
    <updated>2018-09-15T02:19:20.596Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/img/rightJoin.png" alt="屏幕快照 2018-09-14 上午10.54.59"></p><p><img src="/img/leftJoin.png" alt="屏幕快照 2018-09-14 上午10.55.09"></p><p><img src="/img/innerJoin.png" alt="屏幕快照 2018-09-14 上午10.55.39"></p><p><img src="/img/crossJoin.png" alt="屏幕快照 2018-09-14 上午10.55.46"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/img/rightJoin.png&quot; alt=&quot;屏幕快照 2018-09-14 上午10.54.59&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/leftJoin.png&quot; alt=&quot;屏幕快照 2018-09-14 上午10.55.09&quot;&gt;&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce终极整理</title>
    <link href="http://yoursite.com/2018/09/13/MapReduce%E7%BB%88%E6%9E%81%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/13/MapReduce终极整理/</id>
    <published>2018-09-13T06:33:26.110Z</published>
    <updated>2018-09-15T02:14:03.928Z</updated>
    
    <content type="html"><![CDATA[<p>这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。</p><p>首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。</p><h3 id="使用MapReduce的目的"><a href="#使用MapReduce的目的" class="headerlink" title="使用MapReduce的目的"></a>使用MapReduce的目的</h3><p>用一个工具只有知道他的目的和优点，才能找到最合适的使用场景。简单来说，mapreduce提供了一个自动并行和分布式计算的工具（接口），在大规模集群中性能出众。由于它隐藏了这些分布式系统的细节，所以很多不懂分布式的程序员也可以基于此搭建分布式系统。对于一些web服务器、爬虫、排序算法、机器学习、数据挖掘等都很有用。</p><h3 id="MapReduce的架构和运行"><a href="#MapReduce的架构和运行" class="headerlink" title="MapReduce的架构和运行"></a>MapReduce的架构和运行</h3><p>这是本文的重点，不多说，直接上图。</p><p><img src="/img/mapreduce.png" alt="mapreduce"></p><p>这幅图是直接从论文截下来的，生动的讲述了mapreduce的整个流程：</p><ol><li>将用户input文件主动分割成成16-64MB大小的块，然后在集群中做一些备份（一般备份3份，其中两份在同一个机架上）</li><li>master发挥作用，将选择一些idle的worker给其分配map或者reduce任务。</li><li>这时候worker就会去读那些被分割的文件，将一个key/value对读取到用户定义的map函数中。worker在读取这些文件时，会优先读取本地存着的，如果本地没有，选取离它最近的文件，减少网络传输代价。</li><li>map函数会产生一些中间过程数据，然后周期性的通过buffer将它存在本地磁盘上，然后将地址、文件名等信息通知master，这些中间文件通过hash(key) mod R的方式被分成R份（R是reduce worker的数量），master就会将这些块分配给对应的reducer。</li><li>reducer会通过RPC方式读取到这些中间过程数据，然后进行一个排序（shuffle），这个shuffle会让相同value聚在一起。这个很必要，因为不同key会映射到同一个reduce任务上。</li><li>只有在map全部结束后，reduce才会开始。在reducer中处理这个有序数据时，遇到相同key就会把value放在用户定义的reduce函数中。最终reduce函数会把结果文件输出到这个reduce partition中。</li></ol><p>最终得到的结果其实就是part-r-00000这样形式的不同文件。用户没必要把output文件最后聚在一起，如果需要的话，这个结果还可以作为下一轮mapreduce的输入。</p><h3 id="在有Combiner后过程的改良"><a href="#在有Combiner后过程的改良" class="headerlink" title="在有Combiner后过程的改良"></a>在有Combiner后过程的改良</h3><p>上面的过程是论文中解释的，但现在的程序都用了Combiner，Combiner其实和reduce的代码是一样的，那他是做什么用的呢？</p><p>Combiner其实是针对本地的map后的结果进行pre-reduce（或者叫mini-reduce），例如wordcount在map之后大概是(‘a’:1),(‘b’,1),(‘a’,1),(‘c’,1)这样，combiner将local的这些先做一次reduce，变成(‘a’,2),(‘b’,1),(‘c’,1)，之后再去shuffle和reduce。</p><p>综上mapreduce整个过程分为四步：</p><p>​                              <strong>Map -&gt; Combiner -&gt; Shuffle -&gt; Reduce</strong></p><h3 id="Master节点的任务和结构"><a href="#Master节点的任务和结构" class="headerlink" title="Master节点的任务和结构"></a>Master节点的任务和结构</h3><p>对于每个map任务或者reduce任务，都分成三种状态：idel, in-progress, completed。每个任务的这些状态都存在master节点上。</p><p>Master节点是作为map任务和reduce任务通信的管道的。master要存储并更新M个map任务产生的M <em> R个块(每个map任务产生R个块)的位置信息、文件名等。即，master承担 O(M+R)个scheduling决策，存储 O(M </em> R)个状态信息。</p><p>M：map是的m块数据，R: reduce时的r块数据，W: worker机器的数量；三者关系为：</p><p>​                            <strong>M &gt; R &gt; W</strong></p><h3 id="Worker的容错问题"><a href="#Worker的容错问题" class="headerlink" title="Worker的容错问题"></a>Worker的容错问题</h3><p>首先，master出错的话，没啥好说的，直接告诉用户就成。</p><p>worker一旦出错，master应该要感知到，有两种策略感知：</p><ul><li><p>Pull模型 ：worker通过发送heartbeat给master，master在感知到heartbeat之后在heartbeat response里给worker分配任务。</p></li><li><p>Push模型：master一直ping worker，当一段时间ping不通后说明worker失败了。</p></li></ul><p>这个时候master会将worker的任务分配给其他的worker去执行。对于执行完的worker，状态会重新标定为idle，表示有资格接受任务。失败的worker也会被标定为idle，同样可以接受任务。</p><p>对于在失败的worker上completed的map任务，在其他worker上需要重新执行，因为他们存在本地的中间数据访问不到了。但对于在失败的worker上completed的reduce任务则不需要重新执行，因为他们的结果文件存在了global的文件系统下。</p><p>在执行mapreduce时对于一些<strong>straggler</strong>（落伍者），有两种处理方式：</p><ul><li>Job stealing: 对这个job进行分片，将没完成的部分交给其他的worker完成。</li><li>Speculative execution: master，这时候两个类似竞争的关系，当其中一个结束时，这个task会被标定为completed，同时另一个task将会被放弃。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。&lt;/p&gt;
&lt;p&gt;首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。&lt;/p&gt;
&lt;h3 id=&quot;使用MapRe
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>课程信息整理</title>
    <link href="http://yoursite.com/2018/09/13/%E8%AF%BE%E7%A8%8B%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/13/课程信息整理/</id>
    <published>2018-09-13T05:00:48.157Z</published>
    <updated>2018-09-13T08:19:32.862Z</updated>
    
    <content type="html"><![CDATA[<p>之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。</p><hr><h3 id="CMSC5741-Big-Data-Technology-and-Applications"><a href="#CMSC5741-Big-Data-Technology-and-Applications" class="headerlink" title="CMSC5741 - Big Data Technology and Applications"></a><a href="">CMSC5741 - Big Data Technology and Applications</a></h3><p>textbook: Mining of Massive Dataset（已借）</p><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/lyu/" target="_blank" rel="noopener">Prof. Michael R. Lyu</a></p><p>Tutor: Zeng Jichuan</p><p>exam: <strong>Nov.6 Midterm exam</strong> </p><p>Assessment Scheme and Deadlines:</p><ul><li>20% Assignment</li><li>40% Midterm examination</li><li>40% Project : Proposal, Presentation, Report</li></ul><p>Backgroud Knowledge: Tensorflow, Amazon EC2</p><hr><h3 id="CSCI5570-Large-Scale-Data-Processing-Systems"><a href="#CSCI5570-Large-Scale-Data-Processing-Systems" class="headerlink" title="CSCI5570 - Large Scale Data Processing Systems"></a><a href="http://www.cse.cuhk.edu.hk/~jcheng/5570/" target="_blank" rel="noopener">CSCI5570 - Large Scale Data Processing Systems</a></h3><p>website Account:</p><ul><li><p>Username : csci5570</p></li><li><p>Password : huskydatalab</p></li></ul><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~jcheng" target="_blank" rel="noopener">Prof. James CHENG</a> </p><p>Tutor: Tatiana Jin</p><p>Lecture/Lab: </p><ul><li>Tuesday 13:30 Lecture &amp;&amp; Lab</li><li>Wednesday 14:30 Lecture</li></ul><p>Assessment Criteria:</p><ul><li>30% Survey paper : select one topics (DDL: Dec 10, 2018)</li><li>70% project : deadline: DEC 20</li></ul><hr><h3 id="CMSC5724Data-Mining-and-Knowledge-Discovery"><a href="#CMSC5724Data-Mining-and-Knowledge-Discovery" class="headerlink" title="CMSC5724Data Mining and Knowledge Discovery"></a><a href="http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/18-fall/" target="_blank" rel="noopener">CMSC5724Data Mining and Knowledge Discovery</a></h3><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~taoyf/" target="_blank" rel="noopener">Yufei Tao</a></p><p>Tutor: Shangqi Lu</p><p>Assessment Criteria:</p><ul><li>30% Project</li><li>30% Short Tests (three times in class)</li><li>40% Final (Open-book)</li></ul><hr><h3 id="CMSC-5720-Project-I"><a href="#CMSC-5720-Project-I" class="headerlink" title="CMSC 5720 - Project I"></a>CMSC 5720 - Project I</h3><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~jcheng" target="_blank" rel="noopener">Prof. James CHENG</a> </p><p>Options:</p><ol><li>NN-descent （kn-graph的近似算法）</li><li>search with fa2ss（facebook的相似性检索库）</li><li>multiprobe with tree（基于树的哈希方法）</li><li>LSH for MZPS （lsh）</li><li>数据收集-&gt;存储-&gt;分析 系统</li><li>topic modeling on ps archetective -&gt; （LDA FlexPS-&gt;parameter server拓展）</li><li>调度算法，同步/异步 任务，在不同集群下测试算法，分布式，任务的表现</li><li>矩阵分解 Distributed MF（矩阵分解） on Actor Framework （nomad,lftf acf或者akka -&gt; cpu to gpu to scheduling）</li><li>Clustering-aware query (database query optimizer)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;CMSC5741-Big-Data-Technology-and-Applications&quot;
      
    
    </summary>
    
    
      <category term="class note" scheme="http://yoursite.com/tags/class-note/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining Courses</title>
    <link href="http://yoursite.com/2018/09/06/dataMiningCourse/"/>
    <id>http://yoursite.com/2018/09/06/dataMiningCourse/</id>
    <published>2018-09-06T13:41:25.656Z</published>
    <updated>2018-09-07T06:37:04.228Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><p>The feature of attribute:</p><ul><li>Numeral/ordered</li><li>Nominal(domain a subset or not)</li></ul><p>The result of mining from training data : M(A1,A2,A3…Ad) -&gt; model</p><h3 id="Hunt’s-Algorithm"><a href="#Hunt’s-Algorithm" class="headerlink" title="Hunt’s Algorithm"></a>Hunt’s Algorithm</h3><p>When you should end up with the leaf?</p><ol><li>you got the purity classification</li><li>you got the majority result</li></ol><p>What the attribute you put? How to split this attribute into two parts?</p><p>quatify the quality of the split -&gt; Gini-index (not the only choices)</p><p>Gini-index -&gt; measure the purity of content</p><p>Algo:</p><p>ni : Yes in S, p1 = n1/|S|</p><p>no : No in S, p1 = n0/|S|</p><p>Gini(S) = 1 - p0^2 - p1^2</p><p>Calculate Quality:</p><p>( Gini(S1) <em> S1/(S1 + S2) ) + ( Gini(S2) </em> S2/(S1 + S2) )</p><p>sample:</p><ul><li>representative -&gt; random</li><li>large enough</li></ul><p>each part of rooms has a few points -&gt; overfitting</p><p>DecisionTree Problem:</p><p>U = domain(A1) <em> …</em> domain(A2)</p><p>U * { 0, 1 }</p><p>D is a distribution on U*{ 0 , 1 }</p><p>R is drawn independently fro D</p><p>M(A1,A2,A3…Ad) = {0,1}</p><p>the real error : errD(M) = Pr(e belongto D){M(e) != e.C}</p><p>What you can control : errR(M) = | {e belongto R | M(e) != e.C} |</p><p>errD(M) &lt;= errR(M) + 根号（(b*ln4 + lnR(sigma))/2|R|）-&gt; NP-hard</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Classification&quot;&gt;&lt;a href=&quot;#Classification&quot; class=&quot;headerlink&quot; title=&quot;Classification&quot;&gt;&lt;/a&gt;Classification&lt;/h2&gt;&lt;h3 id=&quot;Decision-Tree&quot;&gt;&lt;a
      
    
    </summary>
    
    
      <category term="class note" scheme="http://yoursite.com/tags/class-note/"/>
    
  </entry>
  
  <entry>
    <title>地道的口语表达积累</title>
    <link href="http://yoursite.com/2018/09/01/%E5%9C%B0%E9%81%93%E7%9A%84%E5%8F%A3%E8%AF%AD%E8%A1%A8%E8%BE%BE/"/>
    <id>http://yoursite.com/2018/09/01/地道的口语表达/</id>
    <published>2018-09-01T07:40:05.605Z</published>
    <updated>2018-09-13T08:08:18.213Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-给别人加油"><a href="#1-给别人加油" class="headerlink" title="1. 给别人加油"></a>1. 给别人加油</h3><p>不要再用fighting啦～</p><p><strong>Go get‘em.</strong></p><p>完整说法：Go get’em tiger.</p><p>追女生：go get her</p><h3 id="2-问别人想不想要"><a href="#2-问别人想不想要" class="headerlink" title="2. 问别人想不想要"></a>2. 问别人想不想要</h3><p>不要再用do you want啦～</p><p><strong>be up for something</strong></p><p>e.g: </p><p>Are you up for going clubing?</p><p>你想去蹦迪吗？</p><p>be up三个意思：</p><ul><li><p>醒着 Are you up?</p></li><li><p>下一个 who’s up next?</p></li><li><p>怎么了 what’s up？</p></li></ul><h3 id="3-和朋友约出去玩"><a href="#3-和朋友约出去玩" class="headerlink" title="3. 和朋友约出去玩"></a>3. 和朋友约出去玩</h3><p>play with不行，一般是小朋友或者人和宠物，不适合成人一起玩</p><p>hang out</p><p>还可以用chill out，自己一个人出去玩</p><p>e.g.</p><p> Do you wanna hang out with us?</p><p>###4. 如何约一场“夜生活”</p><p><strong>night out～(名词)</strong></p><p>e.g. </p><p>It’s been a while since we had a girls’ night out.</p><p>姐妹们我们已经很久没有出去玩啦</p><p>have(need) a night out.</p><p>e.g.</p><p>If you are up for a boys’ night out~</p><p>如果想约不会太晚的夜生活用：evening out</p><p>Night owl 夜猫子</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-给别人加油&quot;&gt;&lt;a href=&quot;#1-给别人加油&quot; class=&quot;headerlink&quot; title=&quot;1. 给别人加油&quot;&gt;&lt;/a&gt;1. 给别人加油&lt;/h3&gt;&lt;p&gt;不要再用fighting啦～&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Go get‘em.&lt;/strong
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>daily life in HK 2</title>
    <link href="http://yoursite.com/2018/08/30/daily%20life%20in%20HK%202/"/>
    <id>http://yoursite.com/2018/08/30/daily life in HK 2/</id>
    <published>2018-08-30T01:45:38.758Z</published>
    <updated>2018-08-30T01:45:39.446Z</updated>
    
    <content type="html"><![CDATA[<p>Sigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can’t enjoy your life. While, I come here not for enjoy the postgraduate career but for open my mind and learn something useful.</p><p>Another feeling deeps me most is if I can’t speak english well, the embarrassment will rise upon my face. In the afternoon, I met with my project teacher about the future work. From what he said, I could speculate that the former students may make him disappointed. As for project, he gave us much freedom to do what we interested in based on our foundation.</p><p>At night, I hung out with Doc.Lin, and it is my first time to meet with old friends after I came to HK. We walked around 中环 and 香港岛, where we smell at the taste of dollars. What a fame of capitalism! Then we returned to 九龙 by ship and ate dinner there. Life of Doc.Lin seemed as relaxed and funny as the time in SAP. It’s admired by everyone, right?</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Sigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can’t enjoy your life. While, I com
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>daily life in HK  1</title>
    <link href="http://yoursite.com/2018/08/27/daily%20life%20in%20HK%20%201/"/>
    <id>http://yoursite.com/2018/08/27/daily life in HK  1/</id>
    <published>2018-08-27T10:44:40.328Z</published>
    <updated>2018-08-27T15:21:07.557Z</updated>
    
    <content type="html"><![CDATA[<p>Finally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog unregularly.</p><p>The journey from SZ to HK is tremendously hard for my. I took all my packages (more than 30kg, wooo~~~) comming in HK custom. At that time, I thought the most difficulty time had been gone though. I was wrong, cause it was just the start of my suffering. The long lines of bus station made me crying, at the same time, I realized that I forgot to take some change. Of course, as a freshman of HK, I have no HK card. So I had to beg all around. After two times of tranfer, I got to Sha Tin station.</p><p>From the introduction of my room, I knew the building I live is not far from the station. While, I made the mistake for twice. Unfortunately, It lies on a unknown mountain, so I was supposed to climb mountain with huge bags.</p><p>Finally, I got in my rooms and met with my new roommate, who is a programmer, too. He is a nice guy and I was looking forward to make more friends here.</p><p>What was worth to mention is we have a free dinner organized by our college, other guys in my table feel embarrassed to pack the left-overs so I took it all back. It solved my two-day meal problems.</p><p>Never be shamed with yourself, keep on  doing what you believe in and you will make it sooner or later!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Finally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog 
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>20180811日记</title>
    <link href="http://yoursite.com/2018/08/11/title%2020180811%E6%97%A5%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/08/11/title 20180811日记/</id>
    <published>2018-08-10T16:43:11.578Z</published>
    <updated>2018-09-04T02:25:56.218Z</updated>
    
    <content type="html"><![CDATA[<p>估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！</p><p>Flag～ X</p><p>Flag~</p><p>Flag~</p><p>Flag~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！&lt;/p&gt;
&lt;p&gt;Flag～ X&lt;/p&gt;
&lt;p&gt;Flag~&lt;/p&gt;
&lt;p&gt;Flag~&lt;
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>mac在nginx下部署php遇到的坑</title>
    <link href="http://yoursite.com/2018/08/09/mac%E5%9C%A8nginx%E4%B8%8B%E9%83%A8%E7%BD%B2php%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>http://yoursite.com/2018/08/09/mac在nginx下部署php遇到的坑/</id>
    <published>2018-08-08T16:43:26.827Z</published>
    <updated>2018-08-10T16:40:03.685Z</updated>
    
    <content type="html"><![CDATA[<p>受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php不太熟悉（之前都用xampp这类软件），所以花了一点时间才搞定。</p><p>首先要明白的是，nginx本身不能处理php，它只是一个web服务器，当前端请求php时，nginx需要把界面发给php解释器处理，然后把结果返回给前端。一般地，nginx是把请求发给fastcgi管理进程处理。如nginx中配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">location ~ \.php$ &#123;</span><br><span class="line">    root           html;</span><br><span class="line">    fastcgi_pass   127.0.0.1:9000;</span><br><span class="line">    fastcgi_index  index.php;</span><br><span class="line">    fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;#这里原来不是$document_root，搞得我很蒙，还好网上查到改好了，不然会报file not found</span><br><span class="line">    include        fastcgi_params;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以要启动一个fastcgi，这里就用到了php-fpm，它是一个php fastcgi管理器，只用于php语言（旧版php的要单独下php-fpm，我用的php-fpm已经集成了这个）。</p><p>这里有很多奇怪的问题。</p><p><strong>第一次运行php-fpm</strong></p><p>failed: 找不到/private/etc/php-fpm.conf文件，</p><p>Solution:但这个目录下有个php-fpm.conf.default的文件，所以cp了正确名字的新文件</p><p><strong>第二次远行php-fpm</strong></p><p>Failed: 找不到/usr/var/log/php-fpm.log </p><p>Solution：根本没有这个目录，到conf文件里改了但是没有效果，没办法我就通过下面的命令执行php-fpm(后面都用这个命令执行)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php-fpm --fpm-config /private/etc/php-fpm.conf  --prefix /usr/local/var</span><br></pre></td></tr></table></figure><p><strong>第三次运行php-fpm</strong></p><p>Failed: No pool defined. at least one pool section must be specified in config file</p><p>Solution：到/etc/php-fpm.d/ 目录下有文件www.conf.default，cp一份名为<a href="http://www.conf的文件" target="_blank" rel="noopener">www.conf的文件</a></p><p><strong>第四次运行php-fpm</strong></p><p>Failed：端口被占用</p><p>Solution：杀掉这个进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i tcp:9000#找到占用9000端口的进程号</span><br><span class="line">kill -9 port#杀！</span><br></pre></td></tr></table></figure><p><strong>第五次运行php-fpm</strong></p><p>成功！</p><p>##补充：</p><p>在nginx上配的时候又有所一点不同，在mac上php-fpm直接listen了9000端口，但在服务器上它listen了php7.0-fpm.sock但socket文件，这种方式可能快一点，所以要在nginx上php的配置那边将</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastcgi_pass 127.0.0.1:9000;</span><br></pre></td></tr></table></figure><p>改成：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastcgi_pass unix:/run/php/php7.0-fpm.sock;</span><br></pre></td></tr></table></figure><p>才能成功运行php</p><h3 id="继续补充"><a href="#继续补充" class="headerlink" title="继续补充"></a>继续补充</h3><p>很有意思的一个东西，要上传27m的一个视频，nginx直接报了413 Request Entity Too Large，是我没设置…</p><p>到nginx的配置（set-enabled/default）里面添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    ...</span><br><span class="line">    client_max_body_size 80m;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重读配置、重启服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx -s reload</span><br><span class="line">service nginx restart</span><br></pre></td></tr></table></figure><p>然后还要去修改php.ini，在其中修改两条配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">upload_max_filesize = 80M</span><br><span class="line">post_max_size = 80M</span><br></pre></td></tr></table></figure><p>然后关掉php-fpm的进程，再重启即可～</p><p>ps：贺老师真的完全不研究的…mp4传不上去只是在系统里没添加这种类型，这种事都要我自己去找…难受 :(</p><p><strong>note：</strong>在ubuntu下现在比较推荐用apt而不是apt-get…so，是时候改变了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php
      
    
    </summary>
    
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
  </entry>
  
  <entry>
    <title>看Husky的一点整理</title>
    <link href="http://yoursite.com/2018/08/08/Husky%E6%96%87%E6%A1%A3%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/08/Husky文档整理/</id>
    <published>2018-08-08T02:34:42.580Z</published>
    <updated>2018-09-04T06:49:23.084Z</updated>
    
    <content type="html"><![CDATA[<p>Username : csci5570</p><p>Password : huskydatalab</p><p>husky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受…）</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> Required</span><br><span class="line">master_host=master#master跑的地方</span><br><span class="line">master_port=10086#master绑定的端口</span><br><span class="line">comm_port=12306#worker绑定的端口</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Worker information</span><br><span class="line">[worker]</span><br><span class="line">info=worker1:4#worker1有4个线程</span><br><span class="line">info=worker2:4#worker2有4个线程</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>如果用了hdfs，配置hdfs路径</span><br><span class="line">hdfs_namenode=master</span><br><span class="line">hdfs_namenode_port=9000</span><br></pre></td></tr></table></figure><p>运行的时候用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./program --conf=/path/to/config.ini</span><br></pre></td></tr></table></figure><p>写入配置。</p><h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><h3 id="Object-List"><a href="#Object-List" class="headerlink" title="Object List"></a>Object List</h3><p>Object List（objList）是husky中最主要的对象，可以把任何对象都存在objlist中，两个objlist通过channel传递消息。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Obj</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> KeyT = <span class="keyword">int</span>;</span><br><span class="line">    KeyT key;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> KeyT&amp; <span class="title">id</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">    Obj() = <span class="keyword">default</span>;</span><br><span class="line">    explicit Obj(const KeyT&amp; k) : key(k) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>创建一个obj只要3步：</p><ol><li>定义一个key的类型（keyT），一般都用int</li><li>写一个id（）函数来返回该对象对应的key</li><li>需要一个默认构造函数，还需要一个能够接收key参数的构造函数</li></ol><p>接下来就可以创建、使用、删除Object List</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建名叫my_objlist的objlist</span></span><br><span class="line"><span class="keyword">auto</span>&amp; objlist = ObjListStore::create_objlist&lt;Obj&gt;(<span class="string">"my_objlist"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将obj传入创建好的objlist中</span></span><br><span class="line"><span class="function">Obj <span class="title">obj</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line">objlist.add_object(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过名字拿到对应的objlist，注意这里的auto关键字，自动判定类型，很舒服！</span></span><br><span class="line"><span class="keyword">auto</span>&amp; objlist2 = ObjListStore::get_objlist&lt;Obj&gt;(<span class="string">"my_objlist"</span>);  </span><br><span class="line"></span><br><span class="line"><span class="comment">//通过名字删除objlist</span></span><br><span class="line">ObjListStore::drop_objlist(<span class="string">"my_objlist"</span>);</span><br></pre></td></tr></table></figure><p>为了让添加在objlist中的obj被其他线程感知并利用，在多线程情况下需要将objlist全局化一下，husky已经封装好该方法</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">globalize(objlist);</span><br></pre></td></tr></table></figure><p>接下来就是<strong>最重要</strong>的一个函数list_execute（），它规定了list里的每个object需要做的事，这个函数是用户自己定义的。它有两个参数：</p><ul><li>第一个是要操作的objlist</li><li>第二个是这个objlist中每个obj要做的事，例如下面函数就是obj在log中打印id，包括之后用channel发送或接收消息也都是在这个函数里</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list_execute(objlist, [](Obj&amp; obj) &#123;</span><br><span class="line">    base::log_msg(<span class="string">"My id is: "</span> + obj.id());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>channel就是object和object互相通信的工具，他们的关系类似于城市和公路。husky中有四种channel：</p><ul><li>Push Channel：最常见的点对点通信</li><li>Push Combined Channel：在push channel基础上增加了合并发给同个obj的</li><li>Broadcast Channel：将一个key-value广播出去，任何地方都可以通过key拿到值</li><li>Migrate Channel：用来migrate对象，将一个对象发送到另一个线程上</li></ul><p>channel的创建、使用和drop（一定要主动销毁）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create PushChannel</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MsgT, <span class="keyword">typename</span> DstObjT&gt; </span><br><span class="line"><span class="keyword">static</span> PushChannel&lt;MsgT, DstObjT&gt;&amp; </span><br><span class="line">create_push_channel(ChannelSource&amp; src_list,</span><br><span class="line">                    ObjList&lt;DstObjT&gt;&amp; dst_list,</span><br><span class="line">                    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name = <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get PushChannel through name</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MsgT, <span class="keyword">typename</span> DstObjT&gt;</span><br><span class="line"><span class="keyword">static</span> PushChannel&lt;MsgT, DstObjT&gt;&amp; </span><br><span class="line">get_push_channel(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name = <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Drop channel through name</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">drop_channel</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name)</span></span>;</span><br></pre></td></tr></table></figure><p>下面通过例子来说明：</p><p>首先，要想创建channel，就要确定发消息的源objlist和目的objlist，当然，参数里的目的objlist必须是全局化的</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个push_channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ch = ChannelStore::create_push_channel&lt;<span class="keyword">int</span>&gt;(src_list, dst_list);</span><br></pre></td></tr></table></figure><p>一般来说，channel是放在list_execute（）函数里用的，要想清楚从哪个obj发，发什么，哪个obj接收（通过key来标注）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//push channel</span></span><br><span class="line"><span class="comment">//发送端代码</span></span><br><span class="line">list_execute(src_list, [&amp;ch](Obj&amp; obj) &#123;</span><br><span class="line">    ch.push(msg, key);  <span class="comment">// send msg to key</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//接收端代码</span></span><br><span class="line">list_execute(dst_list, [&amp;ch](Obj&amp; obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span>&amp; msgs = ch.get(obj); <span class="comment">// The msgs is of type std::vector&lt;MsgT&gt;, MsgT is int in this case</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//broadcast channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ch4 = ChannelStore::create_broadcast_channel&lt;<span class="keyword">int</span>, <span class="built_in">std</span>::<span class="built_in">string</span>&gt;(src_list);</span><br><span class="line">list_execute(src_list, [&amp;ch4](Obj&amp; obj) &#123;</span><br><span class="line">    ch4.broadcast(key, value);  <span class="comment">// broadcast key, value pair</span></span><br><span class="line">&#125;);</span><br><span class="line">list_execute(src_list, [&amp;ch4](Obj&amp; obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span> msg = ch4.get(key);   <span class="comment">// get the broadcasted value through key.</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>###Aggregator</p><p>用来执行一些聚合操作的类，可以用来做求前k大值，统计数量，计算机器学习梯度总数等。他的构造函数需要两个参数（或以上），一个是init值，另外是lambda函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Aggregator&lt;<span class="keyword">int</span>&gt; agg(<span class="number">0</span>, [](<span class="keyword">int</span>&amp; a, <span class="keyword">const</span> <span class="keyword">int</span>&amp; b)&#123; a += b; &#125;);</span><br></pre></td></tr></table></figure><p>这个lambda函数就是aggregate的规则。</p><p>在创建完agregator后，就要使用它了。可以用update函数或者update_any函数（比update可接受参数类型多）来进行aggregator，例如</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agg.update(<span class="number">1</span>);<span class="comment">//aggregator值加1</span></span><br></pre></td></tr></table></figure><p>在聚合完之后，更新的值其实只在本地，为了让这个值在全局响应要用HuskyAggregatorFactory::sync()函数。另一种方式是通过HuskyAggregatorFactory::get_channel()来拿到通道，然后在list_execute中通过这个channel把消息传播出去，这种方法最后也会去调用sync()函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//两种方式</span></span><br><span class="line">AggregatorFactory::sync();</span><br><span class="line"></span><br><span class="line"><span class="comment">// or using aggregator channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ac = AggregatorFactory::get_channel();</span><br><span class="line">list_execute(obj_list, &#123;&#125;, &#123;&amp;ac&#125;, [&amp;](OBJ&amp; obj) &#123; </span><br><span class="line">  ...  <span class="comment">// here we can give updates to some aggregators</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>当全局划这个聚合之后，就可以用get_value()函数得到值了</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> sum = agg.get_value()</span><br></pre></td></tr></table></figure><p>这个值是被全局共享的，所以对他的修改会影响其他executor，并可能有线程安全问题</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Username : csci5570&lt;/p&gt;
&lt;p&gt;Password : huskydatalab&lt;/p&gt;
&lt;p&gt;husky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受…）&lt;/p&gt;
&lt;h2 id=&quot;配置&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="CUHK" scheme="http://yoursite.com/tags/CUHK/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop了解一下</title>
    <link href="http://yoursite.com/2018/08/05/hadoop%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B/"/>
    <id>http://yoursite.com/2018/08/05/hadoop了解一下/</id>
    <published>2018-08-05T14:20:47.264Z</published>
    <updated>2018-09-15T02:15:36.125Z</updated>
    
    <content type="html"><![CDATA[<p>搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。</p><h3 id="map-reduce"><a href="#map-reduce" class="headerlink" title="map reduce"></a>map reduce</h3><ul><li>automatic parallezation</li><li>Fault tolerance</li><li>a clean abstraction for programmers</li></ul><p>BSP model : Bulk sychronous parallel</p><p>identity reducer？re</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>###Apache Hadoop与HDFS</p><p>Hadoop是一个大的生态系统，最主要是hdfs和一个基于mapreduce的分布式计算引擎。hdfs就是一个文件系统，在mapreduce的时候（包括用spark的时候）都需要对应文件在hdfs里。</p><p><strong>block块：</strong>HDFS在物理上是以block存储的，block大小可以通过配置参数（dfs.blocksize）来规定，默认128M，可以减少寻址开销。大文件会被切分成很多block来存，而小文件存储则不会占用整个块的空间。</p><p><strong>NameNode：</strong>是master。负责管理文件系统的namespace（可以理解是指向具体数据的文件名、路径名这种）和客户端对文件的访问。data path？</p><p><strong>(非Yarn)JobTracker：</strong>在NameNode上，协调在集群运行的所有作业，分配要在tasktracker上运行的map和reduce任务。</p><p><strong>DataNode：</strong>是slave。datanode则负责数据的存储。</p><p><strong>(非Yarn)TaskTracker：</strong>在datanode上，运行分配的任务并定期向jobtracker报告进度。</p><p><strong>流式访问：</strong>指hdfs访问时像流水一样一点一点过。这样也决定了hdfs是一次写入、多次读取的特性，同时只能有一个wirhter。这样访问方式适合做数据分析，而不是网盘这种。</p><p><strong>rack-aware（机架感知）：</strong>这是hdfs的复制策略。hdfs为了数据可靠一般会将数据复制几份（默认三份）。同一个机架的机器传输速度快，不需要通过交换机。为了提高效率，一台机器的数据会把一个备份放在同一机架（相同rack id）的机器里，另一个备份放在其他机架的机器上。机架的错误率很小，所以不影响可靠性。</p><p><strong>hdfs的特点：</strong></p><ul><li>面对构成系统的组件数目很大，所以对硬件的快速检测错误并自动回复非常重要</li><li>hdfs需要流式访问他们的数据集</li><li>运行的数据集非常大，一个典型文件大小一般在几G到几T</li><li>文件访问模型是“一次写入、多次访问”</li><li>将计算移动到数据附近闭将数据移动到计算更好</li></ul><p>###Yarn：</p><p>yarn其实是解决了经典mapreduce中一些问题（例如：jobtracker太累导致的可扩展性问题）的新一代hadoop计算平台。</p><p><strong>ResourceManager：</strong>代替jobtracker，以后台进程的形式运行。追踪有哪些可用的活动节点和资源，指出哪些程序应该何时或者这些资源。</p><p><strong>ApplicationMaster：</strong>代替一个专用而短暂的JobTracker。用户提交一个应用程序时，会启动applicationmaster这个轻量级进程实例来协调程序内任务（监视进度、定时向resourcemanager发送心跳数据、负责容错等），计算数据需要的资源并向resourcemanager申请。它本身也是在一个container里运行的，且可能与它管理的任务运行在同一节点上。</p><p><strong>Container：</strong>是yarn中资源的抽象，封装了某个节点上一定量的资源（如cpu和内存等资源）。它的分配是由applicationmaster向resourcemanager申请的；而它的运行则是applicationmaster向资源所在的nodemanager发起的。</p><p><strong>NodeManager：</strong>代替tasktracker。拥有很多动态创建的资源Container。容器大小取决于它所包含资源量，而一个节点上的容器数量由配置参数和除用于后台进程和操作系统以外资源总量决定。</p><h2 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h2><p>Hdfs采用了master/slave架构。一个hdfs集群由一个namenode和一群datanodes组成。简单来说，就是hdfs通过namenode暴露出了文件系统命名空间的操作，包括打卡、关闭、重命名文件等等。在这个文件系统对一块数据的操作会映射到具体的datanodes上。</p><p>所以一般是一台机器上搭namenode，然后datanode在其他各个机器上。</p><p><img src="/img/hdfsarchitecture.jpg" alt="hdfsarchitecture"></p><h3 id="Hadoop-Streaming"><a href="#Hadoop-Streaming" class="headerlink" title="Hadoop Streaming"></a>Hadoop Streaming</h3><p>java这么规范的东西有人就是不喜欢，非得用python啥的写hadoop，所以就有了hadoop streaming，支持其他语言的hadoop操作。</p><p>##基本操作</p><p>###单节点测试</p><p>比较无聊，只是测一下能不能跑，不需要运行什么</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> Cellar/hadoop/3.1.0/libexec</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir input<span class="comment">#不能是别的名字</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar  grep input output <span class="string">'dfs[a-z.]+'</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br></pre></td></tr></table></figure><p>###伪分布式测试（pseudo-distributed）</p><ol><li><p>保证本机已经装好hadoop，java1.8（java9有些函数被废了会报错）</p></li><li><p>配置本机ssh，确保</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost#mac默认不允许，需要手动去打开</span><br></pre></td></tr></table></figure><p>可以运行。注意：mac默认不允许任何机器远程登录，需要到  系统偏好设置 -&gt; 共享 去勾选远程登录。</p></li><li><p>配置HDFS，包括core-site.xml文件和hdfs-site.xml文件。前者配置用于存储HDFS的临时文件目录和hdsf访问端口，后者确定复制份数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop/libexec/tmp/hadoop- $&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--如果无此目录则去mkdir一个--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>格式化HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>成功的话在tmp目录下可以看到dfs文件</p></li><li><p>启动各个节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start NameNode#启动namenode</span><br><span class="line">hdfs --daemon start DataNode#启动datanode</span><br><span class="line">hdfs --daemon start SecondaryNameNode#它是namenode的快照，保证了namenode的更新</span><br><span class="line"><span class="meta">jps#</span><span class="bash">用来查看这些节点是否真的启动了</span></span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建文件夹及文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /demo#在hdfs上创建demo文件夹</span><br><span class="line">hdfs dfs -ls /demo</span><br><span class="line">hdfs dfs -put test.input /demo#将本地的test.input文件发到hdfs上</span><br></pre></td></tr></table></figure></li><li><p>配置Yarn的mapred-site.xml和yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-sited.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>share/hadoop/mapreduce/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果不加这个property，在后面运行mapreduce任务时会报找不到包 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动resourcemanager和nodemanager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start nodemanager</span><br><span class="line">yarn --daemon start resourcemanager</span><br></pre></td></tr></table></figure><p>yarn端口是8088，可以去localhost:8088看页面</p></li><li><p>运行mapreduce任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar hadoop-mapreduce-examples-3.1.0.jar wordcount /demo/test.input /demo-output/</span><br></pre></td></tr></table></figure><p>可以到对应文件里查看运行结果</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。&lt;/p&gt;
&lt;h3 id=&quot;map-reduce&quot;&gt;&lt;a href=&quot;#map-reduce&quot; class=&quot;headerlink&quot; title=&quot;map reduce&quot;&gt;&lt;/a&gt;map r
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>Spark使用整理</title>
    <link href="http://yoursite.com/2018/08/05/spark%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/05/spark使用整理/</id>
    <published>2018-08-05T13:38:09.544Z</published>
    <updated>2018-09-13T08:10:32.368Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I read the book &lt;\<spark in="" action="">&gt;, the summation about this book will be wrote down here.</spark></p><h3 id="the-notion-of-RDD-resilient-distributed-dataset"><a href="#the-notion-of-RDD-resilient-distributed-dataset" class="headerlink" title="the notion of RDD(resilient distributed dataset)"></a>the notion of RDD(resilient distributed dataset)</h3><p>The RDD is the fundamental abstraction in spark it represents a collection of elements that is</p><ul><li>Immutable ( read-only )</li><li>Resilient ( fault-tolerant )</li><li>Distributed</li></ul><p>Immutable : allow xspark to provide important fault-tolerance guarantees in a straightforward manner.</p><p>Distributed : machines are transparents to users, so working with RDDs is not much differents from working with a lists, maps and so on.</p><p>Resilient: whereas other systems facilitate fault-tolerance by replicating data to multiple machines, RDDs provide falut-tolerant by logging the transformations used to build dataset rather than itself. When fault happens, it just need to repair a subset of dataset.</p><h3 id="Basic-RDD-Operation"><a href="#Basic-RDD-Operation" class="headerlink" title="Basic RDD Operation"></a>Basic RDD Operation</h3><p>there are two types of operations</p><ul><li>Transformations</li><li>actions</li></ul><p>Transformations : like <em>filter</em> and <em>map</em>, perform some useful data manipulation and it will produce a new RDD</p><p>actions : like <em>count</em> and <em>foreach</em>, trigger a computations to return a result.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Recently I read the book &amp;lt;\&lt;spark in=&quot;&quot; action=&quot;&quot;&gt;&amp;gt;, the summation about this book will be wrote down here.&lt;/spark&gt;&lt;/p&gt;
&lt;h3 id=&quot;the
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>homebrew使用整理</title>
    <link href="http://yoursite.com/2018/08/02/homebrew%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/02/homebrew使用整理/</id>
    <published>2018-08-02T04:05:26.949Z</published>
    <updated>2018-09-13T08:07:57.002Z</updated>
    
    <content type="html"><![CDATA[<p>现在新的mac基本都内置homebrew了吧，brew可以说是mac神器之一了。上手简单，但还是用法需要整理一下：</p><p>###brew常用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">brew search 包名 #搜索包</span><br><span class="line">brew info 包名#包信息</span><br><span class="line">brew list #查看有哪些包</span><br><span class="line">brew install 包名#安装包</span><br><span class="line">brew uninstall 包名#删除包</span><br></pre></td></tr></table></figure><h3 id="brew管理服务"><a href="#brew管理服务" class="headerlink" title="brew管理服务"></a>brew管理服务</h3><p>brew还有个重要的任务就是管理服务，在我本机的：</p><ul><li>Kafka</li><li>mysql</li><li>nginx</li><li>Redis</li><li>zookeeper</li></ul><p>都是用了brew进行管理，管理他们用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew services start 服务名#开启一个service</span><br><span class="line">brew services stop 服务名#关闭一个service</span><br></pre></td></tr></table></figure><p>每次开启一个服务，就会在～/Library/LaunchAgents里面增加一个plist文件，用来存储这个服务的一些版本信息，同时，本机所有其他服务可以通过</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">launchctl load *.plist #加载 </span><br><span class="line">launchctl unload *.plist #取消</span><br><span class="line">launchctl list#查看服务</span><br></pre></td></tr></table></figure><p>来完成</p><h3 id="brew其他命令"><a href="#brew其他命令" class="headerlink" title="brew其他命令"></a>brew其他命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew link 包名</span><br></pre></td></tr></table></figure><p>这里的link是指symbollink（有点类似于windows里的创建快捷方式）。以hadoop为例，在brew刚下载的hadoop只是存在/usr/local/Cellar目录下的，在全局环境下不能用hadoop命令。只有将其link到bin里（hadoop产生了27个symbolink），才能全局使用hadoop命令。在用brew install时会默认完成link的操作，除非出现意外。</p><p>意外：在安装hadoop时出现了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Error: The `brew link` step did not complete successfully</span><br><span class="line">The formula built, but is not symlinked into /usr/local</span><br><span class="line">Could not symlink sbin/FederationStateStore</span><br><span class="line">/usr/local/sbin is not writable.</span><br></pre></td></tr></table></figure><p>是因为我本机根本没有这个目录，同时权限也不够，所以我建了这个目录，然后用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -r $(whoami) $(brew --prefix)/*</span><br></pre></td></tr></table></figure><p>修改了对应权限，成功安装。这里引出了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew --prefix</span><br></pre></td></tr></table></figure><p>这个是指brew存在的目录，其他brew操作都是在这个目录下搞的（例如cellar就是在这个目录下）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现在新的mac基本都内置homebrew了吧，brew可以说是mac神器之一了。上手简单，但还是用法需要整理一下：&lt;/p&gt;
&lt;p&gt;###brew常用命令&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gu
      
    
    </summary>
    
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
  </entry>
  
  <entry>
    <title>hip-hop中一些slang积累</title>
    <link href="http://yoursite.com/2018/07/27/hip-hop%E4%B8%AD%E4%B8%80%E4%BA%9Bslang%E7%A7%AF%E7%B4%AF/"/>
    <id>http://yoursite.com/2018/07/27/hip-hop中一些slang积累/</id>
    <published>2018-07-27T15:25:18.747Z</published>
    <updated>2018-09-15T02:21:21.625Z</updated>
    
    <content type="html"><![CDATA[<p>听了很久trap，很多都不太懂，之后不懂的就写在这里吧～</p><p>###In my feelings - Drake （KIKI在b榜第一呆了一个月，我满耳朵都是kiki）</p><p>Henny -&gt; Hennessy：轩尼诗（酒）</p><p>wraith：幽灵、幻影（劳斯莱斯品牌）</p><p>code to the safe：保险箱密码</p><p>Neck work：类似的表达有give me some neck，指吹喇叭，等同于blow job或者sucking或者blowing of one’s dick~</p><p>Netflix and chill：一个internet meme，指约pao</p><p>net worth；资产净值（身价）</p><h3 id="Alright-Kendrick-Lamar"><a href="#Alright-Kendrick-Lamar" class="headerlink" title="Alright - Kendrick Lamar"></a>Alright - Kendrick Lamar</h3><p>Mac-11：机械手枪的型号</p><p>Pussy&amp;Benjamin：指代女人和金钱</p><p>Chevy -&gt; chevrolet 雪弗兰</p><p>Get reaping everything I sow：收获我所播种的（种豆得豆）</p><p>My karma：我的命运</p><p>Preliminary hearing：法庭的初审</p><p>fight my vice：和恶习斗争（vice除了副的还有恶习的意思）</p><p>Popo：police，条子</p><p>Preacher：牧师，传道人</p><p>Regal：君主的，这里指别克君威车</p><p>Resentment：愤恨，不满</p><p>Self destruct：自我毁灭</p><p>Lucy -&gt; Lucifer：是指撒旦，Satan, the devil（貌似只有lamar称lucifer为lucy～）</p><h3 id="Bed-Nicki-Minaj-Ariana-Grande-（沉迷黄歌，无法自拔～）"><a href="#Bed-Nicki-Minaj-Ariana-Grande-（沉迷黄歌，无法自拔～）" class="headerlink" title="Bed - Nicki Minaj/Ariana Grande （沉迷黄歌，无法自拔～）"></a>Bed - Nicki Minaj/Ariana Grande （沉迷黄歌，无法自拔～）</h3><p>wit’(with) your name on it：属于某人（不是真的指文字那种）</p><p>Sheet：床单</p><p>Carter III：指Lil Wayne备受赞誉的专辑</p><p>A Milli：Carter III专辑中的一首歌</p><p>GOAT：greatest of all time 史上最佳</p><p>turn down：拒绝（don’t turn me down不要拒绝我）</p><p>Lingerie：内衣如图，不解释</p><p><img src="/img/lingerie.png" alt="img"></p><p>blow it like a feather on you：像一片羽毛xx你（撩的不行啊～）</p><p>Starting five：指nba那种首发五人，也可以用 Starting Line-up（首发阵容）</p><p>Thick skin：脸皮厚，不怕被骂～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;听了很久trap，很多都不太懂，之后不懂的就写在这里吧～&lt;/p&gt;
&lt;p&gt;###In my feelings - Drake （KIKI在b榜第一呆了一个月，我满耳朵都是kiki）&lt;/p&gt;
&lt;p&gt;Henny -&amp;gt; Hennessy：轩尼诗（酒）&lt;/p&gt;
&lt;p&gt;wrait
      
    
    </summary>
    
    
      <category term="hiphop" scheme="http://yoursite.com/tags/hiphop/"/>
    
  </entry>
  
  <entry>
    <title>pandas使用整理</title>
    <link href="http://yoursite.com/2018/07/24/pandas%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/24/pandas使用整理/</id>
    <published>2018-07-24T14:55:02.595Z</published>
    <updated>2018-09-13T08:09:25.940Z</updated>
    
    <content type="html"><![CDATA[<p>之前看了numpy，这两天看了pandas，也在这里整理一下。</p><h3 id="新建"><a href="#新建" class="headerlink" title="新建"></a>新建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'key1'</span>:[<span class="string">'value11,value12'</span>],</span><br><span class="line">    <span class="string">'key2'</span>:[<span class="string">'value21'</span>,<span class="string">'value22'</span>]   </span><br><span class="line">       &#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line">df.index<span class="comment">#描述dataframe的index的【开始/结束）和步长，注意这个是指df的index而非数据的id</span></span><br><span class="line">df.columns<span class="comment">#行名</span></span><br><span class="line">df.dtypes<span class="comment">#类型</span></span><br><span class="line">df.size<span class="comment">#数据总数</span></span><br><span class="line">df.shape<span class="comment">#数据形式（行，列）</span></span><br><span class="line">df.ndim<span class="comment">#维度</span></span><br><span class="line">df.T<span class="comment">#转置</span></span><br></pre></td></tr></table></figure><h3 id="从数据库中得到数据-amp-amp-将数据写入数据库"><a href="#从数据库中得到数据-amp-amp-将数据写入数据库" class="headerlink" title="从数据库中得到数据 &amp;&amp; 将数据写入数据库"></a>从数据库中得到数据 &amp;&amp; 将数据写入数据库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接mysql数据库</span></span><br><span class="line">engine = create_engine(<span class="string">'mysql+pymysql://root:qh129512@127.0.0.1:3306/testdb?charset=utf8'</span>)</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'select * from person'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面有三种获取数据的方式，返回的formlist就是一个dataframe</span></span><br><span class="line">formlist = pd.read_sql_query(sql,con=engine)<span class="comment">#参数为sql语句</span></span><br><span class="line">formlist = pd.read_sql_table(table,con=engine)<span class="comment">#参数为表名</span></span><br><span class="line">formlist = pd.read_sql(sql,con=engine)<span class="comment">#参数可以是sql语句，也可以表名</span></span><br></pre></td></tr></table></figure><p>下面是将数据写入数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(data,columns=[<span class="string">'name,birthday'</span>])<span class="comment">#这里的columns就是指dataframe里有哪些comlumn，这里没有的df里也不会有，如果不添加columns这个参数就默认data中全部数据</span></span><br><span class="line"></span><br><span class="line">df.to_sql(name=<span class="string">'person'</span>,con=engine, if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment">#if_exists:有三种fail-&gt;表存在就不写入；replace-&gt;表存在就删掉原来的表重新创建；append-&gt;在原表基础上追加数据。默认为fail</span></span><br><span class="line"><span class="comment">#index：决定是否将行索引作为数据传入数据库，注意：如果数据库没有专门来存这个的就false，因为表里没有这里用true会有问题</span></span><br></pre></td></tr></table></figure><h3 id="使用dataframe"><a href="#使用dataframe" class="headerlink" title="使用dataframe"></a>使用dataframe</h3><p>直接取某个、某些数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>][<span class="number">0</span>]</span><br><span class="line">df.name[:<span class="number">5</span>]</span><br><span class="line">df[[<span class="string">'name'</span>,<span class="string">'birthday'</span>]][:<span class="number">2</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>取数据的切片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.loc[:<span class="number">5</span>,<span class="string">'name'</span>]<span class="comment">#前一个参数为行索引，后一个是列索引名称</span></span><br><span class="line">df.iloc[<span class="number">0</span>:<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#有条件的切片</span></span><br><span class="line">df.loc[(df[<span class="string">'name'</span>] == <span class="string">'max'</span>),:]<span class="comment">#取name为max的切片</span></span><br><span class="line"></span><br><span class="line">df.iloc[(df[<span class="string">'name'</span>] == <span class="string">'max'</span>).values,:]</span><br></pre></td></tr></table></figure><p>这里有两个函数，loc和iloc，区别有</p><ul><li>loc第一个参数可以为series，例如我传入一个条件，其实相当于一个Series([True,True,False…])这种形式；iloc不可以穿series，但能传一个array，所以可以通过.values的形式传入条件</li><li>行索引时loc是前后闭区间，而iloc是前闭后开区间（python中这种更常见）</li></ul><p>删除数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.drop(labels=rang(<span class="number">9</span>,<span class="number">10</span>),axis=<span class="number">0</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">labels接收string、array，表示删除行/列的标签</span></span><br><span class="line"><span class="string">axis接收0、1，表示操作轴向，0为横，1为纵</span></span><br><span class="line"><span class="string">inplace接收boolean，代表操作是否对原数据生效</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>对dataframe中数据修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接声明就可以添加一列，如</span></span><br><span class="line">df[<span class="string">'prefix_name'</span>] = <span class="string">'MAC_'</span> + df[<span class="string">'name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改某个数据可以将其找出来然后直接赋值</span></span><br><span class="line">df.loc[(df.name == <span class="string">'max'</span>),<span class="string">'name'</span>] = <span class="string">'maxhh'</span></span><br></pre></td></tr></table></figure><p>随机数的使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">series = pd.Series(np.random.randint(high=<span class="number">10000</span>,low=<span class="number">1000</span>,size=<span class="number">8</span>))<span class="comment">#产生一个随机series</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df.loc[:,<span class="string">'net-worth'</span>] = series<span class="comment">#将series值付给df</span></span><br></pre></td></tr></table></figure><p>dataframe算数统计</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'net-worth'</span>].mean()<span class="comment">#平均值</span></span><br><span class="line">np.mean(df[<span class="string">'net-worth'</span>])<span class="comment">#另一种计算平均值的方法</span></span><br><span class="line">df[<span class="string">'net-worth'</span>].min()<span class="comment">#最小值</span></span><br><span class="line">df[<span class="string">'net-worth'</span>].describe()<span class="comment">#描述，包含很多数据可以用！</span></span><br><span class="line">nullNum = df.shape[<span class="number">0</span>] - df[<span class="string">'name'</span>].count()<span class="comment">#统计有多少空值</span></span><br></pre></td></tr></table></figure><h3 id="Category类型的使用"><a href="#Category类型的使用" class="headerlink" title="Category类型的使用"></a>Category类型的使用</h3><p>可以将某一列转化成category类型，这样相当于做了一次分类处理，这样得到的描述性信息会很多</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>] = df[<span class="string">'name'</span>].astype(<span class="string">'category'</span>)<span class="comment">#注意这里是赋值而不是调用</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">'name'</span>].describe()</span><br></pre></td></tr></table></figure><h3 id="时间类型"><a href="#时间类型" class="headerlink" title="时间类型"></a>时间类型</h3><p>pandas里有很多时间类型，不同类型用处不同。如timestamp主要用来记录时间，而timedelta用来做时间运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#有很多方法创建或转化出一个timestamp</span></span><br><span class="line">today_date = pd.to_datetime(<span class="string">'2018-8/5'</span>)<span class="comment">#随意的一个string都可以识别</span></span><br></pre></td></tr></table></figure><p>注意一个概念，从数据库一个datatime拿出来的时间如果调用dtype的话发现是(‘&lt;M8[ns]’)类似的类型，展开来说：</p><p>这个是属于机器的比较特别的类型:<br>小端机器的类型：datatime[ns] == &lt;M8[ns]<br>大端机器的类型：datatime[ns] == &gt;M8[ns]<br>这个可以通过</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.dtype(<span class="string">'datetime64[ns]'</span>) == np.dtype(<span class="string">'&lt;M8[ns]'</span>)</span><br><span class="line"><span class="comment">#out: True</span></span><br></pre></td></tr></table></figure><p>证明。<br>所以当数据库中取出就是这种类型时，不需要再进行处理，但如果取出是个object，则用pd.to_datatime处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'birthday'</span>] = pd.to_datatime(df[<span class="string">'birthday'</span>])</span><br></pre></td></tr></table></figure><p>这里要熟悉的操作有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user_birthday = [i.year <span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">'birthday'</span>]]<span class="comment">#返回所有年份的list</span></span><br><span class="line"></span><br><span class="line">birthday = df[df[<span class="string">'birthday'</span>]&lt;=pd.datetime(<span class="number">1991</span>,<span class="number">1</span>,<span class="number">1</span>)]<span class="comment">#按条件取时间时，一定要那拿timestamp与对应的pdf.datetime()相比</span></span><br></pre></td></tr></table></figure><p>还有时间做加减法也是支持的,timestamp可以加一个tmiedelta来做时间的计算。注意timedelta的参数为weeks,days,hours以及更小的时间<br>直接用+，-符号就可以做计算了。相反的，想算两个时间差，只要用两个datetime做减法，即可得到timedelta类型的时间差距</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">aweek_after = date + pd.Timedelta(weeks = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">today_date = pd.to_datetime(<span class="string">'2018-8-5'</span>)</span><br><span class="line"></span><br><span class="line">time_delta = aweek_after - today_date<span class="comment">#两个timestamp做减法，得到了一个timedelta类型的时间差</span></span><br></pre></td></tr></table></figure><h3 id="分组和聚合"><a href="#分组和聚合" class="headerlink" title="分组和聚合"></a>分组和聚合</h3><p>分组顾名思义，就是将数据按照一定条件进行分组，得到数据整体情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_group = df.groupby(by=<span class="string">'birthday'</span>)</span><br><span class="line">data_group.count()</span><br></pre></td></tr></table></figure><p>这里的count()得到如下表结果</p><table><thead><tr><th></th><th>id</th><th>name</th><th>net-worth</th></tr></thead><tbody><tr><td>birthday</td><td></td><td></td><td></td></tr><tr><td>1987-01-10</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1989-03-10</td><td>6</td><td>0</td><td>4</td></tr><tr><td>1993-08-05</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1995-12-12</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1996-10-12</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>分组的直接结果并不能直接看，因为它返回的只是一个地址，但可以方便的查分组后的一些属性，如：count，head，max等等</p><p>聚合，就是将一组数据做aggretate，聚合的函数自己定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'net-worth'</span>].agg([np.sum,np.mean])<span class="comment">#针对net-worth数据计算两种agg，分别以sum和mean</span></span><br><span class="line"></span><br><span class="line">df.agg(&#123;<span class="string">'net-worth'</span>:[np.sum,sp.mean]&#125;)<span class="comment">#针对不同数据要进行不同的agg，可以用key-value的方式</span></span><br></pre></td></tr></table></figure><p>agg函数的参数是计算函数，这个函数可以用numpy中一些简单的统计函数，如果复杂也可以自定义，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Trinum</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.sum()*<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">df[<span class="string">'net-worth'</span>].agg(Trinum)<span class="comment">#agg函数的参数可以是函数，该函数将data传入做处理，然后返回即可</span></span><br><span class="line"><span class="comment">#稍微注意下sum后面的括号，没有括号是函数，有括号的是执行，但必须声明参数</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">'net-worth'</span>].agg(<span class="keyword">lambda</span> x:x*<span class="number">2</span>)<span class="comment">#agg的参数可以是lambda函数</span></span><br></pre></td></tr></table></figure><p>这里的一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupby(by=<span class="string">'birthday'</span>).agg(np.mean)<span class="comment">#完全等价于data_group.mean()，都是求每组的平均值</span></span><br></pre></td></tr></table></figure><p>当然，apply函数、transform函数和agg函数也大部分相同，但apply方法不能用key-value类型来特定的处理，transform方法只有一个参数function</p><h3 id="创建透视表"><a href="#创建透视表" class="headerlink" title="创建透视表"></a>创建透视表</h3><p>可以通过pandas创建透视表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.pivot_table(df[[<span class="string">'id'</span>,<span class="string">'name'</span>,<span class="string">'net-worth'</span>]],index=[<span class="string">'id'</span>],columns=<span class="string">'name'</span>,fill_value=<span class="number">0</span>)<span class="comment">#不显示无index的值</span></span><br></pre></td></tr></table></figure><p>得到</p><table><thead><tr><th></th><th>net-worth</th><th></th><th></th><th></th></tr></thead><tbody><tr><td>name</td><td>hape</td><td>john</td><td>johnny</td><td>max</td></tr><tr><td>id</td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>6355</td></tr><tr><td>2</td><td>0</td><td>6567</td><td>0</td><td>0</td></tr><tr><td>4</td><td>0</td><td>0</td><td>8491</td><td>0</td></tr><tr><td>8</td><td>6872</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><p>但这不是pandas的重点，略～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前看了numpy，这两天看了pandas，也在这里整理一下。&lt;/p&gt;
&lt;h3 id=&quot;新建&quot;&gt;&lt;a href=&quot;#新建&quot; class=&quot;headerlink&quot; title=&quot;新建&quot;&gt;&lt;/a&gt;新建&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="data science" scheme="http://yoursite.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>看了一点zookeeper的心得</title>
    <link href="http://yoursite.com/2018/07/20/zookeeper%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/20/zookeeper整理/</id>
    <published>2018-07-19T16:10:42.509Z</published>
    <updated>2018-09-15T02:20:32.623Z</updated>
    
    <content type="html"><![CDATA[<p>###Fast Paxos算法：</p><p>这个还没看，先留个坑把。</p><p>###文件结构：</p><p>zookeeper的文件结构大概是这个样子的：</p><p>!å¾ 1 Zookeeper æ°æ®ç”æ](<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/image001.gif" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/image001.gif</a>)</p><h3 id="znode（data-node）："><a href="#znode（data-node）：" class="headerlink" title="znode（data node）："></a>znode（data node）：</h3><ol><li>其中的每个子目录就是一个znode，他们也可以有子的znode（临时znode除外）。</li><li>这些znode必须是绝对路径，不允许相对路径。</li><li>每个znode都维护一个stat structure（linux系统文件的结构），其中包括版本号，acl改变等等。每次更新数据会让版本号自增。</li><li>每个znode可以设置一个watches，当watch触发后，zookeeper将发给client一个提醒。</li><li>在znode中数据读写是原子性的。每个znode都有一个ACL（access control list）来控制其读写权限。zookeeper不是用来做数据库的，其中存储的数据可能都是几kb的配置/状态信息。大块数据都存在hdfs中。</li><li>Ephemeral node就是临时节点，每次会话结束这些节点都会清空，不允许有子节点。</li></ol><p>###Zookeeper Session：</p><p><img src="/img/state_dia.jpg" alt="state_dia"></p><p>###一致性的保证：</p><p>zookeeper是一个非常高效、可扩展的服务。其一致性靠以下几点保证：</p><ol><li>顺序的一致性。一个client的更新会依序发送给其他。</li><li>原子性。更新是原子操作，只有成功和失败，没有中间过程。</li><li>服务器会看到完全相同的服务，不论其连接了哪个服务器。</li><li>可靠性。一旦一个更新被部署，他会一直存在，直到被另一个更新覆盖。</li><li>及时性。client对系统的观察在一个时间段内将是最新的。也即系统的改变在这个时间段内client是看见的，或者可以探测到它失败。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;###Fast Paxos算法：&lt;/p&gt;
&lt;p&gt;这个还没看，先留个坑把。&lt;/p&gt;
&lt;p&gt;###文件结构：&lt;/p&gt;
&lt;p&gt;zookeeper的文件结构大概是这个样子的：&lt;/p&gt;
&lt;p&gt;!å¾ 1 Zookeeper æ°æ®ç”æ](&lt;a href=&quot;https:
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>numpy使用整理</title>
    <link href="http://yoursite.com/2018/07/18/numpy%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/18/numpy使用整理/</id>
    <published>2018-07-18T02:26:21.042Z</published>
    <updated>2018-09-13T08:07:30.513Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了一些numpy的基础，在这里整理一下。</p><h3 id="创建："><a href="#创建：" class="headerlink" title="创建："></a>创建：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#数组</span></span><br><span class="line">array1 = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">np.eye(<span class="number">3</span>)<span class="comment">#单位多维数组</span></span><br><span class="line">np.diag([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])<span class="comment">#对角多维数组</span></span><br><span class="line">np.arange(<span class="number">1</span>,<span class="number">4</span>)<span class="comment">#[1,2,3]数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#矩阵</span></span><br><span class="line">np.mat(<span class="string">"1 2 3;4 5 6;7 8 9"</span>)<span class="comment">#矩阵运算与多维数组运算结果不同，所以要用mat建矩阵，用分号隔开数据</span></span><br><span class="line">matrix = np.mat(array1)<span class="comment">#可以用多维数组初始化矩阵</span></span><br><span class="line">matrix1 = np.bmat(<span class="string">"array1 array2;array1 array2"</span>)<span class="comment">#创建分块矩阵</span></span><br></pre></td></tr></table></figure><h3 id="随机数："><a href="#随机数：" class="headerlink" title="随机数："></a>随机数：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.random(<span class="number">100</span>)<span class="comment">#完全随机</span></span><br><span class="line">np.random.rand(<span class="number">5</span>,<span class="number">5</span>)<span class="comment">#5*5均匀分布</span></span><br><span class="line">np.random.randn(<span class="number">5</span>,<span class="number">5</span>)<span class="comment">#5*5正态分布</span></span><br><span class="line">np.random.randint(<span class="number">2</span>,<span class="number">50</span>,size=(<span class="number">2</span>,<span class="number">3</span>),dtype=<span class="string">'l'</span>)<span class="comment">#大于等于2小于50的2*3的int64型整数</span></span><br></pre></td></tr></table></figure><h3 id="变换数组形态："><a href="#变换数组形态：" class="headerlink" title="变换数组形态："></a>变换数组形态：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr.reshape(<span class="number">3</span>,<span class="number">3</span>)<span class="comment">#转变成3*3的数组，但要求原数组必须9个元素，否则不能reshape</span></span><br><span class="line">np.hstack((arr1,arr2))<span class="comment">#横向组合</span></span><br><span class="line">np.vsplit(arr4,<span class="number">3</span>)<span class="comment">#横向切割，即把横向由1列的变成3列（相当于横着切）</span></span><br></pre></td></tr></table></figure><h3 id="文件存储与读取"><a href="#文件存储与读取" class="headerlink" title="文件存储与读取"></a>文件存储与读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#二进制存储与读取</span></span><br><span class="line"><span class="comment">#存储</span></span><br><span class="line">file = <span class="string">"./temp/save_arr.npy"</span></span><br><span class="line">filez = <span class="string">"./temp/save_arr.npz"</span></span><br><span class="line">np.save(file,arr)<span class="comment">#用save存，文件扩展名.npy，只能存一个数组</span></span><br><span class="line">np.savez(filez,arr1[,arr2...])<span class="comment">#用savez存，文件扩展名.npz，可以存多个数组。注意：不按照要求扩展名，则系统自己添加对应扩展名；二进制存储的数组打开文件看不到真实数据</span></span><br><span class="line"><span class="comment">#读取</span></span><br><span class="line">loaded_data = np.load(file)<span class="comment">#存储可以省略扩展名，读取一定不可以</span></span><br><span class="line">loaded_dataz = np.load(filez)</span><br><span class="line">loaded_dataz[<span class="string">"arr_0"</span>]<span class="comment">#对于多个文件读取，这种方式可以得到单独数组</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#文件存储与读取</span></span><br><span class="line"><span class="comment">#存储</span></span><br><span class="line">np.savetxt(fname,x,fmt=<span class="string">'%d'</span>,delimiter=<span class="string">','</span>,newline=<span class="string">'\n'</span>,header=<span class="string">''</span>,footer=<span class="string">''</span>,comments=<span class="string">'# '</span>)<span class="comment">#x为要存的数组，fmt='%d'表示整数方式存，delimiter表示存储时的分隔符，存储和读取时默认为空格</span></span><br><span class="line"><span class="comment">#读取</span></span><br><span class="line">loaded_data = np.loadtxt(fname,delimiter=<span class="string">","</span>)<span class="comment">#一定也要带上啊delimiter且与文件中的分隔符一致</span></span><br></pre></td></tr></table></figure><h3 id="利用numpy做简单的统计分析"><a href="#利用numpy做简单的统计分析" class="headerlink" title="利用numpy做简单的统计分析"></a>利用numpy做简单的统计分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)<span class="comment">#种子是伪随机数的开头，相同种子对应随机数都相同，一般种子会设为当前时间，确保得到真随机数（numpy默认也是这样）</span></span><br><span class="line">arr = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">arr.sort()<span class="comment">#这个会直接将arr排序</span></span><br><span class="line">arr.sort(axis=<span class="number">0</span>)<span class="comment">#二维数组的sort参数axis可以为0、1，分别对应数组纵向和横向的排序</span></span><br><span class="line"><span class="comment">#去重</span></span><br><span class="line">np.unique(arr)</span><br><span class="line"><span class="comment">#重复</span></span><br><span class="line">np.tile(arr,<span class="number">3</span>)<span class="comment">#arr是重复哪个，3是重复次数</span></span><br><span class="line">np.repeat(arr,<span class="number">3</span>,axis=<span class="number">0</span>)<span class="comment">#axis是重复的方向（tile没有这个参数）</span></span><br><span class="line"><span class="comment">#注意：tile是对数组进行重复，repeat则是对每一个数组的每一个元素进行重复，打破了原来的数组。</span></span><br><span class="line"><span class="comment">#常用统计函数</span></span><br><span class="line">np.sum(arr)<span class="comment">#求和</span></span><br><span class="line">arr.sum(axis=<span class="number">0</span>)<span class="comment">#沿纵轴求和</span></span><br><span class="line">np.mean(arr)<span class="comment">#计算数组均值</span></span><br><span class="line">arr.mean(axis = <span class="number">0</span>)<span class="comment">#沿着纵轴计算数组均值</span></span><br><span class="line">np.std(arr)<span class="comment">#计算标准差</span></span><br><span class="line">np.var(arr)<span class="comment">#计算方差</span></span><br><span class="line">np.min(arr)<span class="comment">#计算最小值</span></span><br><span class="line">np.max(arr)<span class="comment">#计算最大值</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近看了一些numpy的基础，在这里整理一下。&lt;/p&gt;
&lt;h3 id=&quot;创建：&quot;&gt;&lt;a href=&quot;#创建：&quot; class=&quot;headerlink&quot; title=&quot;创建：&quot;&gt;&lt;/a&gt;创建：&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;tabl
      
    
    </summary>
    
    
      <category term="data science" scheme="http://yoursite.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>最近在看的东西</title>
    <link href="http://yoursite.com/2018/07/16/20180716%E6%97%A5%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/16/20180716日记/</id>
    <published>2018-07-16T09:12:39.376Z</published>
    <updated>2018-08-10T10:38:03.271Z</updated>
    
    <content type="html"><![CDATA[<p>最近都没有写博客，想更一篇了。</p><p>前几天在刷算法，这几天看了一些springboot的东西。之前对spring了解的也比较多，而且这次看的也比较浅，就这样吧。</p><p>感觉很慌。马上要找工作了，我却还没开始工作…</p><p>spring-boot用的时候再看吧，</p><p>算法还是要接着刷，</p><p>接下来就做我的python了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近都没有写博客，想更一篇了。&lt;/p&gt;
&lt;p&gt;前几天在刷算法，这几天看了一些springboot的东西。之前对spring了解的也比较多，而且这次看的也比较浅，就这样吧。&lt;/p&gt;
&lt;p&gt;感觉很慌。马上要找工作了，我却还没开始工作…&lt;/p&gt;
&lt;p&gt;spring-boot用的时
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>anaconda基础整理</title>
    <link href="http://yoursite.com/2018/07/04/anaconda%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/04/anaconda基础整理/</id>
    <published>2018-07-04T12:20:26.782Z</published>
    <updated>2018-09-13T08:07:02.449Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个python环境、包管理工具，这玩意很厉害。</p><h3 id="插一个其他东西："><a href="#插一个其他东西：" class="headerlink" title="插一个其他东西："></a>插一个其他东西：</h3><p>在搞conda环境变量的时候在.zshrc里没有注意语句的顺序，变量使用在前，声明在后，导致path里没有这个。。。。。。以后要注意了！</p><p>###使用原因：</p><ol><li>和以前用的virtualenv有点像，可以创建一个独立的python环境，python版本，包都是独立于外部的。</li><li>自带很多数据科学的包，省的下。</li><li>可以将环境与远程同步，也可以clone别人的环境，开发效率高。</li><li>可以与pycharm等工具结合，通用性强。</li><li>Anaconda navigator是一个桌面应用，使用非常简单。</li></ol><p>###常用到的操作：</p><ol><li>在命令行可以用conda来操作一些东西：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">conda create -n &lt;env-name&gt; &lt;package-name&gt;<span class="comment">#创建conda环境</span></span><br><span class="line"></span><br><span class="line">conda remove -n &lt;env-name&gt;<span class="comment">#删除conda环境</span></span><br><span class="line"></span><br><span class="line">conda env list<span class="comment">#查看所有环境，其中带*的为当前环境，在当前环境下，用的python版本、包等都是anaconda的，而不是本机环境</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> activate &lt;env-name&gt;<span class="comment">#激活某个环境，之后zsh前面会加上这个环境的名称</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> deactivate<span class="comment">#退出某个环境</span></span><br><span class="line"></span><br><span class="line">conda install &lt;package-name&gt;[=versionInfo]<span class="comment">#在当前环境下安装包，可以选定版本</span></span><br><span class="line"></span><br><span class="line">conda install -n &lt;env-name&gt; &lt;package-name&gt;<span class="comment">#在特定环境中</span></span><br><span class="line"></span><br><span class="line">conda list<span class="comment">#列出当前环境所有的包</span></span><br><span class="line"></span><br><span class="line">conda search &lt;package-name&gt;<span class="comment">#查找某个包（模糊匹配）</span></span><br></pre></td></tr></table></figure><p>###conda和pycharm的结合：</p><p>pycharm可以直接用conda的environment来做，只要在选择interpreter的时候选conda环境对应的那个即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是一个python环境、包管理工具，这玩意很厉害。&lt;/p&gt;
&lt;h3 id=&quot;插一个其他东西：&quot;&gt;&lt;a href=&quot;#插一个其他东西：&quot; class=&quot;headerlink&quot; title=&quot;插一个其他东西：&quot;&gt;&lt;/a&gt;插一个其他东西：&lt;/h3&gt;&lt;p&gt;在搞conda环境变量的
      
    
    </summary>
    
    
      <category term="data science" scheme="http://yoursite.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>mac&amp;linux命令整理</title>
    <link href="http://yoursite.com/2018/07/04/mac&amp;linux%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/04/mac&amp;linux命令整理/</id>
    <published>2018-07-04T10:03:57.407Z</published>
    <updated>2018-09-13T08:06:55.106Z</updated>
    
    <content type="html"><![CDATA[<p>今天看了一些oh-my-zsh的东西，感觉还是要整理在博客中，不然太容易忘掉了。</p><h3 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h3><p>以后一些好用但是不熟悉的命令就都放在这里了，方便回忆：</p><p><strong>jq</strong> ： 命令行处理json的命令，，支持管道</p><p>用法：</p><p>针对一个json，直接 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head -n 1 xxx.json | jq '.'</span><br></pre></td></tr></table></figure><p>就可以获得format之后的形式，如果想获得json某个key，只要</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head -n 1 xxx.json | jq '.key'</span><br></pre></td></tr></table></figure><p>即可获得</p><p><strong>wc</strong> ： 用于统计指定文件的字节数、字数、行数</p><h3 id="插件："><a href="#插件：" class="headerlink" title="插件："></a>插件：</h3><p>zsh自带很多插件，可以在.zshrc的plugin里写入，就可以用这些插件了，我用的插件包括：</p><ul><li>z。可以直接跳转。它记录（统计）了一些常用的跳转，只要z+destination就可以</li><li>extract。可以直接解压，忽略tar后各种参数。与unzip类似。</li><li>zsh-autosuggestions。这个神器，之前输入的命令可以再提示出来，很方便用。</li><li>Web-search。可以在命令行直接用 google+要查的内容 即可打开搜索页面。</li></ul><h3 id="命令行快捷键："><a href="#命令行快捷键：" class="headerlink" title="命令行快捷键："></a>命令行快捷键：</h3><ul><li>ctrl+q。可以直接删除整行命令。</li><li>ctrl+w。可以删除每一分段的命令。</li><li>ctrl+e。直接跳到命令最后。</li><li>ctrl+a。直接跳到命令最前面。</li><li>command+d 在iterm中分屏</li><li>comand+[ or ] 在iterm的分屏中切换</li></ul><h3 id="文件："><a href="#文件：" class="headerlink" title="文件："></a>文件：</h3><p><strong>/etc/motd</strong> : 改命令行打开的提示语</p><h3 id="遇到的问题："><a href="#遇到的问题：" class="headerlink" title="遇到的问题："></a>遇到的问题：</h3><p>ssh-key生成忘记有什么问题了。。。整理不及时呐～</p><p>scala2.11和autosuggestions配色问题有冲突，导致每次scala都会报错</p><h3 id="Dockerfile的基础应用"><a href="#Dockerfile的基础应用" class="headerlink" title="Dockerfile的基础应用"></a>Dockerfile的基础应用</h3><p>Dockerfile里有个from，就是指从哪个images拿过来的，可以是本地的，所以增量修改images就是新建一个dockerfile然后from原来的images。再加上自己的RUN，最后执行docker build .就可以～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天看了一些oh-my-zsh的东西，感觉还是要整理在博客中，不然太容易忘掉了。&lt;/p&gt;
&lt;h3 id=&quot;命令：&quot;&gt;&lt;a href=&quot;#命令：&quot; class=&quot;headerlink&quot; title=&quot;命令：&quot;&gt;&lt;/a&gt;命令：&lt;/h3&gt;&lt;p&gt;以后一些好用但是不熟悉的命令就都放在
      
    
    </summary>
    
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Max&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/d02dec40f94389c9e1ed922fa6ad3ed2</icon>
  <subtitle>keep hungry, then you&#39;ll be really hungry</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-22T06:06:53.831Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Max Qi</name>
    <email>490949611@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Zookeeper使用整理</title>
    <link href="http://yoursite.com/2019/01/22/Zookeeper%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2019/01/22/Zookeeper简介/</id>
    <published>2019-01-22T05:55:35.996Z</published>
    <updated>2019-01-22T06:06:53.831Z</updated>
    
    <content type="html"><![CDATA[<p>好久没更新博客，主要是前段时间看了一点机器学习的东西，公式不好打字，后来准备面试又看了很多java的基础，过几天会把java的基础整理一下丢在博客上。</p><p>今天主题是zookeeper，bigdata课上老师讲了zookeeper的内容，再加上自己也试了一下，就放在这里整理了。整理只为了自己看，离精通或者熟悉还差着一万八千里呢！</p><h2 id="Zookeeper简介"><a href="#Zookeeper简介" class="headerlink" title="Zookeeper简介"></a>Zookeeper简介</h2><h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><p>zk是一个高扩展和高可用的服务，主要用来支持</p><ul><li>Distributed configuration</li><li>Consensus</li><li>Group Membership</li><li>Leader Election</li><li>Naming</li><li>Coordination</li></ul><h3 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h3><p>Zookeeper分布在不同的服务器上，每个节点是一个server，其中有一个会作为leader，如图：</p><p><img src="/Users/max/Documents/Blog/source/img/zookeeperArc.png" alt="zookeeperArc"></p><p>具体来说，zk的架构有如下特点：</p><ul><li>所有的server都有一份数据、logs、硬盘snipshots的拷贝，被存在内存数据库中（它并不存储真正的数据，所以上面空间通常只有几十M，仅用来存必要的一些配置等）</li><li>在启动时会选择一个作为leader</li><li>client可以选择任何一个server链接</li><li>当大多数（超过一半）server完成了一个改变，那这个修改就被认为成功，并返回update response（可能造成读取旧数据）</li></ul><h3 id="ZK的数据模型"><a href="#ZK的数据模型" class="headerlink" title="ZK的数据模型"></a>ZK的数据模型</h3><p>zk的service到底是什么呢？其实就是一个被全部server追踪的分布式文件系统（这个角度来看有点像hdfs），它也是层次命名空间（类似linux），上面的每个节点称为一个znode。每一个znode都存储着一定的数据，并且可能有子的znode。</p><p>Znode的主要特点：</p><ul><li>基于Key对象来实现/维护分布式一致性信息</li><li>通过数据改变的版本信息、ACL改变和timestamp来维护一致性</li><li>每次改变版本号都会增加</li><li>Znode不是用来存数据的，只是用来存储一些configuration和meta-data</li><li>每个znode可以存timestamp和version信息</li></ul><p>Znode的类型：</p><ol><li>Persist vs. Ephemeral<ul><li>persist节点：一旦被创建不会轻易丢失，即使数据库重启也依然在，每个persist可以包含数据，也可以包含子节点</li><li>ephemeral节点：在session结束或过期后自动删除。服务器的重启也会导致ephemeral接触（可以用于做分布式锁）</li></ul></li><li>Sequence vs. Non-sequence<ul><li>sequence节点：创建出的节点名在指定名称后带有10位10进制数的序号。多克客户端创建同一名称的节点时，都能创建成功，只是序号不同</li><li>non-sequence节点：多客户端在同时创建同一non-sequence节点时，只有一个可创建成功，其他失败。创建出的节点与指定名称完全相同</li></ul></li></ol><p>Watch机制：</p><ul><li>主动推送：当watch被触发时，zk服务器辉主动将更新推送给client，而不是靠client的轮询（观察者模式）</li><li>一次性触发：watch只会被触发一次，后续更新通知必须重新注册一个watch</li></ul><p>Session的特点：</p><ul><li>session是每个client到server的连接，zk支持每个client在保存同个session的基础上切换到不同的server</li><li>内建超时机制</li><li>执行顺序的保证是基于同个session的（即会话一致性）</li></ul><h2 id="Zookeeper的一致性"><a href="#Zookeeper的一致性" class="headerlink" title="Zookeeper的一致性"></a>Zookeeper的一致性</h2><h3 id="数据一致性："><a href="#数据一致性：" class="headerlink" title="数据一致性："></a>数据一致性：</h3><p>一致性是指多副本中数据的一致性，以下是个简单整理（程度由强到弱）：</p><ol><li>强一致性：有原子一致性、线性一致性，有两个要求：<ul><li>任何一次读都能读到某个数据最近一次写到数据</li><li>系统中所有进程，看到的操作顺序都和全局时钟下看到的顺序一致</li></ul></li><li>顺序一致性：也有两个要求：<ul><li>任何一次读都能读到最近一次写的数据</li><li>系统的所有进程顺序一致，但不一定和全局时钟下看到的数据一致</li></ul></li><li>弱一致性：指数据更新后，有些操作可能访问不到，最终一致性是一种弱一致性，是指任意时刻节点上同一份数据不一定相同，但一段时间后，数据总会达到一致。里面有根据访问分为：<ul><li>因果一致性：如果A通知B它更新了一个数据（A,B有因果关系），那么B后续访问只能看到更新过的值</li><li>读写一致性：当进程A自己更新一个值后，它自己只能访问到更新过的值（会话一致性同理）</li><li>单调一致性：当进程看到数据对象的某个值，它就不会再访问到那个值之前的值</li></ul></li><li>补充一点：paxos是共识（consensus）机制，而不是一致性协议</li></ol><h3 id="ZK保证的一致性原则"><a href="#ZK保证的一致性原则" class="headerlink" title="ZK保证的一致性原则"></a>ZK保证的一致性原则</h3><ul><li>FIFO一致性：对于同一个客户端，执行一个请求（get、delete这类操作）应该按照他们被发送的顺序；保证该客户端对于notification和状态改变的顺序一致</li><li>顺序一致性：所以客户端看到的并行写入操作的结果的顺序是一样的，ZAB实现，有点类似paxos</li><li>原子性：update操作要不成功要不失败，不会有中间结果</li><li>单系统镜像：同一个client不论连接那个server，看到的数据都应该是完全一致的。对于不同的client则可能看到不同的（因为有延迟）的数据</li><li>持续性（单调一致性）：当一个更新完成后，他会一直保持直到它被再次更新</li><li>高可用性：2F+1个服务器可以最多容忍F台服务器崩溃</li><li>最终一致性：数据在一段时间后最终辉达成一致</li></ul><h3 id="ZAB-zookeeper-atomic-broadcat-协议："><a href="#ZAB-zookeeper-atomic-broadcat-协议：" class="headerlink" title="ZAB(zookeeper atomic broadcat)协议："></a>ZAB(zookeeper atomic broadcat)协议：</h3><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><p>可靠投递：如果一个事务被提交到一个服务器，那它最终会被提交到所有服务器<br>全局有序：如果一台服务器上事务A在事务B之前提交，那么在所有服务器上事务A一定都在B之前提交<br>因果有序：如果事务A在事务B之前发生（A,B有因果关系），如果一起被提交，一定是A在B之前执行</p><h4 id="协议内容——广播"><a href="#协议内容——广播" class="headerlink" title="协议内容——广播"></a>协议内容——广播</h4><p>为了保证一致性，所有写操作都要经过leader完成，由leader进行广播。广播是个简化的二阶段提交过程：</p><ol><li>leader收到消息请求后，给消息赋予一个全局唯一的64位自增id，叫zxid（类似于rdbms的事务id），通过zxid比较可以实现因果有序</li><li>leader通过fifo队列将带有zxid的proposal分发给所有follwer</li><li>当follwer收到proposal，先存下来，然后返回leader一个ack</li><li>当leader收到过半ack后，leader会向所有follwer发送commit命令，同时在本地执行该请求</li><li>follwer收到commit命令后，执行该请求  </li></ol><p>值得一提的是，写请求是通过leader广播完成的，但读请求leader或者follwer都可以直接处理，只要从本地内存中读取数据返回client即可</p><h4 id="协议内容——恢复"><a href="#协议内容——恢复" class="headerlink" title="协议内容——恢复"></a>协议内容——恢复</h4><p>已经proposal的命令应该被备份。例如一些服务器在收到commit之前leader就挂掉了，这个时候就无法执行该请求。当它重启后应该重新执行（client如果挂掉也需要和server重新同步一下）</p><h2 id="领导选举"><a href="#领导选举" class="headerlink" title="领导选举"></a>领导选举</h2><h3 id="基于TCP的FastLeaderElection"><a href="#基于TCP的FastLeaderElection" class="headerlink" title="基于TCP的FastLeaderElection"></a>基于TCP的FastLeaderElection</h3><p>每个zookeeper的服务器都需要在数据文件夹下创建一个名为myid的文件（里面只有一个整数），用来唯一标识该服务器。而在配置文件必须与其一致，选票包括：</p><ul><li>logicClock：表示服务器发起投票轮数，自增整数</li><li>state：当前服务器状态</li><li>self_id：当前服务器myid</li><li>self_zxid：当前服务器最大zxid</li><li>vote_id：推举的服务器myid</li><li>vote_zxid：被推举服务器上最大zxid</li></ul><p>具体投票流程为：</p><ol><li>初始化选票：清空票箱（票箱中记录了每个服务器最后一次投票）</li><li>发送初始化选票：每个服务器都通过广播票投给自己</li><li>接受外部投票：尝试从其他服务器获得选票，并计入自己票箱。</li><li>判断选举轮数（logicClock）：如果外部logicClock大于自己，则说明自己选举落后于其他服务器，立即清空票箱并将自己logicClock更新为收到的logicClock。再对比自己之前的投票和收到投票logicClock，确认是否需要更新；如果外部logicClock小于自己，则忽略这次投票；相等则进行下一步选票PK</li><li>选票PK：是基于myid和zxid的比较，即先选zxid大的，如果zxid相同再选myid比较大的</li><li>统计投票：如果半数服务器认可了自己的投票，则终止，否则继续</li><li>更新服务器状态：每个服务器根据投票结果更新服务器状态为leading或following</li></ol><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="分布式一致性锁"><a href="#分布式一致性锁" class="headerlink" title="分布式一致性锁"></a>分布式一致性锁</h3><p>利用名称唯一性，枷锁操作时，只需要所有客户端创建/test/lock节点，只有一个创建成功，即可以获得锁。而解锁时，只要删除/test/lock节点，其他客户端通过watch机制可以继续进入竞争创建节点。如下图：</p><p><img src="/Users/max/Documents/Blog/source/img/zkdistributedlock.png" alt="zookeeperArc"></p><h3 id="并发的惊群效应"><a href="#并发的惊群效应" class="headerlink" title="并发的惊群效应"></a>并发的惊群效应</h3><p>以上的实现非常简单，但会产生惊群效应，即当锁释放时，所以客户端都会被唤醒，但只有一个客户端可以获得锁。这个可以通过改良分布式锁实现的方式解决。</p><p>让所有客户端都在/lock下创建临时顺序节点，如果创建客户端自身节点编号是/lock下最小的节点，则获得锁。其他节点只监视比自己小的的最大节点（如创建节点1、2、5，1获得锁，2只监视1，5只监视2），只有当自己监视的节点释放锁自己才可以获得锁。这种其实是一种按照创建顺序排队的实现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;好久没更新博客，主要是前段时间看了一点机器学习的东西，公式不好打字，后来准备面试又看了很多java的基础，过几天会把java的基础整理一下丢在博客上。&lt;/p&gt;
&lt;p&gt;今天主题是zookeeper，bigdata课上老师讲了zookeeper的内容，再加上自己也试了一下，就放
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Data Analytics Systems</title>
    <link href="http://yoursite.com/2018/11/18/Distributed%20Data%20Analytics%20Systems/"/>
    <id>http://yoursite.com/2018/11/18/Distributed Data Analytics Systems/</id>
    <published>2018-11-18T14:27:02.392Z</published>
    <updated>2018-12-09T08:41:10.145Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Distributed-Data-Analytics-Systems"><a href="#Distributed-Data-Analytics-Systems" class="headerlink" title="Distributed Data Analytics Systems"></a>Distributed Data Analytics Systems</h2><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>Drawback:</p><ul><li>Data exchange requires synchronization</li><li>Difficult to cope with partial system failure</li></ul><p>Why Hadoop:</p><ul><li>Reliability: handle partial failures</li><li>Scalability: Automatically scales to more computing nodes</li><li>Programmability: written in high-level code</li></ul><p>How HDFS works?</p><p>When a client application wants to read a file, it communicates with the name node to determine which blocks make up the file, and which datanodes those blocks reside in.Then it communicate directly with the datanodes to read the data.</p><p>MapReduce Execution:</p><ol><li>Pre-loaded local input data</li><li>Intermediate data from mappers</li><li>Values exchanged by shuffle process</li><li>Reducing process generates outputs</li><li>Outputs stored in HDFS</li></ol><p>MapReduce bottleneck:</p><ol><li><p>Problem: huge data transfer takes lot of time in shuffle step.</p><p>Solution:Hadoop will start transfer data from mappers to reduces as the mappers finish work</p></li><li><p>Problem: straggler problem exist indeed, while no reducer can start before every mapper has finished</p><p>Solution: Hadoop uses speculative execution, specifically, if a mapper appears to be running significantly more slowly than others, a new instance of the mapper will start on another machine, operating same data, the first result will be used and the running mapper will be killed</p></li><li><p>Problem: Data must be passed to reducer, which result in a lot of network traffic</p><p>Solution: Combiner, like a “mini-reduce”, runs locally on single mapper’s output, and the codes are often identical with reducer.</p></li><li><p>Problem: potential performance issues or secondary sort is needed.</p><p>Solution: Write you own Custom Partitioners</p></li></ol><h3 id="HaLoop"><a href="#HaLoop" class="headerlink" title="HaLoop"></a>HaLoop</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>Drawbacks:</p><p>Hard to handle recursive program, for example: Graph analytics, machine learning, data mining or some recursive queries. mapreduce: Load and Shuffle data on each iteration</p><p>Why HaLoop:</p><ul><li><p>TaskTracker (Cache management)</p></li><li><p>Scheduler (Cache awareness)</p></li><li><p>Programming model (multi-step loop bodies, cache control)</p></li></ul><p>It is a efficient common runtime for recursive languages: Map, Reduce, Fixpoint.</p><p>Solution:</p><p>Inter-iteration caching:</p><ul><li>Mapper input cache (MI)</li><li>Reducer input cache (RI)</li><li>Reducer output cache (RO)</li></ul><p>RI - Reducer Input Cache:</p><p>Access to loop invariant data without map/shuffle, used by reducer function.</p><p>RO - Reducer Output Cache:</p><p>Distributed access to output of previous iteration, used by fixpoint evaluation</p><p>MI - Mapper Input Cache:</p><p>Access to non-local mapper input on later iterations, used during scheduling of map tasks.</p><p>Architecture:</p><ul><li>Loop Control</li><li>Caching</li><li>Indexing</li></ul><h3 id="FlumeJava"><a href="#FlumeJava" class="headerlink" title="FlumeJava"></a>FlumeJava</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>When meet long and complicated data-parallel pipelines, it is difficult to program and manage, besides each mapreduce job needs to keep intermediate results, what’s more, high overhead at synchronization barrier between different mapreduce jobs.</p><p>Why flume?</p><p>Expressiveness</p><p>Abstractions</p><p>Performance (lazy evaluation and Dynamic optimization)</p><p>Usability &amp; deployability (implemented as a java library)</p><p>Optimization:</p><ol><li>Sink flatten</li><li>ParallelDo fusion</li><li>MSCR fusion</li></ol><h3 id="Dryad"><a href="#Dryad" class="headerlink" title="Dryad"></a>Dryad</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>General-purpose execution engine for coarse-grained data-parallel applications</p><p>Easy to write simple programs, execution engine automatically manages scheduling, distribution, FT, etc.</p><p>Why Dryad?</p><p>Job = Directed Acyclic Graph</p><p>Computational “vertices” connected by communication “channels”(edges)</p><p>What GDL (Graph Description Language)?</p><p>A lower-level programming model than SQL</p><p>Architecture?</p><ul><li>Job Manager</li><li>Name Server</li><li>Daemons</li></ul><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>complex applications</p><p>interactive ad-hoc queries</p><p>Reuse of intermediate results across multiple computatios</p><p>RDD (Resilient Distributed Datasets)?</p><ol><li>Restricted form of distributed shared memory, only be built through coarse-grained deterministic transformations</li><li>Fault recovery using lineage (Log transformations used to build a dataset, log enough info how it was derived from other RDDs)</li></ol><p>RDD good for:</p><p>Apply the same operation to all elements of a dataset (coarse-grained operation)</p><p>Remember each transformation as one step in a lineage graph</p><p>Recovery of lost partitions without having to log large amounts of data</p><p>Not good for: asynchronous fine-grained updates to shared state</p><p>Task Scheduler:</p><p>Dryad-like DAGs</p><h3 id="Naiad"><a href="#Naiad" class="headerlink" title="Naiad"></a>Naiad</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>Iterative processing on streaming data, interactive queries on a fresh, consistent view of the results.</p><p>Whay Naiad?</p><p>A new computational model: timely dataflow</p><p>Solusion:</p><p>iteractive and incremental computations : Structured loops allowing feedback in the dataflow, stateful dataflow vertices capable of consuming and producing records without global coordination</p><p>producing consistent results -&gt; notifications for vertices once they have received all records for a given round of input or loop iteration.</p><p>Key point:</p><p>Timestamp: </p><p>in the graph, every stateful vertices receive timestamped message along directed edges.</p><p>In nested cycle, use timestamp to distiguish data in different input and loop iterations</p><p>Two methods:</p><p>Supports asynchronous and fine-grained synchronous execution</p><ol><li>Batching: sychronous, one-to-one correspondence between input and output</li><li>Streaming: asychronous, overlapping computation (latency is low)</li></ol><p>Low latency?</p><ol><li>programming model: Asynchronous and fine-grained synchronous execution.</li><li>Distributed progress tracking protocol: enables processes to deliver notifications promptly.</li></ol><h3 id="Husky"><a href="#Husky" class="headerlink" title="Husky"></a>Husky</h3><p>Drawbacks of traditional distributed Framework,Why this?</p><p>High performance, flat learning curve, good reusability, low maintenance cost and high compatibility</p><p>Why husky?</p><p>A new computational model that makes Husky general and expressive</p><p>Architecure?</p><p>Master-Worker architecture </p><p>master: </p><p>keeps worker information and data partitioning scheme</p><p>Does not sit on the data path and don’t compute</p><p>coordinates work among workers and monitors the progress of workers</p><p>Worker:</p><p>Read/write data, communicate with other workers. compute in parallel</p><p>Send heartbeat to master periodically</p><p>Implementation:</p><p>Channel-based messaging subsystem -&gt; makes streaming computation posible</p><p>Store attribute lists as in a column-store</p><p>Better locality, more oppotunity to optimize (vectorization). Adding attributes without recompiling, useful for interactive data analysis.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Distributed-Data-Analytics-Systems&quot;&gt;&lt;a href=&quot;#Distributed-Data-Analytics-Systems&quot; class=&quot;headerlink&quot; title=&quot;Distributed Data Analyti
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>笔试和面试准备</title>
    <link href="http://yoursite.com/2018/10/24/%E9%9D%A2%E8%AF%95%E5%92%8C%E7%AC%94%E8%AF%95%E7%9A%84%E5%87%86%E5%A4%87/"/>
    <id>http://yoursite.com/2018/10/24/面试和笔试的准备/</id>
    <published>2018-10-24T12:54:57.115Z</published>
    <updated>2018-11-11T06:55:11.158Z</updated>
    
    <content type="html"><![CDATA[<p>由于不想错过秋招，并且为明年春招做准备，之后会不定期的在这里整理一些问题。</p><h2 id="语言——java、python、cpp、scala"><a href="#语言——java、python、cpp、scala" class="headerlink" title="语言——java、python、cpp、scala"></a>语言——java、python、cpp、scala</h2><h3 id="并发编程、java内存模型（Java-Memory-Model）："><a href="#并发编程、java内存模型（Java-Memory-Model）：" class="headerlink" title="并发编程、java内存模型（Java Memory Model）："></a>并发编程、java内存模型（Java Memory Model）：</h3><h4 id="线程之间的通信机制有两种："><a href="#线程之间的通信机制有两种：" class="headerlink" title="线程之间的通信机制有两种："></a>线程之间的通信机制有两种：</h4><ul><li><p><strong>共享内存 </strong>：不同线程共享内存中的公共状态来隐式的通行，比较典型的就是通过<strong>共享对象</strong>来进行通信。对于线程的同步必须是显式的，即程序员手动指定某个方法或者某段代码必须在线程之间互斥的进行。</p></li><li><p><strong>消息传递</strong>：线程之间没有公共状态，线程之间必须通过明确的发送信息进行显式通信，比较典型的就是调用<strong>wait()</strong>或者<strong>notify()</strong>等函数的方式。同步是隐式的，因为消息的发送必须在消息接受前。</p></li></ul><h4 id="内存模型："><a href="#内存模型：" class="headerlink" title="内存模型："></a>内存模型：</h4><p><img src="../img/jmm.png" alt="jmm"></p><p>每个线程都有自己的本地内存，然后也有主内存。本地内存中有一份共享变量的副本。线程A刷新了自己本地内存中的变量x，但AB需要通信时，会先通过副本刷新主内存中的x，然后线程B再读取主内存中的x，此时B的本地内存中x也成了新值。</p><h4 id="内存模型的实现"><a href="#内存模型的实现" class="headerlink" title="内存模型的实现"></a>内存模型的实现</h4><p><img src="../img/jmmImplement.png" alt="jmmImplement"></p><p>所有的原始类型变量都在stack中，一个线程中的本地变量对另一个线程是不可见的。而java创建的对象，以及原始类型的封装类则存在heap中（不管是成员变量还是方法中的本地变量）。heap中的变量是可以被共享的，只要另一个线程获得了这个对象，那它就可以访问这个对象的成员变量。</p><h4 id="Volatile和sychronized区别"><a href="#Volatile和sychronized区别" class="headerlink" title="Volatile和sychronized区别"></a>Volatile和sychronized区别</h4><p>首先要理解线程安全的两个方面：</p><ul><li><strong>执行控制：</strong>目的是控制代码执行顺序及是否可以并发进行</li><li><strong>内存可见：</strong>线程执行的结果在内存中的可见性</li></ul><p>特征：</p><p>synchronized：是解决执行控制的问题，即阻止其他线程获得当前对象的监控锁，使得被synchronized保护的代码块无法被其他线程所访问，也就无法并发执行。</p><p>volatile：解决的是内存可见性的问题。使得所有对volatile变量对读写都直接刷新到主存，即保证了变量的可见性，但只能保证对原始类型变量（除了double、long）的操作原子性。</p><p>区别：</p><ol><li>volatile本质是告诉jvm这个变量值是不确定的，需要从主内存中取，而synchronized则是直接锁定当前变量。</li><li>volatile只能针对变量，而synchronized可以用在变量、方法或类的级别。</li><li>volatile只能实现对变量修改的可见性，而无法保证原子性。synchronized则可以保证原子性</li><li>volatile不会造成线程阻塞，而synchronized会造成</li><li>volatile标记变量不会被编译器优化，而synchronized则会被优化。</li></ol><h4 id="happen-before原则："><a href="#happen-before原则：" class="headerlink" title="happen-before原则："></a>happen-before原则：</h4><p>一个线程内，按照代码执行顺序，书写在前面的操作先行发生于书写在后面的操作。</p><h4 id="内存栅栏："><a href="#内存栅栏：" class="headerlink" title="内存栅栏："></a>内存栅栏：</h4><p>为了提高性能，cpu通常不按照顺序执行指令，而是通过重排指令来提高存储器的利用率，但这也意味着会有不可预测的问题发生。内存栅栏就是解决指令重排问题的方法，具体来说，是指处理器在重排时，不能采用先执行栅栏后的内存访问，在执行栅栏前内存访问的方式。这也是volatile的实现方式。</p><h2 id="大数据相关"><a href="#大数据相关" class="headerlink" title="大数据相关"></a>大数据相关</h2><h3 id="mysql数据库"><a href="#mysql数据库" class="headerlink" title="mysql数据库"></a>mysql数据库</h3><h4 id="mysql主键和唯一索引"><a href="#mysql主键和唯一索引" class="headerlink" title="mysql主键和唯一索引"></a>mysql主键和唯一索引</h4><p>之前不太了解唯一索引，这次才知道。唯一索引就是mysql里的unique，可以在创建表时或者建表后增加，如建表后可以用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">UNIQUE</span> <span class="keyword">INDEX</span> nm <span class="keyword">on</span> PERSON(<span class="keyword">name</span>);</span><br></pre></td></tr></table></figure><p>执行成功后，如果在原表已经有一条类似 (‘1’,’max’,’1995-12-12’)类似的记录后，还要插入如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO person(name,birthday) values (&apos;max&apos;,&apos;1995-12-13&apos;);</span><br></pre></td></tr></table></figure><p>会直接报错，告诉用户name是唯一索引，无法再插入相同。这个有个使用场景，即在高并发下为了保证某一个键不会插入重复信息，需要给这个键增加唯一索引。</p><p>主键和唯一索引的区别：</p><ul><li>唯一索引允许空值，但主键不能为空</li><li>主键创建后一定包含一个唯一索引，但唯一索引不一定是主键</li><li>主键可以被其他表引为外间，但唯一索引不能</li><li>一个表只能有一个主键，但可以有多个外键</li></ul><p>mysql里key其实包含三种：PRI（主键）,UNQ（唯一索引）,MUL（普通key，可以重复）</p><h4 id="外键："><a href="#外键：" class="headerlink" title="外键："></a>外键：</h4><p>一个表的主键可以作为它从表的一个外键，外键本质是一种约束。外键要求和主键Type完全相同（int(10)和int(11)的差别都不可以），用如下语句可以创建外键约束：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> my_profile <span class="keyword">add</span> <span class="keyword">CONSTRAINT</span> <span class="keyword">id</span> foreign <span class="keyword">key</span>(<span class="keyword">id</span>) <span class="keyword">references</span> person(<span class="keyword">id</span>) <span class="keyword">on</span> <span class="keyword">delete</span> <span class="keyword">cascade</span> <span class="keyword">on</span> <span class="keyword">update</span> <span class="keyword">no</span> <span class="keyword">action</span>;</span><br></pre></td></tr></table></figure><p>注意后面可以对这个键的delete、update进行约定，确定是否进行级联操作。</p><p>注意：外键只有InnoDB支持，而MyISAM不支持。</p><h4 id="InnoDB和MyISAM的几点区别："><a href="#InnoDB和MyISAM的几点区别：" class="headerlink" title="InnoDB和MyISAM的几点区别："></a>InnoDB和MyISAM的几点区别：</h4><ul><li>InnoDB支持事物和外键，而MyISAM不支持</li><li>InnoDB支持行及锁，MyISAM只支持表级锁</li><li>InnoDB不支持全文索引，而MyISAM支持</li><li>MyISAM相对简单，提供了告诉存储和检索，在效率上要优于InnoDB，适合管理非事物表，适合有大量的select的程序。</li><li>InnoDB则更安全，多用户的并发性能更好，适合有大量insert、update的事物处理的应用程序。</li></ul><h4 id="MySQL的锁机制："><a href="#MySQL的锁机制：" class="headerlink" title="MySQL的锁机制："></a>MySQL的锁机制：</h4><p>mysql有几种类型的锁：</p><ul><li>排他锁（写锁，X锁）：如果事务T对A加上排他锁，那其他事务不能对A加任何类型的锁。获得排他锁既能读数据也能写数据。</li><li>共享锁（读锁，S锁）：如果事务T对A加上共享锁，则其他事务不能对A加排他锁，获得共享锁的事务只能读数据，不能写数据。</li><li>行级锁：行级锁氛围共享锁和排他锁，行级锁是mysql中锁定粒度最细的锁。行级锁开销大，加锁慢，锁定力度小，发生锁冲突的概率最低，并发度最高。</li><li>表级锁：表级锁分为表共享锁和表独占锁。表级锁开销小，加锁快，锁定粒度大，发生冲突概率高，并发度低。</li><li>悲观锁：即悲观并发控制，简称PCC。悲观锁是指在数据处理过程中加锁，使数据处于锁定状态，使用数据库锁机制实现。注意在mysql中使用悲观锁要关闭mysql的自动提交。——安全，但产生额外开销，增加产生死锁机会。</li><li>乐观锁：通过记录数据版本的方式实现。为数据增加一个版本标识，读取数据时，将版本标识一起取出，数据每更新一次，就对版本标识进行更新。</li></ul><p>语句示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#共享锁</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> person <span class="keyword">WHERE</span> ... <span class="keyword">LOCK</span> <span class="keyword">IN</span> <span class="keyword">SHARE</span> <span class="keyword">MODE</span>;</span><br><span class="line">#排他锁</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> person <span class="keyword">WHERE</span> ... <span class="keyword">FOR</span> <span class="keyword">UPDATE</span>;</span><br></pre></td></tr></table></figure><p>实现：</p><p>InnoDB实现行级锁需要在表上增加索引，InnoDB会对索引项加锁，即当InnoDB没有索引项时，也只能进行表锁。</p><h4 id="InnoDB中MVCC的一些理解"><a href="#InnoDB中MVCC的一些理解" class="headerlink" title="InnoDB中MVCC的一些理解"></a>InnoDB中MVCC的一些理解</h4><p>mvcc（multiversion concurrency control），即多版本并发控制技术，它使得行锁不再简单通过锁来进行并发控制。它本身希望同一个数据有多个版本，版本号是单向增长的。而读事务只读在该事务开始前的数据库的快照。实质是用来解决读写冲突的无锁并发控制。乐观锁就是这种思路。</p><h2 id="算法相关"><a href="#算法相关" class="headerlink" title="算法相关"></a>算法相关</h2><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于不想错过秋招，并且为明年春招做准备，之后会不定期的在这里整理一些问题。&lt;/p&gt;
&lt;h2 id=&quot;语言——java、python、cpp、scala&quot;&gt;&lt;a href=&quot;#语言——java、python、cpp、scala&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
      <category term="career" scheme="http://yoursite.com/tags/career/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch使用整理</title>
    <link href="http://yoursite.com/2018/10/17/elasticsearch%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2018/10/17/elasticsearch基本概念/</id>
    <published>2018-10-17T09:08:43.297Z</published>
    <updated>2018-10-18T02:22:57.658Z</updated>
    
    <content type="html"><![CDATA[<p>最近项目要使用elasticsearch。。。这里总结一下看elasticsearch文档的一些总结吧。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>elasticsearch是一个全文本搜索的服务，比如github从几百亿行代码中搜索某个代码就用了elasticsearch。在mac下可以通过brew安装和管理，注意还要再安装一个kibana来进行可视化和管理（可以在kibana的console跑各种请求）。es默认跑在9200，kibana默认跑在5601端口。</p><p>这里注意对于中文场景要安装<a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">elasticsearch-analysis-ik</a>进行分词，ik还支持自定义词典，可以在es-path/config/analysis-ik/custom下自定义dic。</p><p>概念对应关系如下表</p><table><thead><tr><th>传统数据库</th><th>ElasticSearch</th></tr></thead><tbody><tr><td>Database</td><td>Index</td></tr><tr><td>Table</td><td>Type</td></tr><tr><td>Row</td><td>Document</td></tr><tr><td>Column</td><td>Field</td></tr><tr><td>schema</td><td>mapping</td></tr></tbody></table><p>elasticsearch的所有内容都是通过json格式传递的。每个文档都是一个json的形式。一般文档包括下面三个metadata：</p><ul><li>_index：文档存储的地方（类似于database）</li><li>_type：文档代表的对象的类（类似于table）</li><li>_id：文档的唯一标示（类似于主见，可以自定义，也可以让es帮你生成）</li></ul><p>es是支持分布式的，当然也有对应的shard和replica机制。</p><h2 id="单机操作api"><a href="#单机操作api" class="headerlink" title="单机操作api"></a>单机操作api</h2><h3 id="创建："><a href="#创建：" class="headerlink" title="创建："></a>创建：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125; </span><br><span class="line">&#123;</span><br><span class="line">    "field":"value",</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="获取："><a href="#获取：" class="headerlink" title="获取："></a>获取：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GET /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;?_source=title,text#可以获取特定字段（包含metadata）</span><br><span class="line">GET /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_source#只获取_source字段，而无metadata</span><br></pre></td></tr></table></figure><h3 id="存在："><a href="#存在：" class="headerlink" title="存在："></a>存在：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HEAD /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;#判断document是不是存在，只返回状态码而没有具体数据</span><br></pre></td></tr></table></figure><h3 id="更新："><a href="#更新：" class="headerlink" title="更新："></a>更新：</h3><p>由于elasticsearch中的文档是不可变的，所以修改只能通过重建或者脚本的方式更新</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">也可以用post，注意put和post的区别主要是post可以默认自动生成id</span></span><br><span class="line">POST /&#123;index&#125;/&#123;type&#125;</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>脚本的方式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST &#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update#注意最后是一个_update</span><br><span class="line">&#123;</span><br><span class="line">    "script" : &#123;</span><br><span class="line">        "source":"stx._source.tags.add(params.tag)",</span><br><span class="line">        "lang":"painless",</span><br><span class="line">        "params" : &#123;</span><br><span class="line">            "tag":"blue"</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">想增加一个字段tags的内容</span></span><br></pre></td></tr></table></figure><h3 id="删除："><a href="#删除：" class="headerlink" title="删除："></a>删除：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;#删除某个文档</span><br></pre></td></tr></table></figure><h3 id="搜索："><a href="#搜索：" class="headerlink" title="搜索："></a>搜索：</h3><p>elasticsearch最重要的功能就是搜索，搜索的姿势也是很多的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GET /_search#在所有索引类型中搜索</span><br><span class="line">GET /&#123;index1&#125;,&#123;index2&#125;/_search#在index1和index2两种索引类型中搜索</span><br><span class="line">GET /&#123;index&#125;/&#123;type&#125;/_search#在某个type中搜索</span><br><span class="line">GET /_all/&#123;type&#125;,&#123;type&#125;/_search#在所有索引的某些type上搜索</span><br></pre></td></tr></table></figure><p>对于搜索的结果，可以用size分页，用from决定从哪里出发</p><p>当然平时比较多的是查找字符串</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GET /_all/&#123;type&#125;/_search?q=tweet:elasticsearch#查找某个type中tweet：elasticsearch的文档</span><br><span class="line">GET /_all/_doc/_search?q=%2bdescribtion%3amax#查找某个类中describtion字段包含max的文档</span><br></pre></td></tr></table></figure><p>第二个要重点解释一下，其实在url encode之前为：</p><p>+describtion:max</p><p>如果要放在查询中，就要进行urlencode（percent encode），然后放在q后，即：</p><p>q=%2bdescribtion%3amax</p><p>这里的查询不仅限于字符串查找，还可以是特定条件下的查找，如</p><ul><li><code>name</code>字段包含<code>&quot;mary&quot;</code>或<code>&quot;john&quot;</code></li><li><code>date</code>晚于<code>2014-09-10</code></li><li><code>_all</code>字段包含<code>&quot;aggregations&quot;</code>或<code>&quot;geo&quot;</code></li></ul><p>这个可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+name:(mary john) +date:&gt;2014-09-10 +(aggregations geo)</span><br></pre></td></tr></table></figure><p>来表示，最后urlencode即可进行查询。</p><h2 id="mapping和Analysis"><a href="#mapping和Analysis" class="headerlink" title="mapping和Analysis"></a>mapping和Analysis</h2><p>es的mapping对应了传统数据库的schema，其中es会对不同的字段类型进行猜测，得到对应的mapping，在这种情况下，对某个field的类型猜测可能会是date，但如果用_all搜索这个field则认为他是string，从而带来搜索结果的不同。</p><p>对于一个搜索引擎，与数据库最本质的区别在于确切值和全文文本之间。</p><p>传统数据库需要在where语句里的特定匹配，但es则希望如下：</p><ul><li>一个针对<code>&quot;UK&quot;</code>的查询将返回涉及<code>&quot;United Kingdom&quot;</code>的文档</li><li>一个针对<code>&quot;jump&quot;</code>的查询同时能够匹配<code>&quot;jumped&quot;</code>， <code>&quot;jumps&quot;</code>， <code>&quot;jumping&quot;</code>甚至`”leap”</li></ul><p>等等等场景，为此引出了es最关键的技术——倒排索引。</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><p>对于大小写或者同源词等原本是不能匹配的，需要通过特定的分析过程才能被检索，具体过程如下：</p><ul><li>首先，标记化一个文本快为适用于倒排索引的term</li><li>标准化这些term为标准行事，提高可搜索率和查全率</li></ul><p>一个完成的分析器（analyzer）包括三部分：</p><ol><li>字符过滤器：去掉一些html标签，或者转化&amp;为and等</li><li>分词器：通过空格或逗号分词（中文需要特定分词工具ik）</li><li>标记过滤：将词进行大小写转换、去掉一些停用词（a、the这些）或者增加词（同义词）</li></ol><p>可以通过如下方式测试analyzer</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">GET _analyze?pretty</span><br><span class="line">&#123;</span><br><span class="line">  "analyzer": "ik_smart",</span><br><span class="line">  "text":"王者荣耀真好玩"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">output:</span></span><br><span class="line">&#123;</span><br><span class="line">  "tokens": [</span><br><span class="line">    &#123;</span><br><span class="line">      "token": "王者荣耀",</span><br><span class="line">      "start_offset": 0,</span><br><span class="line">      "end_offset": 4,</span><br><span class="line">      "type": "CN_WORD",</span><br><span class="line">      "position": 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "token": "真好玩",</span><br><span class="line">      "start_offset": 4,</span><br><span class="line">      "end_offset": 7,</span><br><span class="line">      "type": "CN_WORD",</span><br><span class="line">      "position": 1</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果想使用某个特定analyzer，而不是系统自带的standard，就需要通过mapping。</p><p>查看map需要用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /&#123;index&#125;/_mapping/&#123;type&#125;</span><br></pre></td></tr></table></figure><p>这样得到mappings的内容，注意现在一些field的类型是keyword（而不是text）是因为keyword不会自动分词并建立索引，但text会这么做。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">PUT final</span><br><span class="line">&#123;</span><br><span class="line">  "mappings": &#123;</span><br><span class="line">    "_doc": &#123;</span><br><span class="line">      "properties": &#123;</span><br><span class="line">        "name": &#123;</span><br><span class="line">          "type": "text"</span><br><span class="line">        &#125;,</span><br><span class="line">        "blob": &#123;</span><br><span class="line">          "type": "text",</span><br><span class="line">          "analyzer":"ik_smart"</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近项目要使用elasticsearch。。。这里总结一下看elasticsearch文档的一些总结吧。&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;el
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>最近写openrice爬虫遇到的一些问题（未完待续）</title>
    <link href="http://yoursite.com/2018/10/12/%E5%9F%BA%E4%BA%8Epyhusky%E7%9A%84%E7%88%AC%E8%99%AB%EF%BC%89/"/>
    <id>http://yoursite.com/2018/10/12/基于pyhusky的爬虫）/</id>
    <published>2018-10-12T14:12:42.304Z</published>
    <updated>2018-10-14T11:49:23.583Z</updated>
    
    <content type="html"><![CDATA[<p>最近项目中需要写爬虫，需要爬openrice，其中有一些tips需要记录一下。</p><h2 id="tor-and-polipo"><a href="#tor-and-polipo" class="headerlink" title="tor and polipo"></a>tor and polipo</h2><h3 id="Tor"><a href="#Tor" class="headerlink" title="Tor"></a>Tor</h3><p>tor是一个socks5代理工具，可以实现匿名访问网页，本质其实是利用了分布在世界各地的一些肉机做请求转发。</p><p>tor有两部分，tor和ControlPort，他们都需要在tor/config下手动配置：</p><ul><li>tor，一般绑定在9050端口，socks5代理服务器，如果在mac下，可以通过brew下载并通过brew services进行管理</li><li>ControlPort，一般绑定在9051端口，用来管理本地的tor，在python下用TorCtl包来操作。开启前要通过tor –keygen设置passphrase</li></ul><p>在linux下可以用lsof -i:port-num来查看有没有正确开启这两个服务。</p><p>可以通过Control向tor发送不同的signal的方式来实现对tor的管理，例如获取新ip可以</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNewIp</span><span class="params">()</span>:</span></span><br><span class="line">    conn = TorCtl.connect(controlAddr=<span class="string">"127.0.0.1"</span>, controlPort=<span class="number">9051</span>, passphrase=<span class="string">"my password"</span>)</span><br><span class="line">    conn.send_signal(<span class="string">"NEWNYM"</span>)</span><br></pre></td></tr></table></figure><h3 id="Polipo"><a href="#Polipo" class="headerlink" title="Polipo"></a>Polipo</h3><p>由于tor是通过sock5代理的，但实际使用的很多python库只支持http/https代理，例如requests、urllib3等，这样需要一个web服务器来进行转化。polipo就是这样的工具。</p><p>mac端可以通过brew来下载和管理polipo，但polipo的config不会自动生成，需要我手动去/usr/local/etc下mkdir polipo文件并自己写config，我的config如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">proxyAddress = "::0"</span><br><span class="line">allowedClients = 127.0.0.1, 192.168.1.0/24</span><br><span class="line">proxyAddress = "0.0.0.0"</span><br><span class="line">socksParentProxy = "localhost:9050"</span><br><span class="line">socksProxyType = socks5</span><br></pre></td></tr></table></figure><p>这里一个坑是我刚开始在polipo的config里没有写proxyaddress = “0.0.0.0”，会有很多问题</p><p>polipo默认绑定端口是8123，如果成功开启，在python可以通过</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">proxy_manager = urllib3.ProxyManager(</span><br><span class="line">    <span class="string">'http://127.0.0.1:8123'</span>,</span><br><span class="line">    timeout=urllib3.Timeout(connect=<span class="number">10</span>, read=<span class="number">10</span>),</span><br><span class="line">    retries=urllib3.Retry(<span class="number">5</span>),</span><br><span class="line">    maxsize=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>proxy_manager可以正常访问web页面，但是速度较慢。</p><h2 id="Pyhusky"><a href="#Pyhusky" class="headerlink" title="Pyhusky"></a>Pyhusky</h2><p>可以通过husky的map来运行下载任务，提高分布式效率。</p><p>首先需要在本地启动husky的master和daemon任务（目前遗留了一个问题：每个任务只能跑一次，第二次跑不了？），然后在python通过</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.pyhusky_start(<span class="string">'127.0.0.1'</span>, <span class="number">20000</span>)</span><br><span class="line">pages = env.parallelize(url_list)</span><br><span class="line">tuple = pages.map(<span class="keyword">lambda</span> url: mapper_process(url)).collect()</span><br></pre></td></tr></table></figure><p>来运行的，注意这里有一个坑是parallelize分成几份会由master config的 [worker] 下的配置决定的，所以如果发现本机只跑了所有任务的一半或三分之一，就去check一下husky的master config有无问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近项目中需要写爬虫，需要爬openrice，其中有一些tips需要记录一下。&lt;/p&gt;
&lt;h2 id=&quot;tor-and-polipo&quot;&gt;&lt;a href=&quot;#tor-and-polipo&quot; class=&quot;headerlink&quot; title=&quot;tor and polipo&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="crawler" scheme="http://yoursite.com/tags/crawler/"/>
    
  </entry>
  
  <entry>
    <title>pyhusky的单机编译与运行遇到的坑</title>
    <link href="http://yoursite.com/2018/10/10/pyhusky%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E8%BF%90%E8%A1%8C/"/>
    <id>http://yoursite.com/2018/10/10/pyhusky的编译与运行/</id>
    <published>2018-10-10T11:20:22.632Z</published>
    <updated>2018-10-14T11:49:43.670Z</updated>
    
    <content type="html"><![CDATA[<p>由于项目要用到husky的python版，pyhusky。所以要在本机运行husky。不得不说，c++的项目的可移植性和兼容性真的差java好几条街，所以编译这个看起来简单，但实操坑非常多。</p><h3 id="坑1-——-zeromq"><a href="#坑1-——-zeromq" class="headerlink" title="坑1 —— zeromq"></a>坑1 —— zeromq</h3><p>我很早以前就在mac上通过brew装好了zeromq，这次在编译husky的时候还报了找不到。最后发现homebrew安装的zmq只是一个底层的c版本，而不是合适版本。这里我只好手动下载了libzmq和cppzmq。clone下来后大概流程就是：</p><ol><li>mkdir build &amp; cd build</li><li>make ..</li><li>make -j4</li><li>make install</li></ol><p>可以说是build一个项目最典型的操作了。刚开始不理解学长讲的要把hpp放在系统的include里，后来了解了一下原来cpp会把自己的一些hpp文件（类似快捷方式）放在/usr/local/include文件目录下，其他proj就可以去调用了。而make install里面集成了这一步。</p><h3 id="坑2-——-libhdfs3"><a href="#坑2-——-libhdfs3" class="headerlink" title="坑2 —— libhdfs3"></a>坑2 —— libhdfs3</h3><p>这个库是cpp用来连接hdfs的，但放弃维护了，所以只能找别人的，这里我找到了一个可用的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:ContinuumIO/libhdfs3-downstream.git</span><br><span class="line">mkdir build &amp; cd build</span><br><span class="line">../bootstrap --prefix=/usr/local --dependency=$(brew --prefix openssl)</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>这个需要提前装好gtest和gmock，就要去下载gtest和gmock，还是一样的build</p><h3 id="坑3-——-build完Master依然不存在"><a href="#坑3-——-build完Master依然不存在" class="headerlink" title="坑3 —— build完Master依然不存在"></a>坑3 —— build完Master依然不存在</h3><p>在make master之后，应该出现Master应用程序，但我的程序里没出现。后来才知道因为mac的默认文件格式是不支持大小写敏感的，所以会有问题。这样我需要去改master下build的CMakeList.txt，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> MasterHusky</span></span><br><span class="line">add_executable(MasterHusky master.cpp $&#123;master_plugins&#125;)</span><br><span class="line">target_link_libraries(MasterHusky $&#123;husky&#125;)</span><br><span class="line">target_link_libraries(MasterHusky $&#123;EXTERNAL_LIB&#125;)</span><br><span class="line">set_property(TARGET MasterHusky PROPERTY CXX_STANDARD 14)</span><br></pre></td></tr></table></figure><p>原来这里的MasterHusky叫Master，与build出的master文件冲突，所以要将它改成MasterHusky的名字，下面相应的地方都要改，然后再重新cmake就可以了。</p><h3 id="坑4-——-Daemon无法执行"><a href="#坑4-——-Daemon无法执行" class="headerlink" title="坑4 —— Daemon无法执行"></a>坑4 —— Daemon无法执行</h3><p>刚开始不知道要跑起来master以后再跑daemon，daemon也是build出来的，但刚开始不能跑，我需要装好libhdfs3以后，先运行master，再运行daemon（master不能停顿太久，因为可能挂掉了）</p><h3 id="坑5-——-bindings路径问题"><a href="#坑5-——-bindings路径问题" class="headerlink" title="坑5 —— bindings路径问题"></a>坑5 —— bindings路径问题</h3><p>这个是最大的一个坑，我需要把husky目录下的bindings放在自己执行./Daemon和./Master的地方才可以！！！</p><p>例如我是直接在release里执行的，就需要把bindings文件夹和crawler下的hcrawl都放在release目录下！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于项目要用到husky的python版，pyhusky。所以要在本机运行husky。不得不说，c++的项目的可移植性和兼容性真的差java好几条街，所以编译这个看起来简单，但实操坑非常多。&lt;/p&gt;
&lt;h3 id=&quot;坑1-——-zeromq&quot;&gt;&lt;a href=&quot;#坑1-——
      
    
    </summary>
    
    
      <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>降维的常用方法总结（未完待续）</title>
    <link href="http://yoursite.com/2018/10/10/%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4/"/>
    <id>http://yoursite.com/2018/10/10/数据降维/</id>
    <published>2018-10-10T11:19:12.720Z</published>
    <updated>2018-10-14T11:50:00.299Z</updated>
    
    <content type="html"><![CDATA[<p>课上讲了常见的数据降维的方法PCA，感觉比较有用，再结合一下网上看到的比较新的方法，总结在这里吧。</p><h3 id="常见的词汇"><a href="#常见的词汇" class="headerlink" title="常见的词汇"></a>常见的词汇</h3><p>最近都只看英文，所以相关的很多词汇中文不太熟悉，以后面试内地的话不太好聊，先把中文翻译放在这里</p><p>rank of matrix：矩阵的秩</p><p>determinant：行列式</p><p>conjugate transpose：转置</p><p>orthonormal matrix：正交矩阵</p><p>similarity matrix：相似矩阵</p><p>diagonal matrix：对角矩阵</p><p>diagonal element：对角元素</p><p>Singular Value Decomposition (SVD)：奇异值分解 ：）</p><p>Eigen-Decomposition：特征分解</p><p>eigenvector：特征向量</p><p>eigenpair：特征值</p><p>Priciple Component Analysis (PCA) ：主成分分析</p><h3 id="SVD-Singular-Value-Decomposition"><a href="#SVD-Singular-Value-Decomposition" class="headerlink" title="SVD (Singular Value Decomposition)"></a>SVD (Singular Value Decomposition)</h3><p>奇异值分解是一种提取特征值的很好的方法。公式如下：</p><p><img src="/Users/max/Desktop/屏幕快照 2018-10-10 下午6.01.35.png" alt="屏幕快照 2018-10-10 下午6.01.35"></p><p>A可以通过这样的方式分解，而当：rank（A）= r时，有如下：</p><p><img src="/Users/max/Desktop/屏幕快照 2018-10-10 下午8.30.45.png" alt="屏幕快照 2018-10-10 下午8.30.45"></p><p>U：左奇异向量</p><p>Σ：奇异值</p><p>V：右奇异向量</p><p>这个总是成立的如果U,V是正交矩阵并且Σ为对角矩阵并且所有元素降序非负</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;课上讲了常见的数据降维的方法PCA，感觉比较有用，再结合一下网上看到的比较新的方法，总结在这里吧。&lt;/p&gt;
&lt;h3 id=&quot;常见的词汇&quot;&gt;&lt;a href=&quot;#常见的词汇&quot; class=&quot;headerlink&quot; title=&quot;常见的词汇&quot;&gt;&lt;/a&gt;常见的词汇&lt;/h3&gt;&lt;p&gt;最
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>流数据挖掘算法</title>
    <link href="http://yoursite.com/2018/09/29/%E6%95%B0%E6%8D%AE%E6%B5%81%E6%8C%96%E6%8E%98/"/>
    <id>http://yoursite.com/2018/09/29/数据流挖掘/</id>
    <published>2018-09-29T14:06:01.697Z</published>
    <updated>2018-10-01T09:05:18.915Z</updated>
    
    <content type="html"><![CDATA[<p>流数据，顾名思义，就是针对流数据（例如google查询、twitter的状态更新这类数据）的一些挖掘。不同于普通数据集，流数据源源不断地产生，而我们的存储空间是有限的。所以要采取一些办法，：</p><ul><li>Samping data from stream</li><li>Queries over sliding windows</li><li>Filtering a data streaming</li></ul><h3 id="Sampling-data-from-stream"><a href="#Sampling-data-from-stream" class="headerlink" title="Sampling data from stream"></a>Sampling data from stream</h3><p>由于我们不能存储整个stream，所以要存一些sample，具体有两种方式</p><ol><li>Sample一个stream的固定比例</li><li>维护一个固定大小的随机Sample —— 蓄水池采样</li></ol><h4 id="固定比例"><a href="#固定比例" class="headerlink" title="固定比例"></a>固定比例</h4><p>以用户的查询为例：假设有s个单次查询，有d个两次查询，即总共有s+2d次查询，sample率为p（0&lt;p&lt;=1），求用户两次查询的比例</p><p>正确答案：d/(s+d)</p><p>按照条件推倒：dp^2/(sp+dp^2+2p(1-p)d)</p><p>这个准确率问题还是比较大的，为了改良，可以采用以取用户sample的方式。</p><h4 id="固定大小的sample-——-蓄水池采样"><a href="#固定大小的sample-——-蓄水池采样" class="headerlink" title="固定大小的sample —— 蓄水池采样"></a>固定大小的sample —— 蓄水池采样</h4><p>很好玩的问题：假如有一本非常厚的电话簿，你要随机挑1000个人打电话，并且保证每个人被选中的几率相等，你要怎么做？</p><p>这里的电话簿就像数据流，你不能先数一遍总共有多少人，再产生一千个随机数这样，效率太低。</p><p><strong>解决方法：</strong></p><p>先选前1000人，然后后面的每一位以k/n的概率决定替换掉前面的任意一位。其中n是当前总共人数，k是蓄水池大小。例如第1001位就有1000/1001的概率替换掉前面1000个人中任意一个。</p><p><strong>证明：</strong></p><p>条件: sample size: k，total size: n，probability of each element: s/n</p><p>求证: 对于n-&gt;n+1的情况，probability of each element: k/n+1，即无论n多大，每个元素被选中概率均等</p><p>证明: </p><p>每个元素被选中概率：k/n</p><p>n-&gt;n+1后，每个被替换前面概率：k/n+1</p><p>而假设A而原来在sample里的，仍然还在的概率为: (1-k/n+1) + (k/n+1)*(k-1/k) = n/n+1，这个公式前者是n+1这个元素没被选中，后者是没替换自己的概率</p><p>那么A的概率就是：k/n * n/n+1 = k/n+1</p><p>综上，概率相等～</p><h3 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a>Sliding Windows</h3><p>就是滑动窗口，简单来说：最多储存N bits,当又来一个新bit时，吐出第N+1个bits（也就是窗口里最老的那个）。</p><p>问题：如果我们要统计N中有一个1，那必须全部存储N中的内容，但窗口的存储空间有限，因此这里用了DGIM方法-&gt;exponential Window。</p><h4 id="DGIM算法"><a href="#DGIM算法" class="headerlink" title="DGIM算法"></a>DGIM算法</h4><p>它是通过Bucket来对滑动窗口进行划分，每个桶包括：</p><ol><li>桶最右边的timestamp（O(logN) bits）</li><li>在每个桶中1刀数量（O(log logN)bits）</li></ol><p>桶的限制：每个桶里的1的数量是2的次方。</p><p>桶的特征：</p><ul><li>只有1个或者2个桶有相同数目的2的次方个1</li><li>桶和桶的timestamps不会重叠</li><li>桶是按照大小有序的</li><li>当桶的end-time &gt; N的时候（即整个桶已经从窗口中走了），桶会消失</li></ul><p><img src="/img/DGIMbucket.png" alt="DGIMbucket"></p><p>  DGIM算法中数据结构的更新（2048式更新）：</p><ol><li>每一个新的位进入滑动窗口后，最左边一个位从窗口中移出（同时从桶中移出）；如果最左边的桶的时间戳是当前时间戳减去N（也就是说桶里已经没有处于窗口中的位），则放弃这个桶；</li><li>对于新加入的位，如果其为0，则无操作；否则建立一个包含新加入位的大小为1的桶；</li><li>由于新增一个大小为1的桶而出现3个桶大小为1，则合并最左边的两个桶为一个大小为2的桶；合并之后可能出现3个大小为2的桶，则合并最左边两个大小为2的桶得到一个大小为4的桶……依次类推直到到达最左边的桶。</li></ol><p>如何估计最新N窗口中1的数目呢？</p><ul><li>将除了最后一个的所有的bucket大小（这里大小都是指1的数目）加起来</li><li>再加上最后一个bucket的一半大小</li></ul><p>准确率：&gt;= 50%</p><h3 id="Filtering-Data-Streams"><a href="#Filtering-Data-Streams" class="headerlink" title="Filtering Data Streams"></a>Filtering Data Streams</h3><p>问题：垃圾邮件的过滤问题，我们知道有10亿个好的email address，如果邮件来自这些address，那么它就不是spam。即现在有一封邮件{“address”:”contents”}，如果key可以匹配10亿email address就不是spam。</p><h4 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h4><p>设S是刚刚好的email address集合，而B是大小为n，全部初始化为0的一个bit array</p><p><img src="/img/initBFbits.png" alt="initBFbits"></p><p>使用k个相互独立的hash函数h1,h2……hk，他们可以将元素映射到[0,n]的bit array上。将S中元素通过k个hash函数映射，映射到的地方设置为1</p><p><img src="/img/findBFbits.png" alt="findBFbits"></p><p>这里的x1，x2就是S的元素。</p><p>当判断一个元素y1，y2是否在集合S中时，就通过相同的hash函数将其映射到B上，如果任何一个hash函数映射到的地方存在0，那y就不在S中，反之则证明在S上，或者是一个false positive（将错的分到对的里）。</p><p><img src="/img/hashedBFbits.png" alt="hashedBFbits"></p><p>如图y1就不在S中，y2或是在S中，或是一个false positive。但这种方法不存在true negative（将对的分到错的里）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;流数据，顾名思义，就是针对流数据（例如google查询、twitter的状态更新这类数据）的一些挖掘。不同于普通数据集，流数据源源不断地产生，而我们的存储空间是有限的。所以要采取一些办法，：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Samping data from stream&lt;/li
      
    
    </summary>
    
    
      <category term="data mining" scheme="http://yoursite.com/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB使用整理</title>
    <link href="http://yoursite.com/2018/09/25/MongoDB%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/25/MongoDB使用整理/</id>
    <published>2018-09-25T06:14:39.539Z</published>
    <updated>2018-11-11T06:56:12.499Z</updated>
    
    <content type="html"><![CDATA[<p>最近project需要mongo做存储，因此在这里总结一些mongo的东西</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Mongo是一个开源的文档型数据库，它提供了高性能、高可用和自动扩展的特性。</p><p>概念对应关系如下表：</p><table><thead><tr><th>SQL术语/概念</th><th>MongoDB术语/概念</th><th>解释/说明</th></tr></thead><tbody><tr><td>database</td><td>database</td><td>数据库</td></tr><tr><td>table</td><td>collection</td><td>数据库表/集合</td></tr><tr><td>row</td><td>document</td><td>数据记录行/文档</td></tr><tr><td>column</td><td>field</td><td>数据字段/域</td></tr><tr><td>index</td><td>index</td><td>索引</td></tr><tr><td>table joins</td><td></td><td>表连接,MongoDB不支持</td></tr><tr><td>primary key</td><td>primary key</td><td>主键,MongoDB自动将_id字段设置为主键</td></tr><tr><td>aggregation(group by)</td><td>aggregation pipeline</td><td>聚合函数</td></tr><tr><td>transactions</td><td>transactions</td><td>事物</td></tr></tbody></table><p>mongo支持包括mongo shell、c++、java、python、php等几乎所有主流语言。只需要下好对应的driver即可，下面的操作我们使用pymongo来完成。</p><h2 id="CURD操作"><a href="#CURD操作" class="headerlink" title="CURD操作"></a>CURD操作</h2><p>CURD是数据库基本操作。这里用pymongo来创建mongo client。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"></span><br><span class="line"><span class="comment">#首先要创建一个mongo client来运行mongo instance</span></span><br><span class="line">client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#选中需要操作的database（test）和collection（inventory）</span></span><br><span class="line">inventory = client[<span class="string">'test'</span>][<span class="string">'inventory'</span>]</span><br></pre></td></tr></table></figure><h3 id="Read操作"><a href="#Read操作" class="headerlink" title="Read操作"></a>Read操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result = collection.find_one(&#123;filter&#125;)</span><br><span class="line"></span><br><span class="line">results = collection.find(&#123;filter&#125;).limit(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">values = inventory.find(&#123;<span class="string">"type"</span>: &#123;<span class="string">"$regex"</span>: <span class="string">"Event"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><h3 id="Update操作"><a href="#Update操作" class="headerlink" title="Update操作"></a>Update操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">collection.update_one(&#123;filter&#125;,&lt;&#123;$set&#125;&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">inventory.update_one(&#123;<span class="string">"id"</span>: <span class="string">"2614896652"</span>&#125;, &#123;<span class="string">"$set"</span>: &#123;<span class="string">"type"</span>: <span class="string">"CreateEvent"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><h3 id="Create操作"><a href="#Create操作" class="headerlink" title="Create操作"></a>Create操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在这里我读取一个json文件，然后存入数据库中</span></span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"/data/test.json"</span>, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> f.readlines():</span><br><span class="line">        data.append(json.loads(content))</span><br><span class="line"></span><br><span class="line">inventory.insert_many(data)</span><br></pre></td></tr></table></figure><h3 id="Delete操作"><a href="#Delete操作" class="headerlink" title="Delete操作"></a>Delete操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">collection.delete_many(&#123;filter&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">inventory.delete_many(&#123;&#125;) <span class="comment">#删除全部记录</span></span><br></pre></td></tr></table></figure><h3 id="Bulk-Write操作"><a href="#Bulk-Write操作" class="headerlink" title="Bulk Write操作"></a>Bulk Write操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> InsertOne,DeleteMany,UpdateOne</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一个参数数组为operations数组，ordered表示是否顺序执行</span></span><br><span class="line">inventory.bulk_write([</span><br><span class="line">    InsertOne(data[<span class="number">0</span>]),</span><br><span class="line">    UpdateOne(&#123;<span class="string">"id"</span>: <span class="string">"2614896652"</span>&#125;, &#123;<span class="string">"$set"</span>: &#123;<span class="string">"type"</span>: <span class="string">"PushEvent"</span>&#125;&#125;),</span><br><span class="line">    DeleteMany(&#123;&#125;)</span><br><span class="line">], ordered=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Text-Search操作"><a href="#Text-Search操作" class="headerlink" title="Text Search操作"></a>Text Search操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#mongo提供了针对文本内容search的方法，要想使用这个就要先对对应field创建text index</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建text index</span></span><br><span class="line">db.inventory.createIndex(&#123;<span class="string">"type"</span>:<span class="string">"text"</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#文本搜索</span></span><br><span class="line">values = inventory.find(&#123;<span class="string">"$text"</span>: &#123;<span class="string">"$search"</span>: <span class="string">"CreateEvent"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><h2 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h2><p><img src="/img/replica.png" alt="replica"></p><p>一组replica只指存相同数据的节点集。其中只有一个被认为是primary节点，其他是secondary节点（类似于master-slave）。</p><p>只有primary节点可以接受所有的写操作，并且通过{w:”majority”}来确认通知。同时primary会把所有数据操作放在oplog里。</p><p>对于secondary节点，他们会备份primary节点的oplog，然后把log里的东西apply到自己的节点上，这样primary上的dataset也就映射到了secondary上。当primary不可用时，secondaries会通过选举决定谁是primary。</p><p><img src="/img/secondaySelection.png" alt="secondaySelection"></p><p>你可以额外增加一个叫arbiter的节点，arbiter不维护任何数据，它通过应答其他节点发出的心跳和选举请求，维护了投票时需要的法定人数。因为它本身不维护dataset，所以它占用的资源很少，也不要单独硬件。所以当节点数为偶数时，可以增加一个arbiter来维护。</p><p><img src="/img/arbiter.png" alt="arbiter"></p><h3 id="Asynchronous-Replication"><a href="#Asynchronous-Replication" class="headerlink" title="Asynchronous Replication"></a>Asynchronous Replication</h3><p>secondary本身异步地从primary复制。完成备份后，即使。</p><h4 id="Automatic-Failover"><a href="#Automatic-Failover" class="headerlink" title="Automatic Failover"></a>Automatic Failover</h4><p>当primary在一定时间（configured by electionTimeoutMillis，10s default）不与secondary通信时，有资格的secondary会提名自己参加选举，cluster会试图完成选举并恢复正常功能。</p><p>在选举阶段，这个replica set都无法进行写操作，个别secondary如果配置了允许读操作的话还是正常进行的。通常系统会在12s只能判定一个primary不可用并完成选举工作。</p><h2 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h2><p>Sharding是Mongo采用的一种在多台机器上分布数据的方法。对于在大量、高吞吐率的数据在单台服务器上无法承载，有两种方法来扩展机器：</p><ul><li>Vertical Scaling：增加单机的容量、内存，使用更好的cpu等方式。但这种受限于目前可用的技术能力。</li><li>Horizontal Scaling：增加机器的数量，每个机器只要handle一部分数据。比较便宜，但维护和部署基础设施成本较高。</li></ul><p>Sharding cluster包含一下组件：</p><ul><li>Shard：每个shard包含总的一个子数据集。每个shard都可以被部署为备用数据集。</li><li>mongos：mongos作为query router，提供了client和sharded cluster之间的借口。</li><li>config servers：储存了metadata和cluster的配置设置。在mongo3.4，其必须被部署为一个备用数据集（CSRS）。</li></ul><p>关系组图如下：</p><p><img src="/img/sharded cluster.png" alt="sharded cluster"></p><p>MongoDB从collection层面来分割数据。</p><h3 id="Shard-Keys"><a href="#Shard-Keys" class="headerlink" title="Shard Keys"></a>Shard Keys</h3><p>MongoDB是根据shard key来分割一个collection的。shard key是由目标collection中每个document都有的fields或者不变的field组成的。</p><p>在分割collection时要选择shard key，分割完后shard key就不能再改变了。</p><p>对于非空collection，必须有一个index是以shard key开头的；而对于空collection，MongoDB则会创建一个index。</p><p>shard keys的选择会影响sharded cluster的性能、效率、扩展性等问题，它和它背后的index也会影响你的集群可以使用的sharding策略。</p><h3 id="Chunks"><a href="#Chunks" class="headerlink" title="Chunks"></a>Chunks</h3><p>MongoDB会数据分成一个个的chunks，其中包含了左闭右开的shard key范围。Mongo使用shared cluster balancer来实现在sharded cluster上的不同shards间移动chunks。</p><h3 id="Sharded-and-Non-sharded-Collections"><a href="#Sharded-and-Non-sharded-Collections" class="headerlink" title="Sharded and Non-sharded Collections"></a>Sharded and Non-sharded Collections</h3><p>每个数据库都可以既有sharded collections又有un-sharded collections。unsharded collection存在数据库的primary shard上。每个数据库都有primary shard。</p><h3 id="Connecting-to-a-sharded-Cluster"><a href="#Connecting-to-a-sharded-Cluster" class="headerlink" title="Connecting to a sharded Cluster"></a>Connecting to a sharded Cluster</h3><p><img src="/img/shaded connection.png" alt="shaded connection"></p><p>用户通过mongos router来实现和sharded cluster（包含sharded和unsharded collections）的交互。client做读写操作时不能只连接一个shard。</p><h3 id="Sharding-Strategy"><a href="#Sharding-Strategy" class="headerlink" title="Sharding Strategy"></a>Sharding Strategy</h3><p>具体来说sharding的方式有两种：</p><h4 id="Hashed-Sharding"><a href="#Hashed-Sharding" class="headerlink" title="Hashed Sharding"></a>Hashed Sharding</h4><p>通过计算shard key的hash值来分到不同的chunk中。在使用hashed indexes来处理查询时，client不需要计算hash，这个由MongoDB自动完成。</p><p><img src="/img/hashed sharding.png" alt="hashed sharding"></p><p>对于那种很接近的shard key（例如单调递增的），hash通常会比较均匀地把他们分到不同的chunk里。这样存在个问题：当我要取一定范围内的数据时，需要从各个不同的chunk里取，造成较大范围的broadcast operations。</p><h4 id="Ranged-Sharding"><a href="#Ranged-Sharding" class="headerlink" title="Ranged Sharding"></a>Ranged Sharding</h4><p>把shard key的值分成一定范围区间，然后分配。</p><p><img src="/img/ranged sharding.png" alt="ranged sharding"></p><p>这样相近的shard key实在同一个chunk里的，这样mongos只需要把operation route到包含这些数据的shards。</p><p>这个方式很依赖shard key的选择，不理想的话可能造成数据分配不平衡。</p><h3 id="Zones-in-Sharded-clusters"><a href="#Zones-in-Sharded-clusters" class="headerlink" title="Zones in Sharded clusters"></a>Zones in Sharded clusters</h3><p>在sharded clusters里，可以把几个shard对应到一个zone中，一个shard也可以对应到多个zone里。chunks只在相同zone下的几个shards中转移。</p><p><img src="/img/zoneMongoDB.png" alt="zoneMongoDB"></p><p>zones存在的目的是提高在大的sharded clusters里数据的本地化程度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近project需要mongo做存储，因此在这里总结一些mongo的东西&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;Mongo是一个开源的文档型数据库
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>局部敏感哈希(LSH)——Locality Sensitive Hashing</title>
    <link href="http://yoursite.com/2018/09/19/%E5%B1%80%E9%83%A8%E6%95%8F%E6%84%9F%E5%93%88%E5%B8%8C/"/>
    <id>http://yoursite.com/2018/09/19/局部敏感哈希/</id>
    <published>2018-09-19T03:34:16.899Z</published>
    <updated>2018-09-19T05:12:12.981Z</updated>
    
    <content type="html"><![CDATA[<p>课上学了一种大数据计算相似性的算法，觉得很巧妙，在这里总结下。</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>求相似性的场景有很多，比如比较两图相似性从而做一些处理，或者搜索引擎比较文本相似性从而确定返回的搜索结果等等。这种场景常常面临的问题是要进行比较的数据特征量非常大，同时比较对象又非常多，不能直接比较。LSH正是解决了这个问题。</p><h2 id="LSH流程"><a href="#LSH流程" class="headerlink" title="LSH流程"></a>LSH流程</h2><p>LSH分为三步：</p><ol><li>Shingling: Convert documents to sets</li><li>Min Hashing: Convert large sets to short signatures, while preserving similarity</li><li>Locality Sensitive Hashing: Focus on pairs of signatures likely to b from similar documents - <strong>Candidate pairs!</strong></li></ol><p><img src="/img/LSHFlow.png" alt="LSHFlow"></p><p>要测相似性就要确定距离/相似性函数，LSH使用的是jaccard距离/相似性，具体如下：</p><ul><li>sim(C1,C2) =  | C1 ∩ C2 | / | C1 U C2 |</li><li>d(C1,C2) = 1 -  | C1 ∩ C2 | / | C1 U C2 |</li></ul><h3 id="第一步：Shingling"><a href="#第一步：Shingling" class="headerlink" title="第一步：Shingling"></a>第一步：Shingling</h3><p>Shingling是将整个文档转化为一个set，这里我们不重视文档顺序、重要词等信息，所以shingling是最好选择。</p><p>K-shingle（k-gram）则是将文档分成以k个token为一个单位的序列，token可以是字符、单词或者其他。例如k=2，则对于文档D = abcab，2-shingle的set则是：C = S(D) = {ab, bc, ca}。k的取值：</p><ul><li>k = 5，对于比较短的文章</li><li>k = 10，对于很长的文章</li></ul><p>随后可以将每一个shingle作为一个维度将所有文档联合起来构建一个0/1矩阵，这个 矩阵:</p><ul><li>Rows = elements (shingles)</li><li>Columns = set(documents)</li></ul><p>例如：</p><table><thead><tr><th></th><th>C1</th><th>C2</th><th>C3</th><th>C4</th></tr></thead><tbody><tr><td>Shingles1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>Shingles2</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>…</td><td>0</td><td>1</td><td>0</td><td>1</td></tr><tr><td>…</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>很显然这个矩阵非常稀疏的。这样计算量非常大，所以有第二步来“压缩矩阵”。</p><h3 id="第二步：Minhash"><a href="#第二步：Minhash" class="headerlink" title="第二步：Minhash"></a>第二步：Minhash</h3><p>minhash需要压缩矩阵，将原来100，000维的压缩到100维，采用了很聪明的方法：概率聚类大法！</p><p>具体描述：将上述矩阵每一列随机排列，然后记录第一个1的对应位置（位置标注方式不一定是递增序列，而和产生随机的方式有关）</p><p><img src="/img/minhash.png" alt="minhash"></p><p>每一次随机都会产生1个signature，100次随机就针对每个文档产生一个长度100的signature，文档之前只要互相比较一下这个signature的相似性就相当于比较文档的相似性。</p><p>这个算法的可行性用公式表达则是：</p><ul><li>Pr[ min( π (C1 )) = min ( π (C2 )) ] = sim(C1,C2)</li></ul><p>这个结论是可以证明的，此处略过。</p><p>由于这里要随机100次，这个显然是比较麻烦的，所以随机用hash方法来进行。</p><p>全局hash为：</p><ul><li>h_a,b(x) = (( a*x + b )mod p ) mod N</li></ul><p>然后每次只要随机产生a，b两个值就好。之后按照产生的hash函数的大小顺序进行排列。hash产生重复的值也不要紧，signature就是产生的这个hash值即可。</p><p>这里解决了维度太多的问题，但相互比较问题还是没解决，因为文档很多，所以要非常多次比较。第三步将减少比较流程，只要候选文档之间相互比较即可。</p><h3 id="第三步：Locality-Sensitive-Hashing"><a href="#第三步：Locality-Sensitive-Hashing" class="headerlink" title="第三步：Locality Sensitive Hashing"></a>第三步：Locality Sensitive Hashing</h3><p>这里先讲M矩阵分成了b个bands，每个bands里包含r行，如图：</p><p><img src="/img/LSHBand.png" alt="LshBand"></p><p>之后再对每个内容进行hash，使其map到k个buckets中，如果C1,C2有大于等于一个band在同一buckets中，则对他们进行比较，如图：</p><p><img src="/img/hashBand.png" alt="hashBand"></p><p>这样将每个bands都map到Buckets的不同块里，如果map到同一个buckets的块里，则称为candidate pair，他们不一定相等，要进行比较。但如果map到不同的块里，表明这两个band一定不同，不需要进行比较。</p><p>下面验证一下这个算法的可行性。</p><p>假设C1和C2的相似度为t，b为bands数目，r为每个band多少row，可以得知：</p><ul><li>band中所有row相等（该band会映射到同一bucket块）概率： t^r</li><li>band中row存在不相等row概率：1-t^r</li><li>没有一个band是相等的概率：(1 - t^r)^b</li><li>至少有一个band相等的概率：1 - (1 - t^r)^b</li></ul><p>按照公式计算，当b取20，r取5，对于相似度30%的文档C1,C2（不希望有bands出现在同一buckets中），有4.7%的几率至少一个bands分在同一buckets中，我们花多余的代价去计算它概率比较小。而相对于相似度80%的文档，有99.96%的几率我们要去比较他们，我们会错过一对相似文档的概率不到0.04%，可以接受。</p><p>综上改算法是可行的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;课上学了一种大数据计算相似性的算法，觉得很巧妙，在这里总结下。&lt;/p&gt;
&lt;h2 id=&quot;使用场景&quot;&gt;&lt;a href=&quot;#使用场景&quot; class=&quot;headerlink&quot; title=&quot;使用场景&quot;&gt;&lt;/a&gt;使用场景&lt;/h2&gt;&lt;p&gt;求相似性的场景有很多，比如比较两图相似性从而做
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>图说mysql的四种join</title>
    <link href="http://yoursite.com/2018/09/14/%E5%9B%BE%E8%AF%B4mysql%E7%9A%84%E5%9B%9B%E7%A7%8Djoin/"/>
    <id>http://yoursite.com/2018/09/14/图说mysql的四种join/</id>
    <published>2018-09-14T02:58:13.621Z</published>
    <updated>2018-09-15T02:19:20.596Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/img/rightJoin.png" alt="屏幕快照 2018-09-14 上午10.54.59"></p><p><img src="/img/leftJoin.png" alt="屏幕快照 2018-09-14 上午10.55.09"></p><p><img src="/img/innerJoin.png" alt="屏幕快照 2018-09-14 上午10.55.39"></p><p><img src="/img/crossJoin.png" alt="屏幕快照 2018-09-14 上午10.55.46"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/img/rightJoin.png&quot; alt=&quot;屏幕快照 2018-09-14 上午10.54.59&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/leftJoin.png&quot; alt=&quot;屏幕快照 2018-09-14 上午10.55.09&quot;&gt;&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce终极整理</title>
    <link href="http://yoursite.com/2018/09/13/MapReduce%E7%BB%88%E6%9E%81%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/13/MapReduce终极整理/</id>
    <published>2018-09-13T06:33:26.110Z</published>
    <updated>2018-09-15T02:14:03.928Z</updated>
    
    <content type="html"><![CDATA[<p>这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。</p><p>首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。</p><h3 id="使用MapReduce的目的"><a href="#使用MapReduce的目的" class="headerlink" title="使用MapReduce的目的"></a>使用MapReduce的目的</h3><p>用一个工具只有知道他的目的和优点，才能找到最合适的使用场景。简单来说，mapreduce提供了一个自动并行和分布式计算的工具（接口），在大规模集群中性能出众。由于它隐藏了这些分布式系统的细节，所以很多不懂分布式的程序员也可以基于此搭建分布式系统。对于一些web服务器、爬虫、排序算法、机器学习、数据挖掘等都很有用。</p><h3 id="MapReduce的架构和运行"><a href="#MapReduce的架构和运行" class="headerlink" title="MapReduce的架构和运行"></a>MapReduce的架构和运行</h3><p>这是本文的重点，不多说，直接上图。</p><p><img src="/img/mapreduce.png" alt="mapreduce"></p><p>这幅图是直接从论文截下来的，生动的讲述了mapreduce的整个流程：</p><ol><li>将用户input文件主动分割成成16-64MB大小的块，然后在集群中做一些备份（一般备份3份，其中两份在同一个机架上）</li><li>master发挥作用，将选择一些idle的worker给其分配map或者reduce任务。</li><li>这时候worker就会去读那些被分割的文件，将一个key/value对读取到用户定义的map函数中。worker在读取这些文件时，会优先读取本地存着的，如果本地没有，选取离它最近的文件，减少网络传输代价。</li><li>map函数会产生一些中间过程数据，然后周期性的通过buffer将它存在本地磁盘上，然后将地址、文件名等信息通知master，这些中间文件通过hash(key) mod R的方式被分成R份（R是reduce worker的数量），master就会将这些块分配给对应的reducer。</li><li>reducer会通过RPC方式读取到这些中间过程数据，然后进行一个排序（shuffle），这个shuffle会让相同value聚在一起。这个很必要，因为不同key会映射到同一个reduce任务上。</li><li>只有在map全部结束后，reduce才会开始。在reducer中处理这个有序数据时，遇到相同key就会把value放在用户定义的reduce函数中。最终reduce函数会把结果文件输出到这个reduce partition中。</li></ol><p>最终得到的结果其实就是part-r-00000这样形式的不同文件。用户没必要把output文件最后聚在一起，如果需要的话，这个结果还可以作为下一轮mapreduce的输入。</p><h3 id="在有Combiner后过程的改良"><a href="#在有Combiner后过程的改良" class="headerlink" title="在有Combiner后过程的改良"></a>在有Combiner后过程的改良</h3><p>上面的过程是论文中解释的，但现在的程序都用了Combiner，Combiner其实和reduce的代码是一样的，那他是做什么用的呢？</p><p>Combiner其实是针对本地的map后的结果进行pre-reduce（或者叫mini-reduce），例如wordcount在map之后大概是(‘a’:1),(‘b’,1),(‘a’,1),(‘c’,1)这样，combiner将local的这些先做一次reduce，变成(‘a’,2),(‘b’,1),(‘c’,1)，之后再去shuffle和reduce。</p><p>综上mapreduce整个过程分为四步：</p><p>​                              <strong>Map -&gt; Combiner -&gt; Shuffle -&gt; Reduce</strong></p><h3 id="Master节点的任务和结构"><a href="#Master节点的任务和结构" class="headerlink" title="Master节点的任务和结构"></a>Master节点的任务和结构</h3><p>对于每个map任务或者reduce任务，都分成三种状态：idel, in-progress, completed。每个任务的这些状态都存在master节点上。</p><p>Master节点是作为map任务和reduce任务通信的管道的。master要存储并更新M个map任务产生的M <em> R个块(每个map任务产生R个块)的位置信息、文件名等。即，master承担 O(M+R)个scheduling决策，存储 O(M </em> R)个状态信息。</p><p>M：map是的m块数据，R: reduce时的r块数据，W: worker机器的数量；三者关系为：</p><p>​                            <strong>M &gt; R &gt; W</strong></p><h3 id="Worker的容错问题"><a href="#Worker的容错问题" class="headerlink" title="Worker的容错问题"></a>Worker的容错问题</h3><p>首先，master出错的话，没啥好说的，直接告诉用户就成。</p><p>worker一旦出错，master应该要感知到，有两种策略感知：</p><ul><li><p>Pull模型 ：worker通过发送heartbeat给master，master在感知到heartbeat之后在heartbeat response里给worker分配任务。</p></li><li><p>Push模型：master一直ping worker，当一段时间ping不通后说明worker失败了。</p></li></ul><p>这个时候master会将worker的任务分配给其他的worker去执行。对于执行完的worker，状态会重新标定为idle，表示有资格接受任务。失败的worker也会被标定为idle，同样可以接受任务。</p><p>对于在失败的worker上completed的map任务，在其他worker上需要重新执行，因为他们存在本地的中间数据访问不到了。但对于在失败的worker上completed的reduce任务则不需要重新执行，因为他们的结果文件存在了global的文件系统下。</p><p>在执行mapreduce时对于一些<strong>straggler</strong>（落伍者），有两种处理方式：</p><ul><li>Job stealing: 对这个job进行分片，将没完成的部分交给其他的worker完成。</li><li>Speculative execution: master，这时候两个类似竞争的关系，当其中一个结束时，这个task会被标定为completed，同时另一个task将会被放弃。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。&lt;/p&gt;
&lt;p&gt;首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。&lt;/p&gt;
&lt;h3 id=&quot;使用MapRe
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>课程信息整理</title>
    <link href="http://yoursite.com/2018/09/13/%E8%AF%BE%E7%A8%8B%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/13/课程信息整理/</id>
    <published>2018-09-13T05:00:48.157Z</published>
    <updated>2018-09-13T08:19:32.862Z</updated>
    
    <content type="html"><![CDATA[<p>之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。</p><hr><h3 id="CMSC5741-Big-Data-Technology-and-Applications"><a href="#CMSC5741-Big-Data-Technology-and-Applications" class="headerlink" title="CMSC5741 - Big Data Technology and Applications"></a><a href="">CMSC5741 - Big Data Technology and Applications</a></h3><p>textbook: Mining of Massive Dataset（已借）</p><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/lyu/" target="_blank" rel="noopener">Prof. Michael R. Lyu</a></p><p>Tutor: Zeng Jichuan</p><p>exam: <strong>Nov.6 Midterm exam</strong> </p><p>Assessment Scheme and Deadlines:</p><ul><li>20% Assignment</li><li>40% Midterm examination</li><li>40% Project : Proposal, Presentation, Report</li></ul><p>Backgroud Knowledge: Tensorflow, Amazon EC2</p><hr><h3 id="CSCI5570-Large-Scale-Data-Processing-Systems"><a href="#CSCI5570-Large-Scale-Data-Processing-Systems" class="headerlink" title="CSCI5570 - Large Scale Data Processing Systems"></a><a href="http://www.cse.cuhk.edu.hk/~jcheng/5570/" target="_blank" rel="noopener">CSCI5570 - Large Scale Data Processing Systems</a></h3><p>website Account:</p><ul><li><p>Username : csci5570</p></li><li><p>Password : huskydatalab</p></li></ul><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~jcheng" target="_blank" rel="noopener">Prof. James CHENG</a> </p><p>Tutor: Tatiana Jin</p><p>Lecture/Lab: </p><ul><li>Tuesday 13:30 Lecture &amp;&amp; Lab</li><li>Wednesday 14:30 Lecture</li></ul><p>Assessment Criteria:</p><ul><li>30% Survey paper : select one topics (DDL: Dec 10, 2018)</li><li>70% project : deadline: DEC 20</li></ul><hr><h3 id="CMSC5724Data-Mining-and-Knowledge-Discovery"><a href="#CMSC5724Data-Mining-and-Knowledge-Discovery" class="headerlink" title="CMSC5724Data Mining and Knowledge Discovery"></a><a href="http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/18-fall/" target="_blank" rel="noopener">CMSC5724Data Mining and Knowledge Discovery</a></h3><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~taoyf/" target="_blank" rel="noopener">Yufei Tao</a></p><p>Tutor: Shangqi Lu</p><p>Assessment Criteria:</p><ul><li>30% Project</li><li>30% Short Tests (three times in class)</li><li>40% Final (Open-book)</li></ul><hr><h3 id="CMSC-5720-Project-I"><a href="#CMSC-5720-Project-I" class="headerlink" title="CMSC 5720 - Project I"></a>CMSC 5720 - Project I</h3><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~jcheng" target="_blank" rel="noopener">Prof. James CHENG</a> </p><p>Options:</p><ol><li>NN-descent （kn-graph的近似算法）</li><li>search with fa2ss（facebook的相似性检索库）</li><li>multiprobe with tree（基于树的哈希方法）</li><li>LSH for MZPS （lsh）</li><li>数据收集-&gt;存储-&gt;分析 系统</li><li>topic modeling on ps archetective -&gt; （LDA FlexPS-&gt;parameter server拓展）</li><li>调度算法，同步/异步 任务，在不同集群下测试算法，分布式，任务的表现</li><li>矩阵分解 Distributed MF（矩阵分解） on Actor Framework （nomad,lftf acf或者akka -&gt; cpu to gpu to scheduling）</li><li>Clustering-aware query (database query optimizer)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;CMSC5741-Big-Data-Technology-and-Applications&quot;
      
    
    </summary>
    
    
      <category term="class note" scheme="http://yoursite.com/tags/class-note/"/>
    
  </entry>
  
  <entry>
    <title>地道的口语表达积累</title>
    <link href="http://yoursite.com/2018/09/01/%E5%9C%B0%E9%81%93%E7%9A%84%E5%8F%A3%E8%AF%AD%E8%A1%A8%E8%BE%BE/"/>
    <id>http://yoursite.com/2018/09/01/地道的口语表达/</id>
    <published>2018-09-01T07:40:05.605Z</published>
    <updated>2018-09-13T08:08:18.213Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-给别人加油"><a href="#1-给别人加油" class="headerlink" title="1. 给别人加油"></a>1. 给别人加油</h3><p>不要再用fighting啦～</p><p><strong>Go get‘em.</strong></p><p>完整说法：Go get’em tiger.</p><p>追女生：go get her</p><h3 id="2-问别人想不想要"><a href="#2-问别人想不想要" class="headerlink" title="2. 问别人想不想要"></a>2. 问别人想不想要</h3><p>不要再用do you want啦～</p><p><strong>be up for something</strong></p><p>e.g: </p><p>Are you up for going clubing?</p><p>你想去蹦迪吗？</p><p>be up三个意思：</p><ul><li><p>醒着 Are you up?</p></li><li><p>下一个 who’s up next?</p></li><li><p>怎么了 what’s up？</p></li></ul><h3 id="3-和朋友约出去玩"><a href="#3-和朋友约出去玩" class="headerlink" title="3. 和朋友约出去玩"></a>3. 和朋友约出去玩</h3><p>play with不行，一般是小朋友或者人和宠物，不适合成人一起玩</p><p>hang out</p><p>还可以用chill out，自己一个人出去玩</p><p>e.g.</p><p> Do you wanna hang out with us?</p><p>###4. 如何约一场“夜生活”</p><p><strong>night out～(名词)</strong></p><p>e.g. </p><p>It’s been a while since we had a girls’ night out.</p><p>姐妹们我们已经很久没有出去玩啦</p><p>have(need) a night out.</p><p>e.g.</p><p>If you are up for a boys’ night out~</p><p>如果想约不会太晚的夜生活用：evening out</p><p>Night owl 夜猫子</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-给别人加油&quot;&gt;&lt;a href=&quot;#1-给别人加油&quot; class=&quot;headerlink&quot; title=&quot;1. 给别人加油&quot;&gt;&lt;/a&gt;1. 给别人加油&lt;/h3&gt;&lt;p&gt;不要再用fighting啦～&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Go get‘em.&lt;/strong
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>daily life in HK 2</title>
    <link href="http://yoursite.com/2018/08/30/daily%20life%20in%20HK%202/"/>
    <id>http://yoursite.com/2018/08/30/daily life in HK 2/</id>
    <published>2018-08-30T01:45:38.758Z</published>
    <updated>2018-08-30T01:45:39.446Z</updated>
    
    <content type="html"><![CDATA[<p>Sigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can’t enjoy your life. While, I come here not for enjoy the postgraduate career but for open my mind and learn something useful.</p><p>Another feeling deeps me most is if I can’t speak english well, the embarrassment will rise upon my face. In the afternoon, I met with my project teacher about the future work. From what he said, I could speculate that the former students may make him disappointed. As for project, he gave us much freedom to do what we interested in based on our foundation.</p><p>At night, I hung out with Doc.Lin, and it is my first time to meet with old friends after I came to HK. We walked around 中环 and 香港岛, where we smell at the taste of dollars. What a fame of capitalism! Then we returned to 九龙 by ship and ate dinner there. Life of Doc.Lin seemed as relaxed and funny as the time in SAP. It’s admired by everyone, right?</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Sigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can’t enjoy your life. While, I com
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>daily life in HK  1</title>
    <link href="http://yoursite.com/2018/08/27/daily%20life%20in%20HK%20%201/"/>
    <id>http://yoursite.com/2018/08/27/daily life in HK  1/</id>
    <published>2018-08-27T10:44:40.328Z</published>
    <updated>2018-08-27T15:21:07.557Z</updated>
    
    <content type="html"><![CDATA[<p>Finally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog unregularly.</p><p>The journey from SZ to HK is tremendously hard for my. I took all my packages (more than 30kg, wooo~~~) comming in HK custom. At that time, I thought the most difficulty time had been gone though. I was wrong, cause it was just the start of my suffering. The long lines of bus station made me crying, at the same time, I realized that I forgot to take some change. Of course, as a freshman of HK, I have no HK card. So I had to beg all around. After two times of tranfer, I got to Sha Tin station.</p><p>From the introduction of my room, I knew the building I live is not far from the station. While, I made the mistake for twice. Unfortunately, It lies on a unknown mountain, so I was supposed to climb mountain with huge bags.</p><p>Finally, I got in my rooms and met with my new roommate, who is a programmer, too. He is a nice guy and I was looking forward to make more friends here.</p><p>What was worth to mention is we have a free dinner organized by our college, other guys in my table feel embarrassed to pack the left-overs so I took it all back. It solved my two-day meal problems.</p><p>Never be shamed with yourself, keep on  doing what you believe in and you will make it sooner or later!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Finally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog 
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>20180811日记</title>
    <link href="http://yoursite.com/2018/08/11/title%2020180811%E6%97%A5%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/08/11/title 20180811日记/</id>
    <published>2018-08-10T16:43:11.578Z</published>
    <updated>2018-09-04T02:25:56.218Z</updated>
    
    <content type="html"><![CDATA[<p>估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！</p><p>Flag～ X</p><p>Flag~</p><p>Flag~</p><p>Flag~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！&lt;/p&gt;
&lt;p&gt;Flag～ X&lt;/p&gt;
&lt;p&gt;Flag~&lt;/p&gt;
&lt;p&gt;Flag~&lt;
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>mac在nginx下部署php遇到的坑</title>
    <link href="http://yoursite.com/2018/08/09/mac%E5%9C%A8nginx%E4%B8%8B%E9%83%A8%E7%BD%B2php%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>http://yoursite.com/2018/08/09/mac在nginx下部署php遇到的坑/</id>
    <published>2018-08-08T16:43:26.827Z</published>
    <updated>2018-08-10T16:40:03.685Z</updated>
    
    <content type="html"><![CDATA[<p>受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php不太熟悉（之前都用xampp这类软件），所以花了一点时间才搞定。</p><p>首先要明白的是，nginx本身不能处理php，它只是一个web服务器，当前端请求php时，nginx需要把界面发给php解释器处理，然后把结果返回给前端。一般地，nginx是把请求发给fastcgi管理进程处理。如nginx中配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">location ~ \.php$ &#123;</span><br><span class="line">    root           html;</span><br><span class="line">    fastcgi_pass   127.0.0.1:9000;</span><br><span class="line">    fastcgi_index  index.php;</span><br><span class="line">    fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;#这里原来不是$document_root，搞得我很蒙，还好网上查到改好了，不然会报file not found</span><br><span class="line">    include        fastcgi_params;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以要启动一个fastcgi，这里就用到了php-fpm，它是一个php fastcgi管理器，只用于php语言（旧版php的要单独下php-fpm，我用的php-fpm已经集成了这个）。</p><p>这里有很多奇怪的问题。</p><p><strong>第一次运行php-fpm</strong></p><p>failed: 找不到/private/etc/php-fpm.conf文件，</p><p>Solution:但这个目录下有个php-fpm.conf.default的文件，所以cp了正确名字的新文件</p><p><strong>第二次远行php-fpm</strong></p><p>Failed: 找不到/usr/var/log/php-fpm.log </p><p>Solution：根本没有这个目录，到conf文件里改了但是没有效果，没办法我就通过下面的命令执行php-fpm(后面都用这个命令执行)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php-fpm --fpm-config /private/etc/php-fpm.conf  --prefix /usr/local/var</span><br></pre></td></tr></table></figure><p><strong>第三次运行php-fpm</strong></p><p>Failed: No pool defined. at least one pool section must be specified in config file</p><p>Solution：到/etc/php-fpm.d/ 目录下有文件www.conf.default，cp一份名为<a href="http://www.conf的文件" target="_blank" rel="noopener">www.conf的文件</a></p><p><strong>第四次运行php-fpm</strong></p><p>Failed：端口被占用</p><p>Solution：杀掉这个进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i tcp:9000#找到占用9000端口的进程号</span><br><span class="line">kill -9 port#杀！</span><br></pre></td></tr></table></figure><p><strong>第五次运行php-fpm</strong></p><p>成功！</p><p>##补充：</p><p>在nginx上配的时候又有所一点不同，在mac上php-fpm直接listen了9000端口，但在服务器上它listen了php7.0-fpm.sock但socket文件，这种方式可能快一点，所以要在nginx上php的配置那边将</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastcgi_pass 127.0.0.1:9000;</span><br></pre></td></tr></table></figure><p>改成：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastcgi_pass unix:/run/php/php7.0-fpm.sock;</span><br></pre></td></tr></table></figure><p>才能成功运行php</p><h3 id="继续补充"><a href="#继续补充" class="headerlink" title="继续补充"></a>继续补充</h3><p>很有意思的一个东西，要上传27m的一个视频，nginx直接报了413 Request Entity Too Large，是我没设置…</p><p>到nginx的配置（set-enabled/default）里面添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    ...</span><br><span class="line">    client_max_body_size 80m;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重读配置、重启服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx -s reload</span><br><span class="line">service nginx restart</span><br></pre></td></tr></table></figure><p>然后还要去修改php.ini，在其中修改两条配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">upload_max_filesize = 80M</span><br><span class="line">post_max_size = 80M</span><br></pre></td></tr></table></figure><p>然后关掉php-fpm的进程，再重启即可～</p><p>ps：贺老师真的完全不研究的…mp4传不上去只是在系统里没添加这种类型，这种事都要我自己去找…难受 :(</p><p><strong>note：</strong>在ubuntu下现在比较推荐用apt而不是apt-get…so，是时候改变了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php
      
    
    </summary>
    
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
  </entry>
  
  <entry>
    <title>看Husky的一点整理</title>
    <link href="http://yoursite.com/2018/08/08/Husky%E6%96%87%E6%A1%A3%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/08/Husky文档整理/</id>
    <published>2018-08-08T02:34:42.580Z</published>
    <updated>2018-09-04T06:49:23.084Z</updated>
    
    <content type="html"><![CDATA[<p>Username : csci5570</p><p>Password : huskydatalab</p><p>husky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受…）</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> Required</span><br><span class="line">master_host=master#master跑的地方</span><br><span class="line">master_port=10086#master绑定的端口</span><br><span class="line">comm_port=12306#worker绑定的端口</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Worker information</span><br><span class="line">[worker]</span><br><span class="line">info=worker1:4#worker1有4个线程</span><br><span class="line">info=worker2:4#worker2有4个线程</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>如果用了hdfs，配置hdfs路径</span><br><span class="line">hdfs_namenode=master</span><br><span class="line">hdfs_namenode_port=9000</span><br></pre></td></tr></table></figure><p>运行的时候用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./program --conf=/path/to/config.ini</span><br></pre></td></tr></table></figure><p>写入配置。</p><h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><h3 id="Object-List"><a href="#Object-List" class="headerlink" title="Object List"></a>Object List</h3><p>Object List（objList）是husky中最主要的对象，可以把任何对象都存在objlist中，两个objlist通过channel传递消息。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Obj</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> KeyT = <span class="keyword">int</span>;</span><br><span class="line">    KeyT key;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> KeyT&amp; <span class="title">id</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">    Obj() = <span class="keyword">default</span>;</span><br><span class="line">    explicit Obj(const KeyT&amp; k) : key(k) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>创建一个obj只要3步：</p><ol><li>定义一个key的类型（keyT），一般都用int</li><li>写一个id（）函数来返回该对象对应的key</li><li>需要一个默认构造函数，还需要一个能够接收key参数的构造函数</li></ol><p>接下来就可以创建、使用、删除Object List</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建名叫my_objlist的objlist</span></span><br><span class="line"><span class="keyword">auto</span>&amp; objlist = ObjListStore::create_objlist&lt;Obj&gt;(<span class="string">"my_objlist"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将obj传入创建好的objlist中</span></span><br><span class="line"><span class="function">Obj <span class="title">obj</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line">objlist.add_object(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过名字拿到对应的objlist，注意这里的auto关键字，自动判定类型，很舒服！</span></span><br><span class="line"><span class="keyword">auto</span>&amp; objlist2 = ObjListStore::get_objlist&lt;Obj&gt;(<span class="string">"my_objlist"</span>);  </span><br><span class="line"></span><br><span class="line"><span class="comment">//通过名字删除objlist</span></span><br><span class="line">ObjListStore::drop_objlist(<span class="string">"my_objlist"</span>);</span><br></pre></td></tr></table></figure><p>为了让添加在objlist中的obj被其他线程感知并利用，在多线程情况下需要将objlist全局化一下，husky已经封装好该方法</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">globalize(objlist);</span><br></pre></td></tr></table></figure><p>接下来就是<strong>最重要</strong>的一个函数list_execute（），它规定了list里的每个object需要做的事，这个函数是用户自己定义的。它有两个参数：</p><ul><li>第一个是要操作的objlist</li><li>第二个是这个objlist中每个obj要做的事，例如下面函数就是obj在log中打印id，包括之后用channel发送或接收消息也都是在这个函数里</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list_execute(objlist, [](Obj&amp; obj) &#123;</span><br><span class="line">    base::log_msg(<span class="string">"My id is: "</span> + obj.id());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>channel就是object和object互相通信的工具，他们的关系类似于城市和公路。husky中有四种channel：</p><ul><li>Push Channel：最常见的点对点通信</li><li>Push Combined Channel：在push channel基础上增加了合并发给同个obj的</li><li>Broadcast Channel：将一个key-value广播出去，任何地方都可以通过key拿到值</li><li>Migrate Channel：用来migrate对象，将一个对象发送到另一个线程上</li></ul><p>channel的创建、使用和drop（一定要主动销毁）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create PushChannel</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MsgT, <span class="keyword">typename</span> DstObjT&gt; </span><br><span class="line"><span class="keyword">static</span> PushChannel&lt;MsgT, DstObjT&gt;&amp; </span><br><span class="line">create_push_channel(ChannelSource&amp; src_list,</span><br><span class="line">                    ObjList&lt;DstObjT&gt;&amp; dst_list,</span><br><span class="line">                    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name = <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get PushChannel through name</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MsgT, <span class="keyword">typename</span> DstObjT&gt;</span><br><span class="line"><span class="keyword">static</span> PushChannel&lt;MsgT, DstObjT&gt;&amp; </span><br><span class="line">get_push_channel(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name = <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Drop channel through name</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">drop_channel</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name)</span></span>;</span><br></pre></td></tr></table></figure><p>下面通过例子来说明：</p><p>首先，要想创建channel，就要确定发消息的源objlist和目的objlist，当然，参数里的目的objlist必须是全局化的</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个push_channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ch = ChannelStore::create_push_channel&lt;<span class="keyword">int</span>&gt;(src_list, dst_list);</span><br></pre></td></tr></table></figure><p>一般来说，channel是放在list_execute（）函数里用的，要想清楚从哪个obj发，发什么，哪个obj接收（通过key来标注）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//push channel</span></span><br><span class="line"><span class="comment">//发送端代码</span></span><br><span class="line">list_execute(src_list, [&amp;ch](Obj&amp; obj) &#123;</span><br><span class="line">    ch.push(msg, key);  <span class="comment">// send msg to key</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//接收端代码</span></span><br><span class="line">list_execute(dst_list, [&amp;ch](Obj&amp; obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span>&amp; msgs = ch.get(obj); <span class="comment">// The msgs is of type std::vector&lt;MsgT&gt;, MsgT is int in this case</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//broadcast channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ch4 = ChannelStore::create_broadcast_channel&lt;<span class="keyword">int</span>, <span class="built_in">std</span>::<span class="built_in">string</span>&gt;(src_list);</span><br><span class="line">list_execute(src_list, [&amp;ch4](Obj&amp; obj) &#123;</span><br><span class="line">    ch4.broadcast(key, value);  <span class="comment">// broadcast key, value pair</span></span><br><span class="line">&#125;);</span><br><span class="line">list_execute(src_list, [&amp;ch4](Obj&amp; obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span> msg = ch4.get(key);   <span class="comment">// get the broadcasted value through key.</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>###Aggregator</p><p>用来执行一些聚合操作的类，可以用来做求前k大值，统计数量，计算机器学习梯度总数等。他的构造函数需要两个参数（或以上），一个是init值，另外是lambda函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Aggregator&lt;<span class="keyword">int</span>&gt; agg(<span class="number">0</span>, [](<span class="keyword">int</span>&amp; a, <span class="keyword">const</span> <span class="keyword">int</span>&amp; b)&#123; a += b; &#125;);</span><br></pre></td></tr></table></figure><p>这个lambda函数就是aggregate的规则。</p><p>在创建完agregator后，就要使用它了。可以用update函数或者update_any函数（比update可接受参数类型多）来进行aggregator，例如</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agg.update(<span class="number">1</span>);<span class="comment">//aggregator值加1</span></span><br></pre></td></tr></table></figure><p>在聚合完之后，更新的值其实只在本地，为了让这个值在全局响应要用HuskyAggregatorFactory::sync()函数。另一种方式是通过HuskyAggregatorFactory::get_channel()来拿到通道，然后在list_execute中通过这个channel把消息传播出去，这种方法最后也会去调用sync()函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//两种方式</span></span><br><span class="line">AggregatorFactory::sync();</span><br><span class="line"></span><br><span class="line"><span class="comment">// or using aggregator channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ac = AggregatorFactory::get_channel();</span><br><span class="line">list_execute(obj_list, &#123;&#125;, &#123;&amp;ac&#125;, [&amp;](OBJ&amp; obj) &#123; </span><br><span class="line">  ...  <span class="comment">// here we can give updates to some aggregators</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>当全局划这个聚合之后，就可以用get_value()函数得到值了</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> sum = agg.get_value()</span><br></pre></td></tr></table></figure><p>这个值是被全局共享的，所以对他的修改会影响其他executor，并可能有线程安全问题</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Username : csci5570&lt;/p&gt;
&lt;p&gt;Password : huskydatalab&lt;/p&gt;
&lt;p&gt;husky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受…）&lt;/p&gt;
&lt;h2 id=&quot;配置&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="CUHK" scheme="http://yoursite.com/tags/CUHK/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop了解一下</title>
    <link href="http://yoursite.com/2018/08/05/hadoop%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B/"/>
    <id>http://yoursite.com/2018/08/05/hadoop了解一下/</id>
    <published>2018-08-05T14:20:47.264Z</published>
    <updated>2019-01-13T04:23:05.133Z</updated>
    
    <content type="html"><![CDATA[<p>搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。</p><h3 id="map-reduce"><a href="#map-reduce" class="headerlink" title="map reduce"></a>map reduce</h3><ul><li>automatic parallezation</li><li>Fault tolerance</li><li>a clean abstraction for programmers</li></ul><p>BSP model : Bulk sychronous parallel</p><p>identity reducer？re</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Apache-Hadoop与HDFS"><a href="#Apache-Hadoop与HDFS" class="headerlink" title="Apache Hadoop与HDFS"></a>Apache Hadoop与HDFS</h3><p>Hadoop是一个大的生态系统，最主要是hdfs和一个基于mapreduce的分布式计算引擎。hdfs就是一个文件系统，在mapreduce的时候（包括用spark的时候）都需要对应文件在hdfs里。</p><p><strong>block块：</strong>HDFS在物理上是以block存储的，block大小可以通过配置参数（dfs.blocksize）来规定，默认128M，可以减少寻址开销。大文件会被切分成很多block来存，而小文件存储则不会占用整个块的空间。</p><p><strong>NameNode：</strong>是master。负责管理文件系统的namespace（可以理解是指向具体数据的文件名、路径名这种）和客户端对文件的访问。data path？</p><p><strong>(非Yarn)JobTracker：</strong>在NameNode上，协调在集群运行的所有作业，分配要在tasktracker上运行的map和reduce任务。</p><p><strong>DataNode：</strong>是slave。datanode则负责数据的存储。</p><p><strong>(非Yarn)TaskTracker：</strong>在datanode上，运行分配的任务并定期向jobtracker报告进度。</p><p><strong>流式访问：</strong>指hdfs访问时像流水一样一点一点过。这样也决定了hdfs是一次写入、多次读取的特性，同时只能有一个wirhter。这样访问方式适合做数据分析，而不是网盘这种。</p><p><strong>rack-aware（机架感知）：</strong>这是hdfs的复制策略。hdfs为了数据可靠一般会将数据复制几份（默认三份）。同一个机架的机器传输速度快，不需要通过交换机。为了提高效率，一台机器的数据会把一个备份放在同一机架（相同rack id）的机器里，另一个备份放在其他机架的机器上。机架的错误率很小，所以不影响可靠性。</p><p><strong>hdfs的特点：</strong></p><ul><li>面对构成系统的组件数目很大，所以对硬件的快速检测错误并自动回复非常重要</li><li>hdfs需要流式访问他们的数据集</li><li>运行的数据集非常大，一个典型文件大小一般在几G到几T</li><li>文件访问模型是“一次写入、多次访问”</li><li>将计算移动到数据附近闭将数据移动到计算更好</li></ul><h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>yarn其实是解决了经典mapreduce中一些问题（例如：jobtracker太累导致的可扩展性问题）的新一代hadoop计算平台。</p><p><strong>ResourceManager：</strong>代替jobtracker，以后台进程的形式运行。追踪有哪些可用的活动节点和资源，指出哪些程序应该何时或者这些资源。</p><p><strong>ApplicationMaster：</strong>代替一个专用而短暂的JobTracker。用户提交一个应用程序时，会启动applicationmaster这个轻量级进程实例来协调程序内任务（监视进度、定时向resourcemanager发送心跳数据、负责容错等），计算数据需要的资源并向resourcemanager申请。它本身也是在一个container里运行的，且可能与它管理的任务运行在同一节点上。</p><p><strong>Container：</strong>是yarn中资源的抽象，封装了某个节点上一定量的资源（如cpu和内存等资源）。它的分配是由applicationmaster向resourcemanager申请的；而它的运行则是applicationmaster向资源所在的nodemanager发起的。</p><p><strong>NodeManager：</strong>代替tasktracker。拥有很多动态创建的资源Container。容器大小取决于它所包含资源量，而一个节点上的容器数量由配置参数和除用于后台进程和操作系统以外资源总量决定。</p><h2 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h2><p>Hdfs采用了master/slave架构。一个hdfs集群由一个namenode和一群datanodes组成。简单来说，就是hdfs通过namenode暴露出了文件系统命名空间的操作，包括打卡、关闭、重命名文件等等。在这个文件系统对一块数据的操作会映射到具体的datanodes上。</p><p>所以一般是一台机器上搭namenode，然后datanode在其他各个机器上。</p><p><img src="/img/hdfsarchitecture.jpg" alt="hdfsarchitecture"></p><h3 id="Hadoop-Streaming"><a href="#Hadoop-Streaming" class="headerlink" title="Hadoop Streaming"></a>Hadoop Streaming</h3><p>java这么规范的东西有人就是不喜欢，非得用python啥的写hadoop，所以就有了hadoop streaming，支持其他语言的hadoop操作。</p><h3 id="Haddop-Resource-Management"><a href="#Haddop-Resource-Management" class="headerlink" title="Haddop Resource Management"></a>Haddop Resource Management</h3><p>hadoop的资源管理进化</p><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="单节点测试"><a href="#单节点测试" class="headerlink" title="单节点测试"></a>单节点测试</h3><p>比较无聊，只是测一下能不能跑，不需要运行什么</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> Cellar/hadoop/3.1.0/libexec</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir input<span class="comment">#不能是别的名字</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar  grep input output <span class="string">'dfs[a-z.]+'</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br></pre></td></tr></table></figure><h3 id="伪分布式测试（pseudo-distributed）"><a href="#伪分布式测试（pseudo-distributed）" class="headerlink" title="伪分布式测试（pseudo-distributed）"></a>伪分布式测试（pseudo-distributed）</h3><ol><li><p>保证本机已经装好hadoop，java1.8（java9有些函数被废了会报错）</p></li><li><p>配置本机ssh，确保</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost#mac默认不允许，需要手动去打开</span><br></pre></td></tr></table></figure><p>可以运行。注意：mac默认不允许任何机器远程登录，需要到  系统偏好设置 -&gt; 共享 去勾选远程登录。</p></li><li><p>配置HDFS，包括core-site.xml文件和hdfs-site.xml文件。前者配置用于存储HDFS的临时文件目录和hdsf访问端口，后者确定复制份数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop/libexec/tmp/hadoop- $&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--如果无此目录则去mkdir一个--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>格式化HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>成功的话在tmp目录下可以看到dfs文件</p></li><li><p>启动各个节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start NameNode#启动namenode</span><br><span class="line">hdfs --daemon start DataNode#启动datanode</span><br><span class="line">hdfs --daemon start SecondaryNameNode#它是namenode的快照，保证了namenode的更新</span><br><span class="line"><span class="meta">jps#</span><span class="bash">用来查看这些节点是否真的启动了</span></span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建文件夹及文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /demo#在hdfs上创建demo文件夹</span><br><span class="line">hdfs dfs -ls /demo</span><br><span class="line">hdfs dfs -put test.input /demo#将本地的test.input文件发到hdfs上</span><br></pre></td></tr></table></figure></li><li><p>配置Yarn的mapred-site.xml和yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-sited.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>share/hadoop/mapreduce/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果不加这个property，在后面运行mapreduce任务时会报找不到包 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动resourcemanager和nodemanager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start nodemanager</span><br><span class="line">yarn --daemon start resourcemanager</span><br></pre></td></tr></table></figure><p>yarn端口是8088，可以去localhost:8088看页面</p></li><li><p>运行mapreduce任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar hadoop-mapreduce-examples-3.1.0.jar wordcount /demo/test.input /demo-output/</span><br></pre></td></tr></table></figure><p>可以到对应文件里查看运行结果</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。&lt;/p&gt;
&lt;h3 id=&quot;map-reduce&quot;&gt;&lt;a href=&quot;#map-reduce&quot; class=&quot;headerlink&quot; title=&quot;map reduce&quot;&gt;&lt;/a&gt;map r
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
</feed>

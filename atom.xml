<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Max&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/d02dec40f94389c9e1ed922fa6ad3ed2</icon>
  <subtitle>keep hungry, then you&#39;ll be really hungry</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-01T09:05:18.915Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Max Qi</name>
    <email>490949611@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>流数据挖掘算法</title>
    <link href="http://yoursite.com/2018/09/29/%E6%95%B0%E6%8D%AE%E6%B5%81%E6%8C%96%E6%8E%98/"/>
    <id>http://yoursite.com/2018/09/29/数据流挖掘/</id>
    <published>2018-09-29T14:06:01.697Z</published>
    <updated>2018-10-01T09:05:18.915Z</updated>
    
    <content type="html"><![CDATA[<p>流数据，顾名思义，就是针对流数据（例如google查询、twitter的状态更新这类数据）的一些挖掘。不同于普通数据集，流数据源源不断地产生，而我们的存储空间是有限的。所以要采取一些办法，：</p><ul><li>Samping data from stream</li><li>Queries over sliding windows</li><li>Filtering a data streaming</li></ul><h3 id="Sampling-data-from-stream"><a href="#Sampling-data-from-stream" class="headerlink" title="Sampling data from stream"></a>Sampling data from stream</h3><p>由于我们不能存储整个stream，所以要存一些sample，具体有两种方式</p><ol><li>Sample一个stream的固定比例</li><li>维护一个固定大小的随机Sample —— 蓄水池采样</li></ol><h4 id="固定比例"><a href="#固定比例" class="headerlink" title="固定比例"></a>固定比例</h4><p>以用户的查询为例：假设有s个单次查询，有d个两次查询，即总共有s+2d次查询，sample率为p（0&lt;p&lt;=1），求用户两次查询的比例</p><p>正确答案：d/(s+d)</p><p>按照条件推倒：dp^2/(sp+dp^2+2p(1-p)d)</p><p>这个准确率问题还是比较大的，为了改良，可以采用以取用户sample的方式。</p><h4 id="固定大小的sample-——-蓄水池采样"><a href="#固定大小的sample-——-蓄水池采样" class="headerlink" title="固定大小的sample —— 蓄水池采样"></a>固定大小的sample —— 蓄水池采样</h4><p>很好玩的问题：假如有一本非常厚的电话簿，你要随机挑1000个人打电话，并且保证每个人被选中的几率相等，你要怎么做？</p><p>这里的电话簿就像数据流，你不能先数一遍总共有多少人，再产生一千个随机数这样，效率太低。</p><p><strong>解决方法：</strong></p><p>先选前1000人，然后后面的每一位以k/n的概率决定替换掉前面的任意一位。其中n是当前总共人数，k是蓄水池大小。例如第1001位就有1000/1001的概率替换掉前面1000个人中任意一个。</p><p><strong>证明：</strong></p><p>条件: sample size: k，total size: n，probability of each element: s/n</p><p>求证: 对于n-&gt;n+1的情况，probability of each element: k/n+1，即无论n多大，每个元素被选中概率均等</p><p>证明: </p><p>每个元素被选中概率：k/n</p><p>n-&gt;n+1后，每个被替换前面概率：k/n+1</p><p>而假设A而原来在sample里的，仍然还在的概率为: (1-k/n+1) + (k/n+1)*(k-1/k) = n/n+1，这个公式前者是n+1这个元素没被选中，后者是没替换自己的概率</p><p>那么A的概率就是：k/n * n/n+1 = k/n+1</p><p>综上，概率相等～</p><h3 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a>Sliding Windows</h3><p>就是滑动窗口，简单来说：最多储存N bits,当又来一个新bit时，吐出第N+1个bits（也就是窗口里最老的那个）。</p><p>问题：如果我们要统计N中有一个1，那必须全部存储N中的内容，但窗口的存储空间有限，因此这里用了DGIM方法-&gt;exponential Window。</p><h4 id="DGIM算法"><a href="#DGIM算法" class="headerlink" title="DGIM算法"></a>DGIM算法</h4><p>它是通过Bucket来对滑动窗口进行划分，每个桶包括：</p><ol><li>桶最右边的timestamp（O(logN) bits）</li><li>在每个桶中1刀数量（O(log logN)bits）</li></ol><p>桶的限制：每个桶里的1的数量是2的次方。</p><p>桶的特征：</p><ul><li>只有1个或者2个桶有相同数目的2的次方个1</li><li>桶和桶的timestamps不会重叠</li><li>桶是按照大小有序的</li><li>当桶的end-time &gt; N的时候（即整个桶已经从窗口中走了），桶会消失</li></ul><p><img src="/img/DGIMbucket.png" alt="DGIMbucket"></p><p>  DGIM算法中数据结构的更新（2048式更新）：</p><ol><li>每一个新的位进入滑动窗口后，最左边一个位从窗口中移出（同时从桶中移出）；如果最左边的桶的时间戳是当前时间戳减去N（也就是说桶里已经没有处于窗口中的位），则放弃这个桶；</li><li>对于新加入的位，如果其为0，则无操作；否则建立一个包含新加入位的大小为1的桶；</li><li>由于新增一个大小为1的桶而出现3个桶大小为1，则合并最左边的两个桶为一个大小为2的桶；合并之后可能出现3个大小为2的桶，则合并最左边两个大小为2的桶得到一个大小为4的桶……依次类推直到到达最左边的桶。</li></ol><p>如何估计最新N窗口中1的数目呢？</p><ul><li>将除了最后一个的所有的bucket大小（这里大小都是指1的数目）加起来</li><li>再加上最后一个bucket的一半大小</li></ul><p>准确率：&gt;= 50%</p><h3 id="Filtering-Data-Streams"><a href="#Filtering-Data-Streams" class="headerlink" title="Filtering Data Streams"></a>Filtering Data Streams</h3><p>问题：垃圾邮件的过滤问题，我们知道有10亿个好的email address，如果邮件来自这些address，那么它就不是spam。即现在有一封邮件{“address”:”contents”}，如果key可以匹配10亿email address就不是spam。</p><h4 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h4><p>设S是刚刚好的email address集合，而B是大小为n，全部初始化为0的一个bit array</p><p><img src="/img/initBFbits.png" alt="initBFbits"></p><p>使用k个相互独立的hash函数h1,h2……hk，他们可以将元素映射到[0,n]的bit array上。将S中元素通过k个hash函数映射，映射到的地方设置为1</p><p><img src="/img/findBFbits.png" alt="findBFbits"></p><p>这里的x1，x2就是S的元素。</p><p>当判断一个元素y1，y2是否在集合S中时，就通过相同的hash函数将其映射到B上，如果任何一个hash函数映射到的地方存在0，那y就不在S中，反之则证明在S上，或者是一个false positive（将错的分到对的里）。</p><p><img src="/img/hashedBFbits.png" alt="hashedBFbits"></p><p>如图y1就不在S中，y2或是在S中，或是一个false positive。但这种方法不存在true negative（将对的分到错的里）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;流数据，顾名思义，就是针对流数据（例如google查询、twitter的状态更新这类数据）的一些挖掘。不同于普通数据集，流数据源源不断地产生，而我们的存储空间是有限的。所以要采取一些办法，：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Samping data from stream&lt;/li
      
    
    </summary>
    
    
      <category term="data mining" scheme="http://yoursite.com/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB使用整理</title>
    <link href="http://yoursite.com/2018/09/25/MongoDB%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/25/MongoDB使用整理/</id>
    <published>2018-09-25T06:14:39.539Z</published>
    <updated>2018-10-01T16:00:06.791Z</updated>
    
    <content type="html"><![CDATA[<p>最近project需要mongo做存储，因此在这里总结一些mongo的东西</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Mongo是一个开源的文档型数据库，它提供了高性能、高可用和自动扩展的特性。</p><p>概念对应关系如下表：</p><table><thead><tr><th>SQL术语/概念</th><th>MongoDB术语/概念</th><th>解释/说明</th></tr></thead><tbody><tr><td>database</td><td>database</td><td>数据库</td></tr><tr><td>table</td><td>collection</td><td>数据库表/集合</td></tr><tr><td>row</td><td>document</td><td>数据记录行/文档</td></tr><tr><td>column</td><td>field</td><td>数据字段/域</td></tr><tr><td>index</td><td>index</td><td>索引</td></tr><tr><td>table joins</td><td></td><td>表连接,MongoDB不支持</td></tr><tr><td>primary key</td><td>primary key</td><td>主键,MongoDB自动将_id字段设置为主键</td></tr><tr><td>aggregation(group by)</td><td>aggregation pipeline</td><td>聚合函数</td></tr><tr><td>transactions</td><td>transactions</td><td>事物</td></tr></tbody></table><p>mongo支持包括mongo shell、c++、java、python、php等几乎所有主流语言。只需要下好对应的driver即可，下面的操作我们使用pymongo来完成。</p><h2 id="CURD操作"><a href="#CURD操作" class="headerlink" title="CURD操作"></a>CURD操作</h2><p>CURD是数据库基本操作。这里用pymongo来创建mongo client。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"></span><br><span class="line"><span class="comment">#首先要创建一个mongo client来运行mongo instance</span></span><br><span class="line">client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#选中需要操作的database（test）和collection（inventory）</span></span><br><span class="line">inventory = client[<span class="string">'test'</span>][<span class="string">'inventory'</span>]</span><br></pre></td></tr></table></figure><h3 id="Retrive操作"><a href="#Retrive操作" class="headerlink" title="Retrive操作"></a>Retrive操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result = collection.find_one(&#123;filter&#125;)</span><br><span class="line"></span><br><span class="line">results = collection.find(&#123;filter&#125;).limit(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">values = inventory.find(&#123;<span class="string">"type"</span>: &#123;<span class="string">"$regex"</span>: <span class="string">"Event"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><h3 id="Update操作"><a href="#Update操作" class="headerlink" title="Update操作"></a>Update操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">collection.update_one(&#123;filter&#125;,&lt;&#123;$set&#125;&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">inventory.update_one(&#123;<span class="string">"id"</span>: <span class="string">"2614896652"</span>&#125;, &#123;<span class="string">"$set"</span>: &#123;<span class="string">"type"</span>: <span class="string">"CreateEvent"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><h3 id="Create操作"><a href="#Create操作" class="headerlink" title="Create操作"></a>Create操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在这里我读取一个json文件，然后存入数据库中</span></span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"/data/test.json"</span>, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> f.readlines():</span><br><span class="line">        data.append(json.loads(content))</span><br><span class="line"></span><br><span class="line">inventory.insert_many(data)</span><br></pre></td></tr></table></figure><h3 id="Delete操作"><a href="#Delete操作" class="headerlink" title="Delete操作"></a>Delete操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">collection.delete_many(&#123;filter&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">inventory.delete_many(&#123;&#125;) <span class="comment">#删除全部记录</span></span><br></pre></td></tr></table></figure><h3 id="Bulk-Write操作"><a href="#Bulk-Write操作" class="headerlink" title="Bulk Write操作"></a>Bulk Write操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> InsertOne,DeleteMany,UpdateOne</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一个参数数组为operations数组，ordered表示是否顺序执行</span></span><br><span class="line">inventory.bulk_write([</span><br><span class="line">    InsertOne(data[<span class="number">0</span>]),</span><br><span class="line">    UpdateOne(&#123;<span class="string">"id"</span>: <span class="string">"2614896652"</span>&#125;, &#123;<span class="string">"$set"</span>: &#123;<span class="string">"type"</span>: <span class="string">"PushEvent"</span>&#125;&#125;),</span><br><span class="line">    DeleteMany(&#123;&#125;)</span><br><span class="line">], ordered=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Text-Search操作"><a href="#Text-Search操作" class="headerlink" title="Text Search操作"></a>Text Search操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#mongo提供了针对文本内容search的方法，要想使用这个就要先对对应field创建text index</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建text index</span></span><br><span class="line">db.inventory.createIndex(&#123;<span class="string">"type"</span>:<span class="string">"text"</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#文本搜索</span></span><br><span class="line">values = inventory.find(&#123;<span class="string">"$text"</span>: &#123;<span class="string">"$search"</span>: <span class="string">"CreateEvent"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><h2 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h2><p><img src="/img/replica.png" alt="replica"></p><p>一组replica只指存相同数据的节点集。其中只有一个被认为是primary节点，其他是secondary节点（类似于master-slave）。</p><p>只有primary节点可以接受所有的写操作，并且通过{w:”majority”}来确认通知。同时primary会把所有数据操作放在oplog里。</p><p>对于secondary节点，他们会备份primary节点的oplog，然后把log里的东西apply到自己的节点上，这样primary上的dataset也就映射到了secondary上。当primary不可用时，secondaries会通过选举决定谁是primary。</p><p><img src="/img/secondaySelection.png" alt="secondaySelection"></p><p>你可以额外增加一个叫arbiter的节点，arbiter不维护任何数据，它通过应答其他节点发出的心跳和选举请求，维护了投票时需要的法定人数。因为它本身不维护dataset，所以它占用的资源很少，也不要单独硬件。所以当节点数为偶数时，可以增加一个arbiter来维护。</p><p><img src="/img/arbiter.png" alt="arbiter"></p><h3 id="Asynchronous-Replication"><a href="#Asynchronous-Replication" class="headerlink" title="Asynchronous Replication"></a>Asynchronous Replication</h3><p>secondary本身异步地从primary复制。完成备份后，即使。</p><h4 id="Automatic-Failover"><a href="#Automatic-Failover" class="headerlink" title="Automatic Failover"></a>Automatic Failover</h4><p>当primary在一定时间（configured by electionTimeoutMillis，10s default）不与secondary通信时，有资格的secondary会提名自己参加选举，cluster会试图完成选举并恢复正常功能。</p><p>在选举阶段，这个replica set都无法进行写操作，个别secondary如果配置了允许读操作的话还是正常进行的。通常系统会在12s只能判定一个primary不可用并完成选举工作。</p><h2 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h2><p>Sharding是Mongo采用的一种在多台机器上分布数据的方法。对于在大量、高吞吐率的数据在单台服务器上无法承载，有两种方法来扩展机器：</p><ul><li>Vertical Scaling：增加单机的容量、内存，使用更好的cpu等方式。但这种受限于目前可用的技术能力。</li><li>Horizontal Scaling：增加机器的数量，每个机器只要handle一部分数据。比较便宜，但维护和部署基础设施成本较高。</li></ul><p>Sharding cluster包含一下组件：</p><ul><li>Shard：每个shard包含总的一个子数据集。每个shard都可以被部署为备用数据集。</li><li>mongos：mongos作为query router，提供了client和sharded cluster之间的借口。</li><li>config servers：储存了metadata和cluster的配置设置。在mongo3.4，其必须被部署为一个备用数据集（CSRS）。</li></ul><p>关系组图如下：</p><p><img src="/img/sharded cluster.png" alt="sharded cluster"></p><p>MongoDB从collection层面来分割数据。</p><h3 id="Shard-Keys"><a href="#Shard-Keys" class="headerlink" title="Shard Keys"></a>Shard Keys</h3><p>MongoDB是根据shard key来分割一个collection的。shard key是由目标collection中每个document都有的fields或者不变的field组成的。</p><p>在分割collection时要选择shard key，分割完后shard key就不能再改变了。</p><p>对于非空collection，必须有一个index是以shard key开头的；而对于空collection，MongoDB则会创建一个index。</p><p>shard keys的选择会影响sharded cluster的性能、效率、扩展性等问题，它和它背后的index也会影响你的集群可以使用的sharding策略。</p><h3 id="Chunks"><a href="#Chunks" class="headerlink" title="Chunks"></a>Chunks</h3><p>MongoDB会数据分成一个个的chunks，其中包含了左闭右开的shard key范围。Mongo使用shared cluster balancer来实现在sharded cluster上的不同shards间移动chunks。</p><h3 id="Sharded-and-Non-sharded-Collections"><a href="#Sharded-and-Non-sharded-Collections" class="headerlink" title="Sharded and Non-sharded Collections"></a>Sharded and Non-sharded Collections</h3><p>每个数据库都可以既有sharded collections又有un-sharded collections。unsharded collection存在数据库的primary shard上。每个数据库都有primary shard。</p><h3 id="Connecting-to-a-sharded-Cluster"><a href="#Connecting-to-a-sharded-Cluster" class="headerlink" title="Connecting to a sharded Cluster"></a>Connecting to a sharded Cluster</h3><p><img src="/img/shaded connection.png" alt="shaded connection"></p><p>用户通过mongos router来实现和sharded cluster（包含sharded和unsharded collections）的交互。client做读写操作时不能只连接一个shard。</p><h3 id="Sharding-Strategy"><a href="#Sharding-Strategy" class="headerlink" title="Sharding Strategy"></a>Sharding Strategy</h3><p>具体来说sharding的方式有两种：</p><h4 id="Hashed-Sharding"><a href="#Hashed-Sharding" class="headerlink" title="Hashed Sharding"></a>Hashed Sharding</h4><p>通过计算shard key的hash值来分到不同的chunk中。在使用hashed indexes来处理查询时，client不需要计算hash，这个由MongoDB自动完成。</p><p><img src="/img/hashed sharding.png" alt="hashed sharding"></p><p>对于那种很接近的shard key（例如单调递增的），hash通常会比较均匀地把他们分到不同的chunk里。这样存在个问题：当我要取一定范围内的数据时，需要从各个不同的chunk里取，造成较大范围的broadcast operations。</p><h4 id="Ranged-Sharding"><a href="#Ranged-Sharding" class="headerlink" title="Ranged Sharding"></a>Ranged Sharding</h4><p>把shard key的值分成一定范围区间，然后分配。</p><p><img src="/img/ranged sharding.png" alt="ranged sharding"></p><p>这样相近的shard key实在同一个chunk里的，这样mongos只需要把operation route到包含这些数据的shards。</p><p>这个方式很依赖shard key的选择，不理想的话可能造成数据分配不平衡。</p><h3 id="Zones-in-Sharded-clusters"><a href="#Zones-in-Sharded-clusters" class="headerlink" title="Zones in Sharded clusters"></a>Zones in Sharded clusters</h3><p>在sharded clusters里，可以把几个shard对应到一个zone中，一个shard也可以对应到多个zone里。chunks只在相同zone下的几个shards中转移。</p><p><img src="/img/zoneMongoDB.png" alt="zoneMongoDB"></p><p>zones存在的目的是提高在大的sharded clusters里数据的本地化程度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近project需要mongo做存储，因此在这里总结一些mongo的东西&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;Mongo是一个开源的文档型数据库
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>局部敏感哈希(LSH)——Locality Sensitive Hashing</title>
    <link href="http://yoursite.com/2018/09/19/%E5%B1%80%E9%83%A8%E6%95%8F%E6%84%9F%E5%93%88%E5%B8%8C/"/>
    <id>http://yoursite.com/2018/09/19/局部敏感哈希/</id>
    <published>2018-09-19T03:34:16.899Z</published>
    <updated>2018-09-19T05:12:12.981Z</updated>
    
    <content type="html"><![CDATA[<p>课上学了一种大数据计算相似性的算法，觉得很巧妙，在这里总结下。</p><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>求相似性的场景有很多，比如比较两图相似性从而做一些处理，或者搜索引擎比较文本相似性从而确定返回的搜索结果等等。这种场景常常面临的问题是要进行比较的数据特征量非常大，同时比较对象又非常多，不能直接比较。LSH正是解决了这个问题。</p><h2 id="LSH流程"><a href="#LSH流程" class="headerlink" title="LSH流程"></a>LSH流程</h2><p>LSH分为三步：</p><ol><li>Shingling: Convert documents to sets</li><li>Min Hashing: Convert large sets to short signatures, while preserving similarity</li><li>Locality Sensitive Hashing: Focus on pairs of signatures likely to b from similar documents - <strong>Candidate pairs!</strong></li></ol><p><img src="/img/LSHFlow.png" alt="LSHFlow"></p><p>要测相似性就要确定距离/相似性函数，LSH使用的是jaccard距离/相似性，具体如下：</p><ul><li>sim(C1,C2) =  | C1 ∩ C2 | / | C1 U C2 |</li><li>d(C1,C2) = 1 -  | C1 ∩ C2 | / | C1 U C2 |</li></ul><h3 id="第一步：Shingling"><a href="#第一步：Shingling" class="headerlink" title="第一步：Shingling"></a>第一步：Shingling</h3><p>Shingling是将整个文档转化为一个set，这里我们不重视文档顺序、重要词等信息，所以shingling是最好选择。</p><p>K-shingle（k-gram）则是将文档分成以k个token为一个单位的序列，token可以是字符、单词或者其他。例如k=2，则对于文档D = abcab，2-shingle的set则是：C = S(D) = {ab, bc, ca}。k的取值：</p><ul><li>k = 5，对于比较短的文章</li><li>k = 10，对于很长的文章</li></ul><p>随后可以将每一个shingle作为一个维度将所有文档联合起来构建一个0/1矩阵，这个 矩阵:</p><ul><li>Rows = elements (shingles)</li><li>Columns = set(documents)</li></ul><p>例如：</p><table><thead><tr><th></th><th>C1</th><th>C2</th><th>C3</th><th>C4</th></tr></thead><tbody><tr><td>Shingles1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>Shingles2</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>…</td><td>0</td><td>1</td><td>0</td><td>1</td></tr><tr><td>…</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>很显然这个矩阵非常稀疏的。这样计算量非常大，所以有第二步来“压缩矩阵”。</p><h3 id="第二步：Minhash"><a href="#第二步：Minhash" class="headerlink" title="第二步：Minhash"></a>第二步：Minhash</h3><p>minhash需要压缩矩阵，将原来100，000维的压缩到100维，采用了很聪明的方法：概率聚类大法！</p><p>具体描述：将上述矩阵每一列随机排列，然后记录第一个1的对应位置（位置标注方式不一定是递增序列，而和产生随机的方式有关）</p><p><img src="/img/minhash.png" alt="minhash"></p><p>每一次随机都会产生1个signature，100次随机就针对每个文档产生一个长度100的signature，文档之前只要互相比较一下这个signature的相似性就相当于比较文档的相似性。</p><p>这个算法的可行性用公式表达则是：</p><ul><li>Pr[ min( π (C1 )) = min ( π (C2 )) ] = sim(C1,C2)</li></ul><p>这个结论是可以证明的，此处略过。</p><p>由于这里要随机100次，这个显然是比较麻烦的，所以随机用hash方法来进行。</p><p>全局hash为：</p><ul><li>h_a,b(x) = (( a*x + b )mod p ) mod N</li></ul><p>然后每次只要随机产生a，b两个值就好。之后按照产生的hash函数的大小顺序进行排列。hash产生重复的值也不要紧，signature就是产生的这个hash值即可。</p><p>这里解决了维度太多的问题，但相互比较问题还是没解决，因为文档很多，所以要非常多次比较。第三步将减少比较流程，只要候选文档之间相互比较即可。</p><h3 id="第三步：Locality-Sensitive-Hashing"><a href="#第三步：Locality-Sensitive-Hashing" class="headerlink" title="第三步：Locality Sensitive Hashing"></a>第三步：Locality Sensitive Hashing</h3><p>这里先讲M矩阵分成了b个bands，每个bands里包含r行，如图：</p><p><img src="/img/LSHBand.png" alt="LshBand"></p><p>之后再对每个内容进行hash，使其map到k个buckets中，如果C1,C2有大于等于一个band在同一buckets中，则对他们进行比较，如图：</p><p><img src="/img/hashBand.png" alt="hashBand"></p><p>这样将每个bands都map到Buckets的不同块里，如果map到同一个buckets的块里，则称为candidate pair，他们不一定相等，要进行比较。但如果map到不同的块里，表明这两个band一定不同，不需要进行比较。</p><p>下面验证一下这个算法的可行性。</p><p>假设C1和C2的相似度为t，b为bands数目，r为每个band多少row，可以得知：</p><ul><li>band中所有row相等（该band会映射到同一bucket块）概率： t^r</li><li>band中row存在不相等row概率：1-t^r</li><li>没有一个band是相等的概率：(1 - t^r)^b</li><li>至少有一个band相等的概率：1 - (1 - t^r)^b</li></ul><p>按照公式计算，当b取20，r取5，对于相似度30%的文档C1,C2（不希望有bands出现在同一buckets中），有4.7%的几率至少一个bands分在同一buckets中，我们花多余的代价去计算它概率比较小。而相对于相似度80%的文档，有99.96%的几率我们要去比较他们，我们会错过一对相似文档的概率不到0.04%，可以接受。</p><p>综上改算法是可行的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;课上学了一种大数据计算相似性的算法，觉得很巧妙，在这里总结下。&lt;/p&gt;
&lt;h2 id=&quot;使用场景&quot;&gt;&lt;a href=&quot;#使用场景&quot; class=&quot;headerlink&quot; title=&quot;使用场景&quot;&gt;&lt;/a&gt;使用场景&lt;/h2&gt;&lt;p&gt;求相似性的场景有很多，比如比较两图相似性从而做
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>图说mysql的四种join</title>
    <link href="http://yoursite.com/2018/09/14/%E5%9B%BE%E8%AF%B4mysql%E7%9A%84%E5%9B%9B%E7%A7%8Djoin/"/>
    <id>http://yoursite.com/2018/09/14/图说mysql的四种join/</id>
    <published>2018-09-14T02:58:13.621Z</published>
    <updated>2018-09-15T02:19:20.596Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/img/rightJoin.png" alt="屏幕快照 2018-09-14 上午10.54.59"></p><p><img src="/img/leftJoin.png" alt="屏幕快照 2018-09-14 上午10.55.09"></p><p><img src="/img/innerJoin.png" alt="屏幕快照 2018-09-14 上午10.55.39"></p><p><img src="/img/crossJoin.png" alt="屏幕快照 2018-09-14 上午10.55.46"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/img/rightJoin.png&quot; alt=&quot;屏幕快照 2018-09-14 上午10.54.59&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/leftJoin.png&quot; alt=&quot;屏幕快照 2018-09-14 上午10.55.09&quot;&gt;&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="database" scheme="http://yoursite.com/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce终极整理</title>
    <link href="http://yoursite.com/2018/09/13/MapReduce%E7%BB%88%E6%9E%81%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/13/MapReduce终极整理/</id>
    <published>2018-09-13T06:33:26.110Z</published>
    <updated>2018-09-15T02:14:03.928Z</updated>
    
    <content type="html"><![CDATA[<p>这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。</p><p>首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。</p><h3 id="使用MapReduce的目的"><a href="#使用MapReduce的目的" class="headerlink" title="使用MapReduce的目的"></a>使用MapReduce的目的</h3><p>用一个工具只有知道他的目的和优点，才能找到最合适的使用场景。简单来说，mapreduce提供了一个自动并行和分布式计算的工具（接口），在大规模集群中性能出众。由于它隐藏了这些分布式系统的细节，所以很多不懂分布式的程序员也可以基于此搭建分布式系统。对于一些web服务器、爬虫、排序算法、机器学习、数据挖掘等都很有用。</p><h3 id="MapReduce的架构和运行"><a href="#MapReduce的架构和运行" class="headerlink" title="MapReduce的架构和运行"></a>MapReduce的架构和运行</h3><p>这是本文的重点，不多说，直接上图。</p><p><img src="/img/mapreduce.png" alt="mapreduce"></p><p>这幅图是直接从论文截下来的，生动的讲述了mapreduce的整个流程：</p><ol><li>将用户input文件主动分割成成16-64MB大小的块，然后在集群中做一些备份（一般备份3份，其中两份在同一个机架上）</li><li>master发挥作用，将选择一些idle的worker给其分配map或者reduce任务。</li><li>这时候worker就会去读那些被分割的文件，将一个key/value对读取到用户定义的map函数中。worker在读取这些文件时，会优先读取本地存着的，如果本地没有，选取离它最近的文件，减少网络传输代价。</li><li>map函数会产生一些中间过程数据，然后周期性的通过buffer将它存在本地磁盘上，然后将地址、文件名等信息通知master，这些中间文件通过hash(key) mod R的方式被分成R份（R是reduce worker的数量），master就会将这些块分配给对应的reducer。</li><li>reducer会通过RPC方式读取到这些中间过程数据，然后进行一个排序（shuffle），这个shuffle会让相同value聚在一起。这个很必要，因为不同key会映射到同一个reduce任务上。</li><li>只有在map全部结束后，reduce才会开始。在reducer中处理这个有序数据时，遇到相同key就会把value放在用户定义的reduce函数中。最终reduce函数会把结果文件输出到这个reduce partition中。</li></ol><p>最终得到的结果其实就是part-r-00000这样形式的不同文件。用户没必要把output文件最后聚在一起，如果需要的话，这个结果还可以作为下一轮mapreduce的输入。</p><h3 id="在有Combiner后过程的改良"><a href="#在有Combiner后过程的改良" class="headerlink" title="在有Combiner后过程的改良"></a>在有Combiner后过程的改良</h3><p>上面的过程是论文中解释的，但现在的程序都用了Combiner，Combiner其实和reduce的代码是一样的，那他是做什么用的呢？</p><p>Combiner其实是针对本地的map后的结果进行pre-reduce（或者叫mini-reduce），例如wordcount在map之后大概是(‘a’:1),(‘b’,1),(‘a’,1),(‘c’,1)这样，combiner将local的这些先做一次reduce，变成(‘a’,2),(‘b’,1),(‘c’,1)，之后再去shuffle和reduce。</p><p>综上mapreduce整个过程分为四步：</p><p>​                              <strong>Map -&gt; Combiner -&gt; Shuffle -&gt; Reduce</strong></p><h3 id="Master节点的任务和结构"><a href="#Master节点的任务和结构" class="headerlink" title="Master节点的任务和结构"></a>Master节点的任务和结构</h3><p>对于每个map任务或者reduce任务，都分成三种状态：idel, in-progress, completed。每个任务的这些状态都存在master节点上。</p><p>Master节点是作为map任务和reduce任务通信的管道的。master要存储并更新M个map任务产生的M <em> R个块(每个map任务产生R个块)的位置信息、文件名等。即，master承担 O(M+R)个scheduling决策，存储 O(M </em> R)个状态信息。</p><p>M：map是的m块数据，R: reduce时的r块数据，W: worker机器的数量；三者关系为：</p><p>​                            <strong>M &gt; R &gt; W</strong></p><h3 id="Worker的容错问题"><a href="#Worker的容错问题" class="headerlink" title="Worker的容错问题"></a>Worker的容错问题</h3><p>首先，master出错的话，没啥好说的，直接告诉用户就成。</p><p>worker一旦出错，master应该要感知到，有两种策略感知：</p><ul><li><p>Pull模型 ：worker通过发送heartbeat给master，master在感知到heartbeat之后在heartbeat response里给worker分配任务。</p></li><li><p>Push模型：master一直ping worker，当一段时间ping不通后说明worker失败了。</p></li></ul><p>这个时候master会将worker的任务分配给其他的worker去执行。对于执行完的worker，状态会重新标定为idle，表示有资格接受任务。失败的worker也会被标定为idle，同样可以接受任务。</p><p>对于在失败的worker上completed的map任务，在其他worker上需要重新执行，因为他们存在本地的中间数据访问不到了。但对于在失败的worker上completed的reduce任务则不需要重新执行，因为他们的结果文件存在了global的文件系统下。</p><p>在执行mapreduce时对于一些<strong>straggler</strong>（落伍者），有两种处理方式：</p><ul><li>Job stealing: 对这个job进行分片，将没完成的部分交给其他的worker完成。</li><li>Speculative execution: master，这时候两个类似竞争的关系，当其中一个结束时，这个task会被标定为completed，同时另一个task将会被放弃。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。&lt;/p&gt;
&lt;p&gt;首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。&lt;/p&gt;
&lt;h3 id=&quot;使用MapRe
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>课程信息整理</title>
    <link href="http://yoursite.com/2018/09/13/%E8%AF%BE%E7%A8%8B%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/13/课程信息整理/</id>
    <published>2018-09-13T05:00:48.157Z</published>
    <updated>2018-09-13T08:19:32.862Z</updated>
    
    <content type="html"><![CDATA[<p>之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。</p><hr><h3 id="CMSC5741-Big-Data-Technology-and-Applications"><a href="#CMSC5741-Big-Data-Technology-and-Applications" class="headerlink" title="CMSC5741 - Big Data Technology and Applications"></a><a href="">CMSC5741 - Big Data Technology and Applications</a></h3><p>textbook: Mining of Massive Dataset（已借）</p><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/lyu/" target="_blank" rel="noopener">Prof. Michael R. Lyu</a></p><p>Tutor: Zeng Jichuan</p><p>exam: <strong>Nov.6 Midterm exam</strong> </p><p>Assessment Scheme and Deadlines:</p><ul><li>20% Assignment</li><li>40% Midterm examination</li><li>40% Project : Proposal, Presentation, Report</li></ul><p>Backgroud Knowledge: Tensorflow, Amazon EC2</p><hr><h3 id="CSCI5570-Large-Scale-Data-Processing-Systems"><a href="#CSCI5570-Large-Scale-Data-Processing-Systems" class="headerlink" title="CSCI5570 - Large Scale Data Processing Systems"></a><a href="http://www.cse.cuhk.edu.hk/~jcheng/5570/" target="_blank" rel="noopener">CSCI5570 - Large Scale Data Processing Systems</a></h3><p>website Account:</p><ul><li><p>Username : csci5570</p></li><li><p>Password : huskydatalab</p></li></ul><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~jcheng" target="_blank" rel="noopener">Prof. James CHENG</a> </p><p>Tutor: Tatiana Jin</p><p>Lecture/Lab: </p><ul><li>Tuesday 13:30 Lecture &amp;&amp; Lab</li><li>Wednesday 14:30 Lecture</li></ul><p>Assessment Criteria:</p><ul><li>30% Survey paper : select one topics (DDL: Dec 10, 2018)</li><li>70% project : deadline: DEC 20</li></ul><hr><h3 id="CMSC5724Data-Mining-and-Knowledge-Discovery"><a href="#CMSC5724Data-Mining-and-Knowledge-Discovery" class="headerlink" title="CMSC5724Data Mining and Knowledge Discovery"></a><a href="http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/18-fall/" target="_blank" rel="noopener">CMSC5724Data Mining and Knowledge Discovery</a></h3><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~taoyf/" target="_blank" rel="noopener">Yufei Tao</a></p><p>Tutor: Shangqi Lu</p><p>Assessment Criteria:</p><ul><li>30% Project</li><li>30% Short Tests (three times in class)</li><li>40% Final (Open-book)</li></ul><hr><h3 id="CMSC-5720-Project-I"><a href="#CMSC-5720-Project-I" class="headerlink" title="CMSC 5720 - Project I"></a>CMSC 5720 - Project I</h3><p>Instructor: <a href="http://www.cse.cuhk.edu.hk/~jcheng" target="_blank" rel="noopener">Prof. James CHENG</a> </p><p>Options:</p><ol><li>NN-descent （kn-graph的近似算法）</li><li>search with fa2ss（facebook的相似性检索库）</li><li>multiprobe with tree（基于树的哈希方法）</li><li>LSH for MZPS （lsh）</li><li>数据收集-&gt;存储-&gt;分析 系统</li><li>topic modeling on ps archetective -&gt; （LDA FlexPS-&gt;parameter server拓展）</li><li>调度算法，同步/异步 任务，在不同集群下测试算法，分布式，任务的表现</li><li>矩阵分解 Distributed MF（矩阵分解） on Actor Framework （nomad,lftf acf或者akka -&gt; cpu to gpu to scheduling）</li><li>Clustering-aware query (database query optimizer)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;CMSC5741-Big-Data-Technology-and-Applications&quot;
      
    
    </summary>
    
    
      <category term="class note" scheme="http://yoursite.com/tags/class-note/"/>
    
  </entry>
  
  <entry>
    <title>地道的口语表达积累</title>
    <link href="http://yoursite.com/2018/09/01/%E5%9C%B0%E9%81%93%E7%9A%84%E5%8F%A3%E8%AF%AD%E8%A1%A8%E8%BE%BE/"/>
    <id>http://yoursite.com/2018/09/01/地道的口语表达/</id>
    <published>2018-09-01T07:40:05.605Z</published>
    <updated>2018-09-13T08:08:18.213Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-给别人加油"><a href="#1-给别人加油" class="headerlink" title="1. 给别人加油"></a>1. 给别人加油</h3><p>不要再用fighting啦～</p><p><strong>Go get‘em.</strong></p><p>完整说法：Go get’em tiger.</p><p>追女生：go get her</p><h3 id="2-问别人想不想要"><a href="#2-问别人想不想要" class="headerlink" title="2. 问别人想不想要"></a>2. 问别人想不想要</h3><p>不要再用do you want啦～</p><p><strong>be up for something</strong></p><p>e.g: </p><p>Are you up for going clubing?</p><p>你想去蹦迪吗？</p><p>be up三个意思：</p><ul><li><p>醒着 Are you up?</p></li><li><p>下一个 who’s up next?</p></li><li><p>怎么了 what’s up？</p></li></ul><h3 id="3-和朋友约出去玩"><a href="#3-和朋友约出去玩" class="headerlink" title="3. 和朋友约出去玩"></a>3. 和朋友约出去玩</h3><p>play with不行，一般是小朋友或者人和宠物，不适合成人一起玩</p><p>hang out</p><p>还可以用chill out，自己一个人出去玩</p><p>e.g.</p><p> Do you wanna hang out with us?</p><p>###4. 如何约一场“夜生活”</p><p><strong>night out～(名词)</strong></p><p>e.g. </p><p>It’s been a while since we had a girls’ night out.</p><p>姐妹们我们已经很久没有出去玩啦</p><p>have(need) a night out.</p><p>e.g.</p><p>If you are up for a boys’ night out~</p><p>如果想约不会太晚的夜生活用：evening out</p><p>Night owl 夜猫子</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-给别人加油&quot;&gt;&lt;a href=&quot;#1-给别人加油&quot; class=&quot;headerlink&quot; title=&quot;1. 给别人加油&quot;&gt;&lt;/a&gt;1. 给别人加油&lt;/h3&gt;&lt;p&gt;不要再用fighting啦～&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Go get‘em.&lt;/strong
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>daily life in HK 2</title>
    <link href="http://yoursite.com/2018/08/30/daily%20life%20in%20HK%202/"/>
    <id>http://yoursite.com/2018/08/30/daily life in HK 2/</id>
    <published>2018-08-30T01:45:38.758Z</published>
    <updated>2018-08-30T01:45:39.446Z</updated>
    
    <content type="html"><![CDATA[<p>Sigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can’t enjoy your life. While, I come here not for enjoy the postgraduate career but for open my mind and learn something useful.</p><p>Another feeling deeps me most is if I can’t speak english well, the embarrassment will rise upon my face. In the afternoon, I met with my project teacher about the future work. From what he said, I could speculate that the former students may make him disappointed. As for project, he gave us much freedom to do what we interested in based on our foundation.</p><p>At night, I hung out with Doc.Lin, and it is my first time to meet with old friends after I came to HK. We walked around 中环 and 香港岛, where we smell at the taste of dollars. What a fame of capitalism! Then we returned to 九龙 by ship and ate dinner there. Life of Doc.Lin seemed as relaxed and funny as the time in SAP. It’s admired by everyone, right?</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Sigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can’t enjoy your life. While, I com
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>daily life in HK  1</title>
    <link href="http://yoursite.com/2018/08/27/daily%20life%20in%20HK%20%201/"/>
    <id>http://yoursite.com/2018/08/27/daily life in HK  1/</id>
    <published>2018-08-27T10:44:40.328Z</published>
    <updated>2018-08-27T15:21:07.557Z</updated>
    
    <content type="html"><![CDATA[<p>Finally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog unregularly.</p><p>The journey from SZ to HK is tremendously hard for my. I took all my packages (more than 30kg, wooo~~~) comming in HK custom. At that time, I thought the most difficulty time had been gone though. I was wrong, cause it was just the start of my suffering. The long lines of bus station made me crying, at the same time, I realized that I forgot to take some change. Of course, as a freshman of HK, I have no HK card. So I had to beg all around. After two times of tranfer, I got to Sha Tin station.</p><p>From the introduction of my room, I knew the building I live is not far from the station. While, I made the mistake for twice. Unfortunately, It lies on a unknown mountain, so I was supposed to climb mountain with huge bags.</p><p>Finally, I got in my rooms and met with my new roommate, who is a programmer, too. He is a nice guy and I was looking forward to make more friends here.</p><p>What was worth to mention is we have a free dinner organized by our college, other guys in my table feel embarrassed to pack the left-overs so I took it all back. It solved my two-day meal problems.</p><p>Never be shamed with yourself, keep on  doing what you believe in and you will make it sooner or later!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Finally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog 
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>20180811日记</title>
    <link href="http://yoursite.com/2018/08/11/title%2020180811%E6%97%A5%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/08/11/title 20180811日记/</id>
    <published>2018-08-10T16:43:11.578Z</published>
    <updated>2018-09-04T02:25:56.218Z</updated>
    
    <content type="html"><![CDATA[<p>估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！</p><p>Flag～ X</p><p>Flag~</p><p>Flag~</p><p>Flag~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！&lt;/p&gt;
&lt;p&gt;Flag～ X&lt;/p&gt;
&lt;p&gt;Flag~&lt;/p&gt;
&lt;p&gt;Flag~&lt;
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>mac在nginx下部署php遇到的坑</title>
    <link href="http://yoursite.com/2018/08/09/mac%E5%9C%A8nginx%E4%B8%8B%E9%83%A8%E7%BD%B2php%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>http://yoursite.com/2018/08/09/mac在nginx下部署php遇到的坑/</id>
    <published>2018-08-08T16:43:26.827Z</published>
    <updated>2018-08-10T16:40:03.685Z</updated>
    
    <content type="html"><![CDATA[<p>受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php不太熟悉（之前都用xampp这类软件），所以花了一点时间才搞定。</p><p>首先要明白的是，nginx本身不能处理php，它只是一个web服务器，当前端请求php时，nginx需要把界面发给php解释器处理，然后把结果返回给前端。一般地，nginx是把请求发给fastcgi管理进程处理。如nginx中配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">location ~ \.php$ &#123;</span><br><span class="line">    root           html;</span><br><span class="line">    fastcgi_pass   127.0.0.1:9000;</span><br><span class="line">    fastcgi_index  index.php;</span><br><span class="line">    fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;#这里原来不是$document_root，搞得我很蒙，还好网上查到改好了，不然会报file not found</span><br><span class="line">    include        fastcgi_params;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以要启动一个fastcgi，这里就用到了php-fpm，它是一个php fastcgi管理器，只用于php语言（旧版php的要单独下php-fpm，我用的php-fpm已经集成了这个）。</p><p>这里有很多奇怪的问题。</p><p><strong>第一次运行php-fpm</strong></p><p>failed: 找不到/private/etc/php-fpm.conf文件，</p><p>Solution:但这个目录下有个php-fpm.conf.default的文件，所以cp了正确名字的新文件</p><p><strong>第二次远行php-fpm</strong></p><p>Failed: 找不到/usr/var/log/php-fpm.log </p><p>Solution：根本没有这个目录，到conf文件里改了但是没有效果，没办法我就通过下面的命令执行php-fpm(后面都用这个命令执行)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php-fpm --fpm-config /private/etc/php-fpm.conf  --prefix /usr/local/var</span><br></pre></td></tr></table></figure><p><strong>第三次运行php-fpm</strong></p><p>Failed: No pool defined. at least one pool section must be specified in config file</p><p>Solution：到/etc/php-fpm.d/ 目录下有文件www.conf.default，cp一份名为<a href="http://www.conf的文件" target="_blank" rel="noopener">www.conf的文件</a></p><p><strong>第四次运行php-fpm</strong></p><p>Failed：端口被占用</p><p>Solution：杀掉这个进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i tcp:9000#找到占用9000端口的进程号</span><br><span class="line">kill -9 port#杀！</span><br></pre></td></tr></table></figure><p><strong>第五次运行php-fpm</strong></p><p>成功！</p><p>##补充：</p><p>在nginx上配的时候又有所一点不同，在mac上php-fpm直接listen了9000端口，但在服务器上它listen了php7.0-fpm.sock但socket文件，这种方式可能快一点，所以要在nginx上php的配置那边将</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastcgi_pass 127.0.0.1:9000;</span><br></pre></td></tr></table></figure><p>改成：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fastcgi_pass unix:/run/php/php7.0-fpm.sock;</span><br></pre></td></tr></table></figure><p>才能成功运行php</p><h3 id="继续补充"><a href="#继续补充" class="headerlink" title="继续补充"></a>继续补充</h3><p>很有意思的一个东西，要上传27m的一个视频，nginx直接报了413 Request Entity Too Large，是我没设置…</p><p>到nginx的配置（set-enabled/default）里面添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    ...</span><br><span class="line">    client_max_body_size 80m;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重读配置、重启服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx -s reload</span><br><span class="line">service nginx restart</span><br></pre></td></tr></table></figure><p>然后还要去修改php.ini，在其中修改两条配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">upload_max_filesize = 80M</span><br><span class="line">post_max_size = 80M</span><br></pre></td></tr></table></figure><p>然后关掉php-fpm的进程，再重启即可～</p><p>ps：贺老师真的完全不研究的…mp4传不上去只是在系统里没添加这种类型，这种事都要我自己去找…难受 :(</p><p><strong>note：</strong>在ubuntu下现在比较推荐用apt而不是apt-get…so，是时候改变了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php
      
    
    </summary>
    
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
  </entry>
  
  <entry>
    <title>看Husky的一点整理</title>
    <link href="http://yoursite.com/2018/08/08/Husky%E6%96%87%E6%A1%A3%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/08/Husky文档整理/</id>
    <published>2018-08-08T02:34:42.580Z</published>
    <updated>2018-09-04T06:49:23.084Z</updated>
    
    <content type="html"><![CDATA[<p>Username : csci5570</p><p>Password : huskydatalab</p><p>husky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受…）</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> Required</span><br><span class="line">master_host=master#master跑的地方</span><br><span class="line">master_port=10086#master绑定的端口</span><br><span class="line">comm_port=12306#worker绑定的端口</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Worker information</span><br><span class="line">[worker]</span><br><span class="line">info=worker1:4#worker1有4个线程</span><br><span class="line">info=worker2:4#worker2有4个线程</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>如果用了hdfs，配置hdfs路径</span><br><span class="line">hdfs_namenode=master</span><br><span class="line">hdfs_namenode_port=9000</span><br></pre></td></tr></table></figure><p>运行的时候用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./program --conf=/path/to/config.ini</span><br></pre></td></tr></table></figure><p>写入配置。</p><h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><h3 id="Object-List"><a href="#Object-List" class="headerlink" title="Object List"></a>Object List</h3><p>Object List（objList）是husky中最主要的对象，可以把任何对象都存在objlist中，两个objlist通过channel传递消息。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Obj</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> KeyT = <span class="keyword">int</span>;</span><br><span class="line">    KeyT key;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> KeyT&amp; <span class="title">id</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">    Obj() = <span class="keyword">default</span>;</span><br><span class="line">    explicit Obj(const KeyT&amp; k) : key(k) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>创建一个obj只要3步：</p><ol><li>定义一个key的类型（keyT），一般都用int</li><li>写一个id（）函数来返回该对象对应的key</li><li>需要一个默认构造函数，还需要一个能够接收key参数的构造函数</li></ol><p>接下来就可以创建、使用、删除Object List</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建名叫my_objlist的objlist</span></span><br><span class="line"><span class="keyword">auto</span>&amp; objlist = ObjListStore::create_objlist&lt;Obj&gt;(<span class="string">"my_objlist"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将obj传入创建好的objlist中</span></span><br><span class="line"><span class="function">Obj <span class="title">obj</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line">objlist.add_object(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过名字拿到对应的objlist，注意这里的auto关键字，自动判定类型，很舒服！</span></span><br><span class="line"><span class="keyword">auto</span>&amp; objlist2 = ObjListStore::get_objlist&lt;Obj&gt;(<span class="string">"my_objlist"</span>);  </span><br><span class="line"></span><br><span class="line"><span class="comment">//通过名字删除objlist</span></span><br><span class="line">ObjListStore::drop_objlist(<span class="string">"my_objlist"</span>);</span><br></pre></td></tr></table></figure><p>为了让添加在objlist中的obj被其他线程感知并利用，在多线程情况下需要将objlist全局化一下，husky已经封装好该方法</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">globalize(objlist);</span><br></pre></td></tr></table></figure><p>接下来就是<strong>最重要</strong>的一个函数list_execute（），它规定了list里的每个object需要做的事，这个函数是用户自己定义的。它有两个参数：</p><ul><li>第一个是要操作的objlist</li><li>第二个是这个objlist中每个obj要做的事，例如下面函数就是obj在log中打印id，包括之后用channel发送或接收消息也都是在这个函数里</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list_execute(objlist, [](Obj&amp; obj) &#123;</span><br><span class="line">    base::log_msg(<span class="string">"My id is: "</span> + obj.id());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>channel就是object和object互相通信的工具，他们的关系类似于城市和公路。husky中有四种channel：</p><ul><li>Push Channel：最常见的点对点通信</li><li>Push Combined Channel：在push channel基础上增加了合并发给同个obj的</li><li>Broadcast Channel：将一个key-value广播出去，任何地方都可以通过key拿到值</li><li>Migrate Channel：用来migrate对象，将一个对象发送到另一个线程上</li></ul><p>channel的创建、使用和drop（一定要主动销毁）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create PushChannel</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MsgT, <span class="keyword">typename</span> DstObjT&gt; </span><br><span class="line"><span class="keyword">static</span> PushChannel&lt;MsgT, DstObjT&gt;&amp; </span><br><span class="line">create_push_channel(ChannelSource&amp; src_list,</span><br><span class="line">                    ObjList&lt;DstObjT&gt;&amp; dst_list,</span><br><span class="line">                    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name = <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get PushChannel through name</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MsgT, <span class="keyword">typename</span> DstObjT&gt;</span><br><span class="line"><span class="keyword">static</span> PushChannel&lt;MsgT, DstObjT&gt;&amp; </span><br><span class="line">get_push_channel(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name = <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Drop channel through name</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">drop_channel</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name)</span></span>;</span><br></pre></td></tr></table></figure><p>下面通过例子来说明：</p><p>首先，要想创建channel，就要确定发消息的源objlist和目的objlist，当然，参数里的目的objlist必须是全局化的</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个push_channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ch = ChannelStore::create_push_channel&lt;<span class="keyword">int</span>&gt;(src_list, dst_list);</span><br></pre></td></tr></table></figure><p>一般来说，channel是放在list_execute（）函数里用的，要想清楚从哪个obj发，发什么，哪个obj接收（通过key来标注）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//push channel</span></span><br><span class="line"><span class="comment">//发送端代码</span></span><br><span class="line">list_execute(src_list, [&amp;ch](Obj&amp; obj) &#123;</span><br><span class="line">    ch.push(msg, key);  <span class="comment">// send msg to key</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//接收端代码</span></span><br><span class="line">list_execute(dst_list, [&amp;ch](Obj&amp; obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span>&amp; msgs = ch.get(obj); <span class="comment">// The msgs is of type std::vector&lt;MsgT&gt;, MsgT is int in this case</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//broadcast channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ch4 = ChannelStore::create_broadcast_channel&lt;<span class="keyword">int</span>, <span class="built_in">std</span>::<span class="built_in">string</span>&gt;(src_list);</span><br><span class="line">list_execute(src_list, [&amp;ch4](Obj&amp; obj) &#123;</span><br><span class="line">    ch4.broadcast(key, value);  <span class="comment">// broadcast key, value pair</span></span><br><span class="line">&#125;);</span><br><span class="line">list_execute(src_list, [&amp;ch4](Obj&amp; obj) &#123;</span><br><span class="line">    <span class="keyword">auto</span> msg = ch4.get(key);   <span class="comment">// get the broadcasted value through key.</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>###Aggregator</p><p>用来执行一些聚合操作的类，可以用来做求前k大值，统计数量，计算机器学习梯度总数等。他的构造函数需要两个参数（或以上），一个是init值，另外是lambda函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Aggregator&lt;<span class="keyword">int</span>&gt; agg(<span class="number">0</span>, [](<span class="keyword">int</span>&amp; a, <span class="keyword">const</span> <span class="keyword">int</span>&amp; b)&#123; a += b; &#125;);</span><br></pre></td></tr></table></figure><p>这个lambda函数就是aggregate的规则。</p><p>在创建完agregator后，就要使用它了。可以用update函数或者update_any函数（比update可接受参数类型多）来进行aggregator，例如</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agg.update(<span class="number">1</span>);<span class="comment">//aggregator值加1</span></span><br></pre></td></tr></table></figure><p>在聚合完之后，更新的值其实只在本地，为了让这个值在全局响应要用HuskyAggregatorFactory::sync()函数。另一种方式是通过HuskyAggregatorFactory::get_channel()来拿到通道，然后在list_execute中通过这个channel把消息传播出去，这种方法最后也会去调用sync()函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//两种方式</span></span><br><span class="line">AggregatorFactory::sync();</span><br><span class="line"></span><br><span class="line"><span class="comment">// or using aggregator channel</span></span><br><span class="line"><span class="keyword">auto</span>&amp; ac = AggregatorFactory::get_channel();</span><br><span class="line">list_execute(obj_list, &#123;&#125;, &#123;&amp;ac&#125;, [&amp;](OBJ&amp; obj) &#123; </span><br><span class="line">  ...  <span class="comment">// here we can give updates to some aggregators</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>当全局划这个聚合之后，就可以用get_value()函数得到值了</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> sum = agg.get_value()</span><br></pre></td></tr></table></figure><p>这个值是被全局共享的，所以对他的修改会影响其他executor，并可能有线程安全问题</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Username : csci5570&lt;/p&gt;
&lt;p&gt;Password : huskydatalab&lt;/p&gt;
&lt;p&gt;husky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受…）&lt;/p&gt;
&lt;h2 id=&quot;配置&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="CUHK" scheme="http://yoursite.com/tags/CUHK/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop了解一下</title>
    <link href="http://yoursite.com/2018/08/05/hadoop%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B/"/>
    <id>http://yoursite.com/2018/08/05/hadoop了解一下/</id>
    <published>2018-08-05T14:20:47.264Z</published>
    <updated>2018-09-15T02:27:39.557Z</updated>
    
    <content type="html"><![CDATA[<p>搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。</p><h3 id="map-reduce"><a href="#map-reduce" class="headerlink" title="map reduce"></a>map reduce</h3><ul><li>automatic parallezation</li><li>Fault tolerance</li><li>a clean abstraction for programmers</li></ul><p>BSP model : Bulk sychronous parallel</p><p>identity reducer？re</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Apache-Hadoop与HDFS"><a href="#Apache-Hadoop与HDFS" class="headerlink" title="Apache Hadoop与HDFS"></a>Apache Hadoop与HDFS</h3><p>Hadoop是一个大的生态系统，最主要是hdfs和一个基于mapreduce的分布式计算引擎。hdfs就是一个文件系统，在mapreduce的时候（包括用spark的时候）都需要对应文件在hdfs里。</p><p><strong>block块：</strong>HDFS在物理上是以block存储的，block大小可以通过配置参数（dfs.blocksize）来规定，默认128M，可以减少寻址开销。大文件会被切分成很多block来存，而小文件存储则不会占用整个块的空间。</p><p><strong>NameNode：</strong>是master。负责管理文件系统的namespace（可以理解是指向具体数据的文件名、路径名这种）和客户端对文件的访问。data path？</p><p><strong>(非Yarn)JobTracker：</strong>在NameNode上，协调在集群运行的所有作业，分配要在tasktracker上运行的map和reduce任务。</p><p><strong>DataNode：</strong>是slave。datanode则负责数据的存储。</p><p><strong>(非Yarn)TaskTracker：</strong>在datanode上，运行分配的任务并定期向jobtracker报告进度。</p><p><strong>流式访问：</strong>指hdfs访问时像流水一样一点一点过。这样也决定了hdfs是一次写入、多次读取的特性，同时只能有一个wirhter。这样访问方式适合做数据分析，而不是网盘这种。</p><p><strong>rack-aware（机架感知）：</strong>这是hdfs的复制策略。hdfs为了数据可靠一般会将数据复制几份（默认三份）。同一个机架的机器传输速度快，不需要通过交换机。为了提高效率，一台机器的数据会把一个备份放在同一机架（相同rack id）的机器里，另一个备份放在其他机架的机器上。机架的错误率很小，所以不影响可靠性。</p><p><strong>hdfs的特点：</strong></p><ul><li>面对构成系统的组件数目很大，所以对硬件的快速检测错误并自动回复非常重要</li><li>hdfs需要流式访问他们的数据集</li><li>运行的数据集非常大，一个典型文件大小一般在几G到几T</li><li>文件访问模型是“一次写入、多次访问”</li><li>将计算移动到数据附近闭将数据移动到计算更好</li></ul><h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>yarn其实是解决了经典mapreduce中一些问题（例如：jobtracker太累导致的可扩展性问题）的新一代hadoop计算平台。</p><p><strong>ResourceManager：</strong>代替jobtracker，以后台进程的形式运行。追踪有哪些可用的活动节点和资源，指出哪些程序应该何时或者这些资源。</p><p><strong>ApplicationMaster：</strong>代替一个专用而短暂的JobTracker。用户提交一个应用程序时，会启动applicationmaster这个轻量级进程实例来协调程序内任务（监视进度、定时向resourcemanager发送心跳数据、负责容错等），计算数据需要的资源并向resourcemanager申请。它本身也是在一个container里运行的，且可能与它管理的任务运行在同一节点上。</p><p><strong>Container：</strong>是yarn中资源的抽象，封装了某个节点上一定量的资源（如cpu和内存等资源）。它的分配是由applicationmaster向resourcemanager申请的；而它的运行则是applicationmaster向资源所在的nodemanager发起的。</p><p><strong>NodeManager：</strong>代替tasktracker。拥有很多动态创建的资源Container。容器大小取决于它所包含资源量，而一个节点上的容器数量由配置参数和除用于后台进程和操作系统以外资源总量决定。</p><h2 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h2><p>Hdfs采用了master/slave架构。一个hdfs集群由一个namenode和一群datanodes组成。简单来说，就是hdfs通过namenode暴露出了文件系统命名空间的操作，包括打卡、关闭、重命名文件等等。在这个文件系统对一块数据的操作会映射到具体的datanodes上。</p><p>所以一般是一台机器上搭namenode，然后datanode在其他各个机器上。</p><p><img src="/img/hdfsarchitecture.jpg" alt="hdfsarchitecture"></p><h3 id="Hadoop-Streaming"><a href="#Hadoop-Streaming" class="headerlink" title="Hadoop Streaming"></a>Hadoop Streaming</h3><p>java这么规范的东西有人就是不喜欢，非得用python啥的写hadoop，所以就有了hadoop streaming，支持其他语言的hadoop操作。</p><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="单节点测试"><a href="#单节点测试" class="headerlink" title="单节点测试"></a>单节点测试</h3><p>比较无聊，只是测一下能不能跑，不需要运行什么</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> Cellar/hadoop/3.1.0/libexec</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir input<span class="comment">#不能是别的名字</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar  grep input output <span class="string">'dfs[a-z.]+'</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br></pre></td></tr></table></figure><h3 id="伪分布式测试（pseudo-distributed）"><a href="#伪分布式测试（pseudo-distributed）" class="headerlink" title="伪分布式测试（pseudo-distributed）"></a>伪分布式测试（pseudo-distributed）</h3><ol><li><p>保证本机已经装好hadoop，java1.8（java9有些函数被废了会报错）</p></li><li><p>配置本机ssh，确保</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost#mac默认不允许，需要手动去打开</span><br></pre></td></tr></table></figure><p>可以运行。注意：mac默认不允许任何机器远程登录，需要到  系统偏好设置 -&gt; 共享 去勾选远程登录。</p></li><li><p>配置HDFS，包括core-site.xml文件和hdfs-site.xml文件。前者配置用于存储HDFS的临时文件目录和hdsf访问端口，后者确定复制份数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop/libexec/tmp/hadoop- $&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--如果无此目录则去mkdir一个--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>格式化HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>成功的话在tmp目录下可以看到dfs文件</p></li><li><p>启动各个节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start NameNode#启动namenode</span><br><span class="line">hdfs --daemon start DataNode#启动datanode</span><br><span class="line">hdfs --daemon start SecondaryNameNode#它是namenode的快照，保证了namenode的更新</span><br><span class="line"><span class="meta">jps#</span><span class="bash">用来查看这些节点是否真的启动了</span></span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建文件夹及文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /demo#在hdfs上创建demo文件夹</span><br><span class="line">hdfs dfs -ls /demo</span><br><span class="line">hdfs dfs -put test.input /demo#将本地的test.input文件发到hdfs上</span><br></pre></td></tr></table></figure></li><li><p>配置Yarn的mapred-site.xml和yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-sited.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>share/hadoop/mapreduce/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果不加这个property，在后面运行mapreduce任务时会报找不到包 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动resourcemanager和nodemanager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start nodemanager</span><br><span class="line">yarn --daemon start resourcemanager</span><br></pre></td></tr></table></figure><p>yarn端口是8088，可以去localhost:8088看页面</p></li><li><p>运行mapreduce任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar hadoop-mapreduce-examples-3.1.0.jar wordcount /demo/test.input /demo-output/</span><br></pre></td></tr></table></figure><p>可以到对应文件里查看运行结果</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。&lt;/p&gt;
&lt;h3 id=&quot;map-reduce&quot;&gt;&lt;a href=&quot;#map-reduce&quot; class=&quot;headerlink&quot; title=&quot;map reduce&quot;&gt;&lt;/a&gt;map r
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>Spark使用整理</title>
    <link href="http://yoursite.com/2018/08/05/spark%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/05/spark使用整理/</id>
    <published>2018-08-05T13:38:09.544Z</published>
    <updated>2018-09-24T10:08:41.377Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I read the book &lt;\<spark in="" action="">&gt;, the summation about this book will be wrote down here.</spark></p><h3 id="the-notion-of-RDD-resilient-distributed-dataset"><a href="#the-notion-of-RDD-resilient-distributed-dataset" class="headerlink" title="the notion of RDD(resilient distributed dataset)"></a>the notion of RDD(resilient distributed dataset)</h3><p>The RDD is the fundamental abstraction in spark it represents a collection of elements that is</p><ul><li>Immutable ( read-only )</li><li>Resilient ( fault-tolerant )</li><li>Distributed</li></ul><p>Immutable : allow xspark to provide important fault-tolerance guarantees in a straightforward manner.</p><p>Distributed : machines are transparents to users, so working with RDDs is not much differents from working with a lists, maps and so on.</p><p>Resilient: whereas other systems facilitate fault-tolerance by replicating data to multiple machines, RDDs provide falut-tolerant by logging the transformations used to build dataset rather than itself. When fault happens, it just need to repair a subset of dataset.</p><h3 id="Basic-RDD-Operation"><a href="#Basic-RDD-Operation" class="headerlink" title="Basic RDD Operation"></a>Basic RDD Operation</h3><p>there are two types of operations</p><ul><li>Transformations</li><li>actions</li></ul><p>Transformations : like <em>filter</em> and <em>map</em>, perform some useful data manipulation and it will produce a new RDD</p><p>actions : like <em>count</em> and <em>foreach</em>, trigger a computations to return a result.</p><h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>Dataframe is a basic elements of Spark, similarily with other Dataframe in python or R, it represent a table-like data with named columns and declared column types. The different among them is it’s distributed nature and spark’s catalyst.</p><p>Fundamental concepts:</p><ul><li>Spark Sql: Consult from Table catalog; Query from Relational DB, Read data from HDFS. Spark Application can using DataFrame DSL to submit spark job, as for non-spark client could connect though JDBC.</li><li>Table catalog: It contains information about registered DataFrames and how to access their data.</li><li>DataFrame: user reads its data from a table in a relational databases.</li></ul><p>Spark sql is supported by Hive. Hive is a distributed warehouse built as a layer of abstraction on the top of Hadoop’s MapReduce. It has its own dialect named HiveQL.</p><h4 id="Create-DataFrame"><a href="#Create-DataFrame" class="headerlink" title="Create DataFrame"></a>Create DataFrame</h4><p>We can create DataFrame in these three ways:</p><ul><li>Converting from RDDs<ul><li>Using RDDs containing row data as tuples</li><li>Using case classes</li><li>Specifying a schema</li></ul></li><li>Running SQL queries</li><li>Loading external data</li></ul><p>Using RDDs containing row data as tuples : use toDF() method to convert RDD to DataFrame. However, all columns are of type String and nullable, which obviously is a bad solution.</p><p>Using case classes : the case class like this:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Post</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">commentCount:<span class="type">Option</span>[<span class="type">Int</span>],</span></span></span><br><span class="line"><span class="class"><span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">body:<span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">......</span>)</span></span><br></pre></td></tr></table></figure><p>Nullable fields are declared to be of type Option[T].</p><p>Specify a schema: This format of schema like this:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> postSchema = <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"commentCount"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>,<span class="type">LongType</span>,<span class="literal">false</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Then invoke spark.createDataFrame(rowRDD, postSchema) to convert.</p><h4 id="DataFrame-API-Basics"><a href="#DataFrame-API-Basics" class="headerlink" title="DataFrame API Basics"></a>DataFrame API Basics</h4><p>Select:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> postIdBody = postsDF.select(<span class="string">"id"</span>,<span class="string">"body"</span>)</span><br><span class="line"><span class="keyword">val</span> postIds = postsIdBody.drop(<span class="string">"body"</span>)<span class="comment">//drop this column</span></span><br></pre></td></tr></table></figure><p>Filtering:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> noAnswer = postsDf.filter((<span class="symbol">'postTypeId</span> === <span class="number">1</span>) and (<span class="symbol">'acceptedAnswerId</span> isNull))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Recently I read the book &amp;lt;\&lt;spark in=&quot;&quot; action=&quot;&quot;&gt;&amp;gt;, the summation about this book will be wrote down here.&lt;/spark&gt;&lt;/p&gt;
&lt;h3 id=&quot;the
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>homebrew使用整理</title>
    <link href="http://yoursite.com/2018/08/02/homebrew%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/08/02/homebrew使用整理/</id>
    <published>2018-08-02T04:05:26.949Z</published>
    <updated>2018-09-13T08:07:57.002Z</updated>
    
    <content type="html"><![CDATA[<p>现在新的mac基本都内置homebrew了吧，brew可以说是mac神器之一了。上手简单，但还是用法需要整理一下：</p><p>###brew常用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">brew search 包名 #搜索包</span><br><span class="line">brew info 包名#包信息</span><br><span class="line">brew list #查看有哪些包</span><br><span class="line">brew install 包名#安装包</span><br><span class="line">brew uninstall 包名#删除包</span><br></pre></td></tr></table></figure><h3 id="brew管理服务"><a href="#brew管理服务" class="headerlink" title="brew管理服务"></a>brew管理服务</h3><p>brew还有个重要的任务就是管理服务，在我本机的：</p><ul><li>Kafka</li><li>mysql</li><li>nginx</li><li>Redis</li><li>zookeeper</li></ul><p>都是用了brew进行管理，管理他们用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew services start 服务名#开启一个service</span><br><span class="line">brew services stop 服务名#关闭一个service</span><br></pre></td></tr></table></figure><p>每次开启一个服务，就会在～/Library/LaunchAgents里面增加一个plist文件，用来存储这个服务的一些版本信息，同时，本机所有其他服务可以通过</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">launchctl load *.plist #加载 </span><br><span class="line">launchctl unload *.plist #取消</span><br><span class="line">launchctl list#查看服务</span><br></pre></td></tr></table></figure><p>来完成</p><h3 id="brew其他命令"><a href="#brew其他命令" class="headerlink" title="brew其他命令"></a>brew其他命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew link 包名</span><br></pre></td></tr></table></figure><p>这里的link是指symbollink（有点类似于windows里的创建快捷方式）。以hadoop为例，在brew刚下载的hadoop只是存在/usr/local/Cellar目录下的，在全局环境下不能用hadoop命令。只有将其link到bin里（hadoop产生了27个symbolink），才能全局使用hadoop命令。在用brew install时会默认完成link的操作，除非出现意外。</p><p>意外：在安装hadoop时出现了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Error: The `brew link` step did not complete successfully</span><br><span class="line">The formula built, but is not symlinked into /usr/local</span><br><span class="line">Could not symlink sbin/FederationStateStore</span><br><span class="line">/usr/local/sbin is not writable.</span><br></pre></td></tr></table></figure><p>是因为我本机根本没有这个目录，同时权限也不够，所以我建了这个目录，然后用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -r $(whoami) $(brew --prefix)/*</span><br></pre></td></tr></table></figure><p>修改了对应权限，成功安装。这里引出了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew --prefix</span><br></pre></td></tr></table></figure><p>这个是指brew存在的目录，其他brew操作都是在这个目录下搞的（例如cellar就是在这个目录下）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现在新的mac基本都内置homebrew了吧，brew可以说是mac神器之一了。上手简单，但还是用法需要整理一下：&lt;/p&gt;
&lt;p&gt;###brew常用命令&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gu
      
    
    </summary>
    
    
      <category term="mac" scheme="http://yoursite.com/tags/mac/"/>
    
  </entry>
  
  <entry>
    <title>hip-hop中一些slang积累</title>
    <link href="http://yoursite.com/2018/07/27/hip-hop%E4%B8%AD%E4%B8%80%E4%BA%9Bslang%E7%A7%AF%E7%B4%AF/"/>
    <id>http://yoursite.com/2018/07/27/hip-hop中一些slang积累/</id>
    <published>2018-07-27T15:25:18.747Z</published>
    <updated>2018-09-15T02:21:21.625Z</updated>
    
    <content type="html"><![CDATA[<p>听了很久trap，很多都不太懂，之后不懂的就写在这里吧～</p><p>###In my feelings - Drake （KIKI在b榜第一呆了一个月，我满耳朵都是kiki）</p><p>Henny -&gt; Hennessy：轩尼诗（酒）</p><p>wraith：幽灵、幻影（劳斯莱斯品牌）</p><p>code to the safe：保险箱密码</p><p>Neck work：类似的表达有give me some neck，指吹喇叭，等同于blow job或者sucking或者blowing of one’s dick~</p><p>Netflix and chill：一个internet meme，指约pao</p><p>net worth；资产净值（身价）</p><h3 id="Alright-Kendrick-Lamar"><a href="#Alright-Kendrick-Lamar" class="headerlink" title="Alright - Kendrick Lamar"></a>Alright - Kendrick Lamar</h3><p>Mac-11：机械手枪的型号</p><p>Pussy&amp;Benjamin：指代女人和金钱</p><p>Chevy -&gt; chevrolet 雪弗兰</p><p>Get reaping everything I sow：收获我所播种的（种豆得豆）</p><p>My karma：我的命运</p><p>Preliminary hearing：法庭的初审</p><p>fight my vice：和恶习斗争（vice除了副的还有恶习的意思）</p><p>Popo：police，条子</p><p>Preacher：牧师，传道人</p><p>Regal：君主的，这里指别克君威车</p><p>Resentment：愤恨，不满</p><p>Self destruct：自我毁灭</p><p>Lucy -&gt; Lucifer：是指撒旦，Satan, the devil（貌似只有lamar称lucifer为lucy～）</p><h3 id="Bed-Nicki-Minaj-Ariana-Grande-（沉迷黄歌，无法自拔～）"><a href="#Bed-Nicki-Minaj-Ariana-Grande-（沉迷黄歌，无法自拔～）" class="headerlink" title="Bed - Nicki Minaj/Ariana Grande （沉迷黄歌，无法自拔～）"></a>Bed - Nicki Minaj/Ariana Grande （沉迷黄歌，无法自拔～）</h3><p>wit’(with) your name on it：属于某人（不是真的指文字那种）</p><p>Sheet：床单</p><p>Carter III：指Lil Wayne备受赞誉的专辑</p><p>A Milli：Carter III专辑中的一首歌</p><p>GOAT：greatest of all time 史上最佳</p><p>turn down：拒绝（don’t turn me down不要拒绝我）</p><p>Lingerie：内衣如图，不解释</p><p><img src="/img/lingerie.png" alt="img"></p><p>blow it like a feather on you：像一片羽毛xx你（撩的不行啊～）</p><p>Starting five：指nba那种首发五人，也可以用 Starting Line-up（首发阵容）</p><p>Thick skin：脸皮厚，不怕被骂～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;听了很久trap，很多都不太懂，之后不懂的就写在这里吧～&lt;/p&gt;
&lt;p&gt;###In my feelings - Drake （KIKI在b榜第一呆了一个月，我满耳朵都是kiki）&lt;/p&gt;
&lt;p&gt;Henny -&amp;gt; Hennessy：轩尼诗（酒）&lt;/p&gt;
&lt;p&gt;wrait
      
    
    </summary>
    
    
      <category term="hiphop" scheme="http://yoursite.com/tags/hiphop/"/>
    
  </entry>
  
  <entry>
    <title>pandas使用整理</title>
    <link href="http://yoursite.com/2018/07/24/pandas%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/24/pandas使用整理/</id>
    <published>2018-07-24T14:55:02.595Z</published>
    <updated>2018-09-13T08:09:25.940Z</updated>
    
    <content type="html"><![CDATA[<p>之前看了numpy，这两天看了pandas，也在这里整理一下。</p><h3 id="新建"><a href="#新建" class="headerlink" title="新建"></a>新建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'key1'</span>:[<span class="string">'value11,value12'</span>],</span><br><span class="line">    <span class="string">'key2'</span>:[<span class="string">'value21'</span>,<span class="string">'value22'</span>]   </span><br><span class="line">       &#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line">df.index<span class="comment">#描述dataframe的index的【开始/结束）和步长，注意这个是指df的index而非数据的id</span></span><br><span class="line">df.columns<span class="comment">#行名</span></span><br><span class="line">df.dtypes<span class="comment">#类型</span></span><br><span class="line">df.size<span class="comment">#数据总数</span></span><br><span class="line">df.shape<span class="comment">#数据形式（行，列）</span></span><br><span class="line">df.ndim<span class="comment">#维度</span></span><br><span class="line">df.T<span class="comment">#转置</span></span><br></pre></td></tr></table></figure><h3 id="从数据库中得到数据-amp-amp-将数据写入数据库"><a href="#从数据库中得到数据-amp-amp-将数据写入数据库" class="headerlink" title="从数据库中得到数据 &amp;&amp; 将数据写入数据库"></a>从数据库中得到数据 &amp;&amp; 将数据写入数据库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接mysql数据库</span></span><br><span class="line">engine = create_engine(<span class="string">'mysql+pymysql://root:qh129512@127.0.0.1:3306/testdb?charset=utf8'</span>)</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'select * from person'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面有三种获取数据的方式，返回的formlist就是一个dataframe</span></span><br><span class="line">formlist = pd.read_sql_query(sql,con=engine)<span class="comment">#参数为sql语句</span></span><br><span class="line">formlist = pd.read_sql_table(table,con=engine)<span class="comment">#参数为表名</span></span><br><span class="line">formlist = pd.read_sql(sql,con=engine)<span class="comment">#参数可以是sql语句，也可以表名</span></span><br></pre></td></tr></table></figure><p>下面是将数据写入数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(data,columns=[<span class="string">'name,birthday'</span>])<span class="comment">#这里的columns就是指dataframe里有哪些comlumn，这里没有的df里也不会有，如果不添加columns这个参数就默认data中全部数据</span></span><br><span class="line"></span><br><span class="line">df.to_sql(name=<span class="string">'person'</span>,con=engine, if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment">#if_exists:有三种fail-&gt;表存在就不写入；replace-&gt;表存在就删掉原来的表重新创建；append-&gt;在原表基础上追加数据。默认为fail</span></span><br><span class="line"><span class="comment">#index：决定是否将行索引作为数据传入数据库，注意：如果数据库没有专门来存这个的就false，因为表里没有这里用true会有问题</span></span><br></pre></td></tr></table></figure><h3 id="使用dataframe"><a href="#使用dataframe" class="headerlink" title="使用dataframe"></a>使用dataframe</h3><p>直接取某个、某些数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>][<span class="number">0</span>]</span><br><span class="line">df.name[:<span class="number">5</span>]</span><br><span class="line">df[[<span class="string">'name'</span>,<span class="string">'birthday'</span>]][:<span class="number">2</span>]</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>取数据的切片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.loc[:<span class="number">5</span>,<span class="string">'name'</span>]<span class="comment">#前一个参数为行索引，后一个是列索引名称</span></span><br><span class="line">df.iloc[<span class="number">0</span>:<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#有条件的切片</span></span><br><span class="line">df.loc[(df[<span class="string">'name'</span>] == <span class="string">'max'</span>),:]<span class="comment">#取name为max的切片</span></span><br><span class="line"></span><br><span class="line">df.iloc[(df[<span class="string">'name'</span>] == <span class="string">'max'</span>).values,:]</span><br></pre></td></tr></table></figure><p>这里有两个函数，loc和iloc，区别有</p><ul><li>loc第一个参数可以为series，例如我传入一个条件，其实相当于一个Series([True,True,False…])这种形式；iloc不可以穿series，但能传一个array，所以可以通过.values的形式传入条件</li><li>行索引时loc是前后闭区间，而iloc是前闭后开区间（python中这种更常见）</li></ul><p>删除数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.drop(labels=rang(<span class="number">9</span>,<span class="number">10</span>),axis=<span class="number">0</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">labels接收string、array，表示删除行/列的标签</span></span><br><span class="line"><span class="string">axis接收0、1，表示操作轴向，0为横，1为纵</span></span><br><span class="line"><span class="string">inplace接收boolean，代表操作是否对原数据生效</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>对dataframe中数据修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接声明就可以添加一列，如</span></span><br><span class="line">df[<span class="string">'prefix_name'</span>] = <span class="string">'MAC_'</span> + df[<span class="string">'name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改某个数据可以将其找出来然后直接赋值</span></span><br><span class="line">df.loc[(df.name == <span class="string">'max'</span>),<span class="string">'name'</span>] = <span class="string">'maxhh'</span></span><br></pre></td></tr></table></figure><p>随机数的使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">series = pd.Series(np.random.randint(high=<span class="number">10000</span>,low=<span class="number">1000</span>,size=<span class="number">8</span>))<span class="comment">#产生一个随机series</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df.loc[:,<span class="string">'net-worth'</span>] = series<span class="comment">#将series值付给df</span></span><br></pre></td></tr></table></figure><p>dataframe算数统计</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'net-worth'</span>].mean()<span class="comment">#平均值</span></span><br><span class="line">np.mean(df[<span class="string">'net-worth'</span>])<span class="comment">#另一种计算平均值的方法</span></span><br><span class="line">df[<span class="string">'net-worth'</span>].min()<span class="comment">#最小值</span></span><br><span class="line">df[<span class="string">'net-worth'</span>].describe()<span class="comment">#描述，包含很多数据可以用！</span></span><br><span class="line">nullNum = df.shape[<span class="number">0</span>] - df[<span class="string">'name'</span>].count()<span class="comment">#统计有多少空值</span></span><br></pre></td></tr></table></figure><h3 id="Category类型的使用"><a href="#Category类型的使用" class="headerlink" title="Category类型的使用"></a>Category类型的使用</h3><p>可以将某一列转化成category类型，这样相当于做了一次分类处理，这样得到的描述性信息会很多</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>] = df[<span class="string">'name'</span>].astype(<span class="string">'category'</span>)<span class="comment">#注意这里是赋值而不是调用</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">'name'</span>].describe()</span><br></pre></td></tr></table></figure><h3 id="时间类型"><a href="#时间类型" class="headerlink" title="时间类型"></a>时间类型</h3><p>pandas里有很多时间类型，不同类型用处不同。如timestamp主要用来记录时间，而timedelta用来做时间运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#有很多方法创建或转化出一个timestamp</span></span><br><span class="line">today_date = pd.to_datetime(<span class="string">'2018-8/5'</span>)<span class="comment">#随意的一个string都可以识别</span></span><br></pre></td></tr></table></figure><p>注意一个概念，从数据库一个datatime拿出来的时间如果调用dtype的话发现是(‘&lt;M8[ns]’)类似的类型，展开来说：</p><p>这个是属于机器的比较特别的类型:<br>小端机器的类型：datatime[ns] == &lt;M8[ns]<br>大端机器的类型：datatime[ns] == &gt;M8[ns]<br>这个可以通过</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.dtype(<span class="string">'datetime64[ns]'</span>) == np.dtype(<span class="string">'&lt;M8[ns]'</span>)</span><br><span class="line"><span class="comment">#out: True</span></span><br></pre></td></tr></table></figure><p>证明。<br>所以当数据库中取出就是这种类型时，不需要再进行处理，但如果取出是个object，则用pd.to_datatime处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'birthday'</span>] = pd.to_datatime(df[<span class="string">'birthday'</span>])</span><br></pre></td></tr></table></figure><p>这里要熟悉的操作有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user_birthday = [i.year <span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">'birthday'</span>]]<span class="comment">#返回所有年份的list</span></span><br><span class="line"></span><br><span class="line">birthday = df[df[<span class="string">'birthday'</span>]&lt;=pd.datetime(<span class="number">1991</span>,<span class="number">1</span>,<span class="number">1</span>)]<span class="comment">#按条件取时间时，一定要那拿timestamp与对应的pdf.datetime()相比</span></span><br></pre></td></tr></table></figure><p>还有时间做加减法也是支持的,timestamp可以加一个tmiedelta来做时间的计算。注意timedelta的参数为weeks,days,hours以及更小的时间<br>直接用+，-符号就可以做计算了。相反的，想算两个时间差，只要用两个datetime做减法，即可得到timedelta类型的时间差距</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">aweek_after = date + pd.Timedelta(weeks = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">today_date = pd.to_datetime(<span class="string">'2018-8-5'</span>)</span><br><span class="line"></span><br><span class="line">time_delta = aweek_after - today_date<span class="comment">#两个timestamp做减法，得到了一个timedelta类型的时间差</span></span><br></pre></td></tr></table></figure><h3 id="分组和聚合"><a href="#分组和聚合" class="headerlink" title="分组和聚合"></a>分组和聚合</h3><p>分组顾名思义，就是将数据按照一定条件进行分组，得到数据整体情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_group = df.groupby(by=<span class="string">'birthday'</span>)</span><br><span class="line">data_group.count()</span><br></pre></td></tr></table></figure><p>这里的count()得到如下表结果</p><table><thead><tr><th></th><th>id</th><th>name</th><th>net-worth</th></tr></thead><tbody><tr><td>birthday</td><td></td><td></td><td></td></tr><tr><td>1987-01-10</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1989-03-10</td><td>6</td><td>0</td><td>4</td></tr><tr><td>1993-08-05</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1995-12-12</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1996-10-12</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>分组的直接结果并不能直接看，因为它返回的只是一个地址，但可以方便的查分组后的一些属性，如：count，head，max等等</p><p>聚合，就是将一组数据做aggretate，聚合的函数自己定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'net-worth'</span>].agg([np.sum,np.mean])<span class="comment">#针对net-worth数据计算两种agg，分别以sum和mean</span></span><br><span class="line"></span><br><span class="line">df.agg(&#123;<span class="string">'net-worth'</span>:[np.sum,sp.mean]&#125;)<span class="comment">#针对不同数据要进行不同的agg，可以用key-value的方式</span></span><br></pre></td></tr></table></figure><p>agg函数的参数是计算函数，这个函数可以用numpy中一些简单的统计函数，如果复杂也可以自定义，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Trinum</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.sum()*<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">df[<span class="string">'net-worth'</span>].agg(Trinum)<span class="comment">#agg函数的参数可以是函数，该函数将data传入做处理，然后返回即可</span></span><br><span class="line"><span class="comment">#稍微注意下sum后面的括号，没有括号是函数，有括号的是执行，但必须声明参数</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">'net-worth'</span>].agg(<span class="keyword">lambda</span> x:x*<span class="number">2</span>)<span class="comment">#agg的参数可以是lambda函数</span></span><br></pre></td></tr></table></figure><p>这里的一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupby(by=<span class="string">'birthday'</span>).agg(np.mean)<span class="comment">#完全等价于data_group.mean()，都是求每组的平均值</span></span><br></pre></td></tr></table></figure><p>当然，apply函数、transform函数和agg函数也大部分相同，但apply方法不能用key-value类型来特定的处理，transform方法只有一个参数function</p><h3 id="创建透视表"><a href="#创建透视表" class="headerlink" title="创建透视表"></a>创建透视表</h3><p>可以通过pandas创建透视表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.pivot_table(df[[<span class="string">'id'</span>,<span class="string">'name'</span>,<span class="string">'net-worth'</span>]],index=[<span class="string">'id'</span>],columns=<span class="string">'name'</span>,fill_value=<span class="number">0</span>)<span class="comment">#不显示无index的值</span></span><br></pre></td></tr></table></figure><p>得到</p><table><thead><tr><th></th><th>net-worth</th><th></th><th></th><th></th></tr></thead><tbody><tr><td>name</td><td>hape</td><td>john</td><td>johnny</td><td>max</td></tr><tr><td>id</td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>6355</td></tr><tr><td>2</td><td>0</td><td>6567</td><td>0</td><td>0</td></tr><tr><td>4</td><td>0</td><td>0</td><td>8491</td><td>0</td></tr><tr><td>8</td><td>6872</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><p>但这不是pandas的重点，略～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前看了numpy，这两天看了pandas，也在这里整理一下。&lt;/p&gt;
&lt;h3 id=&quot;新建&quot;&gt;&lt;a href=&quot;#新建&quot; class=&quot;headerlink&quot; title=&quot;新建&quot;&gt;&lt;/a&gt;新建&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="data science" scheme="http://yoursite.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>看了一点zookeeper的心得</title>
    <link href="http://yoursite.com/2018/07/20/zookeeper%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/20/zookeeper整理/</id>
    <published>2018-07-19T16:10:42.509Z</published>
    <updated>2018-09-15T02:20:32.623Z</updated>
    
    <content type="html"><![CDATA[<p>###Fast Paxos算法：</p><p>这个还没看，先留个坑把。</p><p>###文件结构：</p><p>zookeeper的文件结构大概是这个样子的：</p><p>!å¾ 1 Zookeeper æ°æ®ç”æ](<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/image001.gif" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/image001.gif</a>)</p><h3 id="znode（data-node）："><a href="#znode（data-node）：" class="headerlink" title="znode（data node）："></a>znode（data node）：</h3><ol><li>其中的每个子目录就是一个znode，他们也可以有子的znode（临时znode除外）。</li><li>这些znode必须是绝对路径，不允许相对路径。</li><li>每个znode都维护一个stat structure（linux系统文件的结构），其中包括版本号，acl改变等等。每次更新数据会让版本号自增。</li><li>每个znode可以设置一个watches，当watch触发后，zookeeper将发给client一个提醒。</li><li>在znode中数据读写是原子性的。每个znode都有一个ACL（access control list）来控制其读写权限。zookeeper不是用来做数据库的，其中存储的数据可能都是几kb的配置/状态信息。大块数据都存在hdfs中。</li><li>Ephemeral node就是临时节点，每次会话结束这些节点都会清空，不允许有子节点。</li></ol><p>###Zookeeper Session：</p><p><img src="/img/state_dia.jpg" alt="state_dia"></p><p>###一致性的保证：</p><p>zookeeper是一个非常高效、可扩展的服务。其一致性靠以下几点保证：</p><ol><li>顺序的一致性。一个client的更新会依序发送给其他。</li><li>原子性。更新是原子操作，只有成功和失败，没有中间过程。</li><li>服务器会看到完全相同的服务，不论其连接了哪个服务器。</li><li>可靠性。一旦一个更新被部署，他会一直存在，直到被另一个更新覆盖。</li><li>及时性。client对系统的观察在一个时间段内将是最新的。也即系统的改变在这个时间段内client是看见的，或者可以探测到它失败。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;###Fast Paxos算法：&lt;/p&gt;
&lt;p&gt;这个还没看，先留个坑把。&lt;/p&gt;
&lt;p&gt;###文件结构：&lt;/p&gt;
&lt;p&gt;zookeeper的文件结构大概是这个样子的：&lt;/p&gt;
&lt;p&gt;!å¾ 1 Zookeeper æ°æ®ç”æ](&lt;a href=&quot;https:
      
    
    </summary>
    
    
      <category term="big data" scheme="http://yoursite.com/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>numpy使用整理</title>
    <link href="http://yoursite.com/2018/07/18/numpy%E4%BD%BF%E7%94%A8%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/07/18/numpy使用整理/</id>
    <published>2018-07-18T02:26:21.042Z</published>
    <updated>2018-09-13T08:07:30.513Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了一些numpy的基础，在这里整理一下。</p><h3 id="创建："><a href="#创建：" class="headerlink" title="创建："></a>创建：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#数组</span></span><br><span class="line">array1 = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">np.eye(<span class="number">3</span>)<span class="comment">#单位多维数组</span></span><br><span class="line">np.diag([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])<span class="comment">#对角多维数组</span></span><br><span class="line">np.arange(<span class="number">1</span>,<span class="number">4</span>)<span class="comment">#[1,2,3]数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#矩阵</span></span><br><span class="line">np.mat(<span class="string">"1 2 3;4 5 6;7 8 9"</span>)<span class="comment">#矩阵运算与多维数组运算结果不同，所以要用mat建矩阵，用分号隔开数据</span></span><br><span class="line">matrix = np.mat(array1)<span class="comment">#可以用多维数组初始化矩阵</span></span><br><span class="line">matrix1 = np.bmat(<span class="string">"array1 array2;array1 array2"</span>)<span class="comment">#创建分块矩阵</span></span><br></pre></td></tr></table></figure><h3 id="随机数："><a href="#随机数：" class="headerlink" title="随机数："></a>随机数：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.random(<span class="number">100</span>)<span class="comment">#完全随机</span></span><br><span class="line">np.random.rand(<span class="number">5</span>,<span class="number">5</span>)<span class="comment">#5*5均匀分布</span></span><br><span class="line">np.random.randn(<span class="number">5</span>,<span class="number">5</span>)<span class="comment">#5*5正态分布</span></span><br><span class="line">np.random.randint(<span class="number">2</span>,<span class="number">50</span>,size=(<span class="number">2</span>,<span class="number">3</span>),dtype=<span class="string">'l'</span>)<span class="comment">#大于等于2小于50的2*3的int64型整数</span></span><br></pre></td></tr></table></figure><h3 id="变换数组形态："><a href="#变换数组形态：" class="headerlink" title="变换数组形态："></a>变换数组形态：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr.reshape(<span class="number">3</span>,<span class="number">3</span>)<span class="comment">#转变成3*3的数组，但要求原数组必须9个元素，否则不能reshape</span></span><br><span class="line">np.hstack((arr1,arr2))<span class="comment">#横向组合</span></span><br><span class="line">np.vsplit(arr4,<span class="number">3</span>)<span class="comment">#横向切割，即把横向由1列的变成3列（相当于横着切）</span></span><br></pre></td></tr></table></figure><h3 id="文件存储与读取"><a href="#文件存储与读取" class="headerlink" title="文件存储与读取"></a>文件存储与读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#二进制存储与读取</span></span><br><span class="line"><span class="comment">#存储</span></span><br><span class="line">file = <span class="string">"./temp/save_arr.npy"</span></span><br><span class="line">filez = <span class="string">"./temp/save_arr.npz"</span></span><br><span class="line">np.save(file,arr)<span class="comment">#用save存，文件扩展名.npy，只能存一个数组</span></span><br><span class="line">np.savez(filez,arr1[,arr2...])<span class="comment">#用savez存，文件扩展名.npz，可以存多个数组。注意：不按照要求扩展名，则系统自己添加对应扩展名；二进制存储的数组打开文件看不到真实数据</span></span><br><span class="line"><span class="comment">#读取</span></span><br><span class="line">loaded_data = np.load(file)<span class="comment">#存储可以省略扩展名，读取一定不可以</span></span><br><span class="line">loaded_dataz = np.load(filez)</span><br><span class="line">loaded_dataz[<span class="string">"arr_0"</span>]<span class="comment">#对于多个文件读取，这种方式可以得到单独数组</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#文件存储与读取</span></span><br><span class="line"><span class="comment">#存储</span></span><br><span class="line">np.savetxt(fname,x,fmt=<span class="string">'%d'</span>,delimiter=<span class="string">','</span>,newline=<span class="string">'\n'</span>,header=<span class="string">''</span>,footer=<span class="string">''</span>,comments=<span class="string">'# '</span>)<span class="comment">#x为要存的数组，fmt='%d'表示整数方式存，delimiter表示存储时的分隔符，存储和读取时默认为空格</span></span><br><span class="line"><span class="comment">#读取</span></span><br><span class="line">loaded_data = np.loadtxt(fname,delimiter=<span class="string">","</span>)<span class="comment">#一定也要带上啊delimiter且与文件中的分隔符一致</span></span><br></pre></td></tr></table></figure><h3 id="利用numpy做简单的统计分析"><a href="#利用numpy做简单的统计分析" class="headerlink" title="利用numpy做简单的统计分析"></a>利用numpy做简单的统计分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)<span class="comment">#种子是伪随机数的开头，相同种子对应随机数都相同，一般种子会设为当前时间，确保得到真随机数（numpy默认也是这样）</span></span><br><span class="line">arr = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">arr.sort()<span class="comment">#这个会直接将arr排序</span></span><br><span class="line">arr.sort(axis=<span class="number">0</span>)<span class="comment">#二维数组的sort参数axis可以为0、1，分别对应数组纵向和横向的排序</span></span><br><span class="line"><span class="comment">#去重</span></span><br><span class="line">np.unique(arr)</span><br><span class="line"><span class="comment">#重复</span></span><br><span class="line">np.tile(arr,<span class="number">3</span>)<span class="comment">#arr是重复哪个，3是重复次数</span></span><br><span class="line">np.repeat(arr,<span class="number">3</span>,axis=<span class="number">0</span>)<span class="comment">#axis是重复的方向（tile没有这个参数）</span></span><br><span class="line"><span class="comment">#注意：tile是对数组进行重复，repeat则是对每一个数组的每一个元素进行重复，打破了原来的数组。</span></span><br><span class="line"><span class="comment">#常用统计函数</span></span><br><span class="line">np.sum(arr)<span class="comment">#求和</span></span><br><span class="line">arr.sum(axis=<span class="number">0</span>)<span class="comment">#沿纵轴求和</span></span><br><span class="line">np.mean(arr)<span class="comment">#计算数组均值</span></span><br><span class="line">arr.mean(axis = <span class="number">0</span>)<span class="comment">#沿着纵轴计算数组均值</span></span><br><span class="line">np.std(arr)<span class="comment">#计算标准差</span></span><br><span class="line">np.var(arr)<span class="comment">#计算方差</span></span><br><span class="line">np.min(arr)<span class="comment">#计算最小值</span></span><br><span class="line">np.max(arr)<span class="comment">#计算最大值</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近看了一些numpy的基础，在这里整理一下。&lt;/p&gt;
&lt;h3 id=&quot;创建：&quot;&gt;&lt;a href=&quot;#创建：&quot; class=&quot;headerlink&quot; title=&quot;创建：&quot;&gt;&lt;/a&gt;创建：&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;tabl
      
    
    </summary>
    
    
      <category term="data science" scheme="http://yoursite.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>最近在看的东西</title>
    <link href="http://yoursite.com/2018/07/16/20180716%E6%97%A5%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/16/20180716日记/</id>
    <published>2018-07-16T09:12:39.376Z</published>
    <updated>2018-08-10T10:38:03.271Z</updated>
    
    <content type="html"><![CDATA[<p>最近都没有写博客，想更一篇了。</p><p>前几天在刷算法，这几天看了一些springboot的东西。之前对spring了解的也比较多，而且这次看的也比较浅，就这样吧。</p><p>感觉很慌。马上要找工作了，我却还没开始工作…</p><p>spring-boot用的时候再看吧，</p><p>算法还是要接着刷，</p><p>接下来就做我的python了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近都没有写博客，想更一篇了。&lt;/p&gt;
&lt;p&gt;前几天在刷算法，这几天看了一些springboot的东西。之前对spring了解的也比较多，而且这次看的也比较浅，就这样吧。&lt;/p&gt;
&lt;p&gt;感觉很慌。马上要找工作了，我却还没开始工作…&lt;/p&gt;
&lt;p&gt;spring-boot用的时
      
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
</feed>

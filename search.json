[{"url":"/2019/02/26/kafka架构/","content":"\nkafka是一个高性能，高可靠性的消息系统，具体来说：\n\n- kakfa维护了不同topics的消息流\n- 支持对消息流的发布/订阅\n- 通过多服务器备份进行FT\n- 高效I/O，batch和compression\n- 对消费者和生产者解耦\n\n另外，kafka还提供了消息存储的功能，一定程度上可以当一个分布式存储系统用，它将数据存储在disk上\n\n## kafka架构\n\n![kafka_core](/img/kafka_core.png)\n\nKafka主要组成部分：\n\n- Broker：kafka集群的服务器，类似于node的概念，一个broker上可以有多个topics\n- Topic：每条发布到kafka的消息都有一个类别，这个类别就是topic\n- Commit Log：由message，offset等组成\n- Producer：生产者，向broker发消息\n- Consumer：消费者，从broker取消息\n- offset：在partition中的每条信息都会被分配一个唯一的顺序id叫做offset，consumer会track这个offset\n- Partitions：topics在物理上的分组，每个topic可能分成多个partition，每个partition都是有序的队列，kafka只保证一个partition的顺序，而不能保证partition间的顺序，具体消息会到哪个partition，由hash算法或者round robin决定\n- Zookeeper：用来存储和维护集群配置信息，选举算法和集群切换；在0.8以前还支持追踪offset，后面被取代\n\n![kafka_cluster](/img/kafka_cluster.png)\n\n在这个图中只有一个topic：zerg.hydra，它包括三个partition，分别是P0，P1，P2，每个partition在不同broker上又有replication，这里每个partition有3个replica，但其中只有一个是leader（图中边框加粗），对这个partition所有的读写只能走leader，针对每个partition的读写都可以并发进行。\n\n## Kafka特性\n\n### 保证：\n\n- kafka保证了每个topic的partition的内容会按照其发送的顺序增加\n- consumer看到信息的顺序和其存储在log中的顺序相同\n- 如果想保证全局顺序，有两种方案：只使用一个parttion；在consumer应用中增加全局顺序的处理（例如在storm的topology中）\n\n### 消息传递：\n\nAt Least Once(default)：消息不会丢失但可能被多次递送，为了做到这个，需要满足：\n\n- 在发送信息的时候保证信息的持久性\n- 在读取信息时保证持久性\n- produce发送但没收到ack时->检查最近的commit值\n- consumer要保存offset\n\n消息传递的方式：\n\n- 消息广播：把消息发送给所有consumer\n- 订阅发布：消息发送个单个consumer\n\n通过consumer group的方式实现\n\n![consumer_group](/img/consumer_group.png)\n\n对于订阅发布，每个consumer对应一个独立的consumer group即可，而对于广播，则可以把所有的consumer放在一个consumer group里。不论每个group里有多少个consumer thread（小于等于partitions），对应多个partition，这个consumer thread总会消费全部的partition。因此，最好的设计便是consumer group下的thread与partition数量相等，这样效率最高。\n\n发送信息：\n\n- 可以选择同步/异步，同步在发送时会进入阻塞，异步可以实现高吞吐率\n- 异步时将信息batching化\n- 当queue满之后异步producer可能造成消息的丢失\n\n### 发现leader：\n\n- producers：不与zookeeper直接接触，而是通过bootstraping的方式，让boostrap server（broker）通知producer有哪些存活的broker以及如何找到对应的partition leader。而boostrap broker是通过zk来知道的。\n\n### Rebalancing：\n\n每个topic的partition对应的conusmer是在运行时动态分配的，这个过程会造成rebalancing（因为机器会坏或者consumer会因为一些原因挂掉）会发生rebalance的情况包括：\n\n- consumer加入或离开consumer group（例：通过createMessageStreams()为topic注册consumer）\n- broker加入或离开（例：当broker无法发送heartbeat给zookeeper）\n- topic由于filter而造成的 ”加入/离开“\n\n## 为什么kafka那么快？\n\n- Zero Copy的使用：即使用sendfile()系统调用代替了传统I/O中的read()和write()，使数据由MDA从磁盘直接读取到内核buffer中，然后从内核buffer直接复制到socket buffer（而不用从kernel buffer->application buffer->socket buffer），减少了其中的copy和上下文切换\n- I/O过程中batch化数据\n- 串行硬盘写入\n- 大量依赖linux PageCache（页缓存）：自动使用机器上的所有可用内存\n- 可扩展性强\n\n## 注意：\n\n每次启动kafka前需要先启动zookeeper，kafka和zookeeper都需要在每台机子上都运行start程序（不同于hdfs）","tags":["big data"]},{"title":"Pig & Hive使用整理总结","url":"/2019/02/13/Pig && Hive 简介/","content":"\n## Pig && Hive 简介\n\npig和hive简单来说是使用类sql的语言来做mapreduce jobs数据查询与处理的框架。对于比较复杂的MR程序，可以直接使用缝钻噶后的Pig Latin或者HQL来做，由框架将其转化为hadoop jobs。pig属于大规模数据处理系统，而hive则属于hadoop的一个数据仓库应用。\n\n## Pig\n\n虽然被淘汰了，但还是要简单了解一下pig，毕竟是曾经风靡一时的框架。先简单看一下pig的代码：\n\n```sql\nusers = load 'users.csv' as (username: chararray, age: int);\nusers_1825 = filter users by age>=18 and age<=25; -- 这里的filter和上面的load可以在一个map里完成，减少shuffle内容\n\npages = load 'pages.csv' as (username: chararray, url: chararray);\n\njoined = join users_1825 by username, pages by username;\ngrouped = group joined by url;\nsummed = foreach grouped generate group as url, COUNT(joined) as views;\nsorted = order summed by views desc;\ntop_5 = limit sorted 5;\n\nstore top_5 into 'top_5_sites.csv';\n```\n\n### Pig 好处：\n\n使用pig latin的好处有：\n\n1. pig latin懒加载，可以在pipeline中存储任意节点，而sql无法存储中间结果（可以说pig latin是将sql分拆成单个子语句）。\n2. 使用pig latin而不是直接写hadoop jobs可以极大的减少代码量，减轻程序员负担。\n3. pig latin的运行效率不会太慢，同一个job所用时间不超过原生MR jobs的两倍。\n\n### 语言特性：\n\n- Keywords：Load, Filter, Foreach, Group By等，与sql相似\n- Aggregations：Count, Avg, Sum,Max等，这些操作一般来说会shuffle job\n- Schema：在load数据的时候就需要声明schema\n- User Defined Functions(UDFs)：用户定义的函数，需要先定义在jar包里，然后在pig latin里用register注册对应jar包从而进行调用\n\n### Pig 简单架构如图：\n\n![pigArc](/img/pigArc.png)\n\n注意：基于效率原因，pig后面使用Tez取代直接做mapreduce jobs。\n\n## Tez\n\nHadoop2.x对hadoop的架构进行了大规模的改动，其中比较重要的是：\n\n- 使用Yarn进行资源管理\n- 将Tez作为Pig和hive的执行引擎（而不是直接用mapreduce）\n- 提供了各种各样的用户api，支持更多应用\n\n![hadoop2.x arc](/img/hadoop2.x arc.png)\n\n### Tez执行步骤：\n\nTez是一个基于DAG的框架，具体运行pig步骤为：\n\n1. 将pig的数据处理过程表示成一个DAG图\n2. Tez本身可以用来执行基于DAG的计算过程，在此过程中可以优化mapreduce的流程，提高效率。\n\n## Hive\n\nhive是一个基于hadoop的数据仓库组件，提供了总结，查询和分析等功能。实际使用时，Hive很像SQL database，HQL的命令也和SQL很像。\n\n### Hive安装与启动\n\n1. 从apache官方下载hive\n2. 在conf/下新建hive-site.xml，配置mysql，hdfs等相关设置\n3. 修改conf/hive-env.sh，配置HADOOP_HOME和HIVE_CONF_DIR\n4. hive直接进入cli，hiveserver2 & 可以开启hive server\n\n### Hive数据模型\n\nhive处理的是结构化的数据，有三个层次：\n\n- Tables（path/warehouse/t）\n  - 每列都有类型（int，string，boolean）\n  - 与关系型数据库的table很像\n  - 每个表对应了hdfs的一个文件目录\n  - 支持list、map，类似json的数据\n- Partitions (path/warehouse/t/2)\n  - 由数据在表里的分布决定（例如基于范围的partition）\n  - 每个partition在表的目录下都有自己的子目录\n- Buckets (path/warehouse/t/2/part-00000.part)\n  - partition可以进一步被分成buckets\n  - 每个bucket被存在目录下的一个文件里\n\n### HiveQL命令\n\nHQL的命令主要有三类\n\n- Data Definition Language(DDL)：例如create，table，drop table等\n- Data Manipulation Language(DML)：load和insert命令等\n- Query Statements：select，join，union等\n\n#### Create:\n\n```:arrow_double_up:\nCreate table student(\n\tid int,\n\tname string\n)\npartitioned by (province string)\nclustered by (id)\nsorted by (id)\ninto 4 buckets\nrow format delimited\nfields terminated by '\\t'\nstored as textfile;\n```\n\n\n\n在使用create时要声明：\n\n1. schema：（类似RDBMS）\n2. partitions：partitioned by(province string)来确定partitions的名字，其中partition的列名不能是schema中已有的\n3. bucket：则通过clustered/sorted by (id)来实现，其中的列名必须是schema中已有的，还要明确分成几个bucket\n4. format：指原数据的分隔符是什么，如果表声明为'\\t'，而text文件用空格分开，则load的数据皆为null\n5. stored：存储形式，推荐用orc\n\n#### Load:\n\n```sql\nLoad data local inpath 'path of local file system' overwrite into table student partition(province='shanxi') -- 加载本地text文件\nLoad data inpath ... -- 加载hdfs里的文件\n\n```\n\n对于有partition的表，在load数据时一定要声明partition，否则会报错\n\n### Hive整体架构\n\n![hive arc](/img/hive arc.png)\n\n注意hive主要的组件包括：\n\n- Shell：也就是CLI，允许用户像RDBMS一样交互查询\n- Driver：session的handle、fetch和execute\n- Compiler：针对hql的parse，plan和optimize\n- Execution：基于stage的DAG图（mr，hdfs，metadata）\n- Metastore：包括表定义、表的命名空间和partition信息等，可以存在mysql等关系型数据库里\n\n### Problem\n\n**mysql会报关于ssl访问的warning，但不影响操作？**\n\n在hive-site.xml里的mysql配置里声明useSSL=false，如下：\n\n```xml\n<property>\n\t<name>javax.jdo.option.ConnectionURL</name>\n\t<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value> <!--注意这里的&是用了&amp;否则报错-->\n<property>\n\n```\n\n\n\n**update和delete无法使用，报错信息【Attempt to do update or delete using transaction manager that does not support these operations.】?**\n\n要明白hive的文件存储格式主要有：\n\n- textfile：默认格式，行存储，磁盘开销大，数据解析开销大，hive无法进行合并和拆分，压缩的text文件，load最快\n- sequencefile：二进制，以<key，value>形式序列化，行存储，可分割压缩，与hadoop api中mapfile兼容，需要通过text来转化load\n- rcfile：数据按行分块，每块列存储，读尽量涉及最少block，性能不如sequencefile，load最慢\n- orc：数据按行分块，每块列存储，效率比rcfile高（推荐）\n- 自定义格式\n\n这四种，只有orc是支持update和delete操作的，其他都不可以。我之前stored as textfile，自然报错。\n\n**hive分bucket的时候mr jobs没动，如何处理？**\n\n我使用了tez替代hive-mr来作为hive的执行引擎，网上说这样可以提高3倍效率。\n\n**hive on tez如何配置？**\n\n1. apache官网下载tez\n\n2. 在hadoop/etc/hadoop/下创建tez-site.xml文件，加上配置：\n\n   ```:arrow_double_down:\n   <configuration>\n   \t<property>\n   \t\t<name>tez.lib.uris</name>\n   \t\t<value>/user/tez/tez.tar.gz</value>\n   \t</property>\n   </configuration>\n   ```\n\n3. 把位于apache-tez-0.9.0-bin/share下的tez.tar.gz放在hdfs的/user/tez下\n\n4. 修改hadoop-env.sh追加下面配置\n\n   ```:arrow_double_down:\n   TEZ_CONF_DIR=xxx/tez-site.xml\n   TEZ_JARS=xxx/apache-tez-0.9.0-bin\n   export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*\n   ```\n\n5. 重启hadoop以及hive\n\n6. hive里写set hive.execution.engine=tez; (可以直接配在hive-site.xml里，如果想恢复可以写set hive.execution.engine=mr;)\n\n**如何开启Hive CLI的debug模式？**\n\nhive --hiveconf hive.root.logger=INFO,console 启动hive，这样可以直接在cli里看到log内容\n\n","tags":["big data"]},{"title":"Zookeeper使用整理","url":"/2019/01/22/Zookeeper简介/","content":"\n好久没更新博客，主要是前段时间看了一点机器学习的东西，公式不好打字，后来准备面试又看了很多java的基础，过几天会把java的基础整理一下丢在博客上。\n\n今天主题是zookeeper，bigdata课上老师讲了zookeeper的内容，再加上自己也试了一下，就放在这里整理了。整理只为了自己看，离精通或者熟悉还差着一万八千里呢！\n\n## Zookeeper简介\n\n### 用途\n\nzk是一个高扩展和高可用的服务，主要用来支持\n\n- Distributed configuration\n- Consensus\n- Group Membership\n- Leader Election\n- Naming\n- Coordination\n\n### 基础架构\n\nZookeeper分布在不同的服务器上，每个节点是一个server，其中有一个会作为leader，如图：\n\n![zookeeperArc](/img/zookeeperArc.png)\n\n具体来说，zk的架构有如下特点：\n\n- 所有的server都有一份数据、logs、硬盘snipshots的拷贝，被存在内存数据库中（它并不存储真正的数据，所以上面空间通常只有几十M，仅用来存必要的一些配置等）\n- 在启动时会选择一个作为leader\n- client可以选择任何一个server链接\n- 当大多数（超过一半）server完成了一个改变，那这个修改就被认为成功，并返回update response（可能造成读取旧数据）\n\n### ZK的数据模型\n\nzk的service到底是什么呢？其实就是一个被全部server追踪的分布式文件系统（这个角度来看有点像hdfs），它也是层次命名空间（类似linux），上面的每个节点称为一个znode。每一个znode都存储着一定的数据，并且可能有子的znode。\n\nZnode的主要特点：\n\n- 基于Key对象来实现/维护分布式一致性信息\n- 通过数据改变的版本信息、ACL改变和timestamp来维护一致性\n- 每次改变版本号都会增加\n- Znode不是用来存数据的，只是用来存储一些configuration和meta-data\n- 每个znode可以存timestamp和version信息\n\nZnode的类型：\n1. Persist vs. Ephemeral\n    - persist节点：一旦被创建不会轻易丢失，即使数据库重启也依然在，每个persist可以包含数据，也可以包含子节点\n    - ephemeral节点：在session结束或过期后自动删除。服务器的重启也会导致ephemeral接触（可以用于做分布式锁）\n2. Sequence vs. Non-sequence\n    - sequence节点：创建出的节点名在指定名称后带有10位10进制数的序号。多克客户端创建同一名称的节点时，都能创建成功，只是序号不同\n    - non-sequence节点：多客户端在同时创建同一non-sequence节点时，只有一个可创建成功，其他失败。创建出的节点与指定名称完全相同\n\nWatch机制：\n\n- 主动推送：当watch被触发时，zk服务器辉主动将更新推送给client，而不是靠client的轮询（观察者模式）\n- 一次性触发：watch只会被触发一次，后续更新通知必须重新注册一个watch\n\nSession的特点：\n\n- session是每个client到server的连接，zk支持每个client在保存同个session的基础上切换到不同的server\n- 内建超时机制\n- 执行顺序的保证是基于同个session的（即会话一致性）\n\n## Zookeeper的一致性\n\n### 数据一致性：\n\n一致性是指多副本中数据的一致性，以下是个简单整理（程度由强到弱）：\n\n1. 强一致性：有原子一致性、线性一致性，有两个要求：\n   - 任何一次读都能读到某个数据最近一次写到数据\n   - 系统中所有进程，看到的操作顺序都和全局时钟下看到的顺序一致\n2. 顺序一致性：也有两个要求：\n   - 任何一次读都能读到最近一次写的数据\n   - 系统的所有进程顺序一致，但不一定和全局时钟下看到的数据一致\n3. 弱一致性：指数据更新后，有些操作可能访问不到，最终一致性是一种弱一致性，是指任意时刻节点上同一份数据不一定相同，但一段时间后，数据总会达到一致。里面有根据访问分为：\n   - 因果一致性：如果A通知B它更新了一个数据（A,B有因果关系），那么B后续访问只能看到更新过的值\n   - 读写一致性：当进程A自己更新一个值后，它自己只能访问到更新过的值（会话一致性同理）\n   - 单调一致性：当进程看到数据对象的某个值，它就不会再访问到那个值之前的值\n4. 补充一点：paxos是共识（consensus）机制，而不是一致性协议\n\n### ZK保证的一致性原则\n- FIFO一致性：对于同一个客户端，执行一个请求（get、delete这类操作）应该按照他们被发送的顺序；保证该客户端对于notification和状态改变的顺序一致\n- 顺序一致性：所以客户端看到的并行写入操作的结果的顺序是一样的，ZAB实现，有点类似paxos\n- 原子性：update操作要不成功要不失败，不会有中间结果\n- 单系统镜像：同一个client不论连接那个server，看到的数据都应该是完全一致的。对于不同的client则可能看到不同的（因为有延迟）的数据\n- 持续性（单调一致性）：当一个更新完成后，他会一直保持直到它被再次更新\n- 高可用性：2F+1个服务器可以最多容忍F台服务器崩溃\n- 最终一致性：数据在一段时间后最终辉达成一致\n\n\n### ZAB(zookeeper atomic broadcat)协议：\n\n#### 目标\n可靠投递：如果一个事务被提交到一个服务器，那它最终会被提交到所有服务器\n全局有序：如果一台服务器上事务A在事务B之前提交，那么在所有服务器上事务A一定都在B之前提交\n因果有序：如果事务A在事务B之前发生（A,B有因果关系），如果一起被提交，一定是A在B之前执行\n\n#### 协议内容——广播\n为了保证一致性，所有写操作都要经过leader完成，由leader进行广播。广播是个简化的二阶段提交过程：\n1. leader收到消息请求后，给消息赋予一个全局唯一的64位自增id，叫zxid（类似于rdbms的事务id），通过zxid比较可以实现因果有序\n2. leader通过fifo队列将带有zxid的proposal分发给所有follwer\n3. 当follwer收到proposal，先存下来，然后返回leader一个ack\n4. 当leader收到过半ack后，leader会向所有follwer发送commit命令，同时在本地执行该请求\n5. follwer收到commit命令后，执行该请求  \n\n值得一提的是，写请求是通过leader广播完成的，但读请求leader或者follwer都可以直接处理，只要从本地内存中读取数据返回client即可\n\n#### 协议内容——恢复\n已经proposal的命令应该被备份。例如一些服务器在收到commit之前leader就挂掉了，这个时候就无法执行该请求。当它重启后应该重新执行（client如果挂掉也需要和server重新同步一下）\n\n## 领导选举\n### 基于TCP的FastLeaderElection\n每个zookeeper的服务器都需要在数据文件夹下创建一个名为myid的文件（里面只有一个整数），用来唯一标识该服务器。而在配置文件必须与其一致，选票包括：\n- logicClock：表示服务器发起投票轮数，自增整数\n- state：当前服务器状态\n- self_id：当前服务器myid\n- self_zxid：当前服务器最大zxid\n- vote_id：推举的服务器myid\n- vote_zxid：被推举服务器上最大zxid\n\n具体投票流程为：\n1. 初始化选票：清空票箱（票箱中记录了每个服务器最后一次投票）\n2. 发送初始化选票：每个服务器都通过广播票投给自己\n3. 接受外部投票：尝试从其他服务器获得选票，并计入自己票箱。\n4. 判断选举轮数（logicClock）：如果外部logicClock大于自己，则说明自己选举落后于其他服务器，立即清空票箱并将自己logicClock更新为收到的logicClock。再对比自己之前的投票和收到投票logicClock，确认是否需要更新；如果外部logicClock小于自己，则忽略这次投票；相等则进行下一步选票PK\n5. 选票PK：是基于myid和zxid的比较，即先选zxid大的，如果zxid相同再选myid比较大的\n6. 统计投票：如果半数服务器认可了自己的投票，则终止，否则继续\n7. 更新服务器状态：每个服务器根据投票结果更新服务器状态为leading或following\n\n## 应用\n\n### 分布式一致性锁\n\n利用名称唯一性，枷锁操作时，只需要所有客户端创建/test/lock节点，只有一个创建成功，即可以获得锁。而解锁时，只要删除/test/lock节点，其他客户端通过watch机制可以继续进入竞争创建节点。如下图：\n\n![zookeeperArc](/img/zkdistributedlock.png)\n\n### 并发的惊群效应\n\n以上的实现非常简单，但会产生惊群效应，即当锁释放时，所以客户端都会被唤醒，但只有一个客户端可以获得锁。这个可以通过改良分布式锁实现的方式解决。\n\n让所有客户端都在/lock下创建临时顺序节点，如果创建客户端自身节点编号是/lock下最小的节点，则获得锁。其他节点只监视比自己小的的最大节点（如创建节点1、2、5，1获得锁，2只监视1，5只监视2），只有当自己监视的节点释放锁自己才可以获得锁。这种其实是一种按照创建顺序排队的实现。\n\n\n\n","tags":["big data"]},{"title":"Distributed Data Analytics Systems","url":"/2018/11/18/Distributed Data Analytics Systems/","content":"\n## Distributed Data Analytics Systems\n\n### Hadoop\n\nDrawbacks of traditional distributed Framework,Why this?\n\nDrawback:\n\n- Data exchange requires synchronization\n- Difficult to cope with partial system failure\n\nWhy Hadoop:\n\n- Reliability: handle partial failures\n- Scalability: Automatically scales to more computing nodes\n- Programmability: written in high-level code\n\nHow HDFS works?\n\nWhen a client application wants to read a file, it communicates with the name node to determine which blocks make up the file, and which datanodes those blocks reside in.Then it communicate directly with the datanodes to read the data.\n\nMapReduce Execution:\n\n1. Pre-loaded local input data\n2. Intermediate data from mappers\n3. Values exchanged by shuffle process\n4. Reducing process generates outputs\n5. Outputs stored in HDFS\n\nMapReduce bottleneck:\n\n1. Problem: huge data transfer takes lot of time in shuffle step.\n\n   Solution:Hadoop will start transfer data from mappers to reduces as the mappers finish work\n\n2. Problem: straggler problem exist indeed, while no reducer can start before every mapper has finished\n\n   Solution: Hadoop uses speculative execution, specifically, if a mapper appears to be running significantly more slowly than others, a new instance of the mapper will start on another machine, operating same data, the first result will be used and the running mapper will be killed\n\n3. Problem: Data must be passed to reducer, which result in a lot of network traffic\n\n   Solution: Combiner, like a \"mini-reduce\", runs locally on single mapper's output, and the codes are often identical with reducer.\n\n4. Problem: potential performance issues or secondary sort is needed.\n\n   Solution: Write you own Custom Partitioners\n\n### HaLoop\n\nDrawbacks of traditional distributed Framework,Why this?\n\nDrawbacks:\n\nHard to handle recursive program, for example: Graph analytics, machine learning, data mining or some recursive queries. mapreduce: Load and Shuffle data on each iteration\n\nWhy HaLoop:\n\n- TaskTracker (Cache management)\n\n- Scheduler (Cache awareness)\n\n- Programming model (multi-step loop bodies, cache control)\n\nIt is a efficient common runtime for recursive languages: Map, Reduce, Fixpoint.\n\nSolution:\n\nInter-iteration caching:\n\n- Mapper input cache (MI)\n- Reducer input cache (RI)\n- Reducer output cache (RO)\n\nRI - Reducer Input Cache:\n\nAccess to loop invariant data without map/shuffle, used by reducer function.\n\nRO - Reducer Output Cache:\n\nDistributed access to output of previous iteration, used by fixpoint evaluation\n\nMI - Mapper Input Cache:\n\nAccess to non-local mapper input on later iterations, used during scheduling of map tasks.\n\nArchitecture:\n\n- Loop Control\n- Caching\n- Indexing\n\n### FlumeJava\n\nDrawbacks of traditional distributed Framework,Why this?\n\nWhen meet long and complicated data-parallel pipelines, it is difficult to program and manage, besides each mapreduce job needs to keep intermediate results, what's more, high overhead at synchronization barrier between different mapreduce jobs.\n\nWhy flume?\n\nExpressiveness\n\nAbstractions\n\nPerformance (lazy evaluation and Dynamic optimization)\n\nUsability & deployability (implemented as a java library)\n\nOptimization:\n\n1. Sink flatten\n2. ParallelDo fusion\n3. MSCR fusion\n\n### Dryad\n\nDrawbacks of traditional distributed Framework,Why this?\n\nGeneral-purpose execution engine for coarse-grained data-parallel applications\n\nEasy to write simple programs, execution engine automatically manages scheduling, distribution, FT, etc.\n\nWhy Dryad?\n\nJob = Directed Acyclic Graph\n\nComputational \"vertices\" connected by communication \"channels\"(edges)\n\nWhat GDL (Graph Description Language)?\n\nA lower-level programming model than SQL\n\nArchitecture?\n\n- Job Manager\n- Name Server\n- Daemons\n\n### Spark\n\nDrawbacks of traditional distributed Framework,Why this?\n\ncomplex applications\n\ninteractive ad-hoc queries\n\nReuse of intermediate results across multiple computatios\n\nRDD (Resilient Distributed Datasets)?\n\n1. Restricted form of distributed shared memory, only be built through coarse-grained deterministic transformations\n2. Fault recovery using lineage (Log transformations used to build a dataset, log enough info how it was derived from other RDDs)\n\nRDD good for:\n\nApply the same operation to all elements of a dataset (coarse-grained operation)\n\nRemember each transformation as one step in a lineage graph\n\nRecovery of lost partitions without having to log large amounts of data\n\nNot good for: asynchronous fine-grained updates to shared state\n\nTask Scheduler:\n\nDryad-like DAGs\n\n### Naiad\n\nDrawbacks of traditional distributed Framework,Why this?\n\nIterative processing on streaming data, interactive queries on a fresh, consistent view of the results.\n\nWhay Naiad?\n\nA new computational model: timely dataflow\n\nSolusion:\n\niteractive and incremental computations : Structured loops allowing feedback in the dataflow, stateful dataflow vertices capable of consuming and producing records without global coordination\n\nproducing consistent results -> notifications for vertices once they have received all records for a given round of input or loop iteration.\n\nKey point:\n\nTimestamp: \n\nin the graph, every stateful vertices receive timestamped message along directed edges.\n\nIn nested cycle, use timestamp to distiguish data in different input and loop iterations\n\nTwo methods:\n\nSupports asynchronous and fine-grained synchronous execution\n\n1. Batching: sychronous, one-to-one correspondence between input and output\n2. Streaming: asychronous, overlapping computation (latency is low)\n\nLow latency?\n\n1. programming model: Asynchronous and fine-grained synchronous execution.\n2. Distributed progress tracking protocol: enables processes to deliver notifications promptly.\n\n### Husky\n\nDrawbacks of traditional distributed Framework,Why this?\n\nHigh performance, flat learning curve, good reusability, low maintenance cost and high compatibility\n\nWhy husky?\n\nA new computational model that makes Husky general and expressive\n\nArchitecure?\n\nMaster-Worker architecture \n\nmaster: \n\nkeeps worker information and data partitioning scheme\n\nDoes not sit on the data path and don't compute\n\ncoordinates work among workers and monitors the progress of workers\n\nWorker:\n\nRead/write data, communicate with other workers. compute in parallel\n\nSend heartbeat to master periodically\n\nImplementation:\n\nChannel-based messaging subsystem -> makes streaming computation posible\n\nStore attribute lists as in a column-store\n\nBetter locality, more oppotunity to optimize (vectorization). Adding attributes without recompiling, useful for interactive data analysis."},{"title":"笔试和面试准备","url":"/2018/10/24/面试和笔试的准备/","content":"\n由于不想错过秋招，并且为明年春招做准备，之后会不定期的在这里整理一些问题。\n\n## 语言——java、python、cpp、scala\n\n### 并发编程、java内存模型（Java Memory Model）：\n\n#### 线程之间的通信机制有两种：\n\n- **共享内存 **：不同线程共享内存中的公共状态来隐式的通行，比较典型的就是通过**共享对象**来进行通信。对于线程的同步必须是显式的，即程序员手动指定某个方法或者某段代码必须在线程之间互斥的进行。\n\n- **消息传递**：线程之间没有公共状态，线程之间必须通过明确的发送信息进行显式通信，比较典型的就是调用**wait()**或者**notify()**等函数的方式。同步是隐式的，因为消息的发送必须在消息接受前。\n\n#### 内存模型：\n\n![jmm](../img/jmm.png)\n\n每个线程都有自己的本地内存，然后也有主内存。本地内存中有一份共享变量的副本。线程A刷新了自己本地内存中的变量x，但AB需要通信时，会先通过副本刷新主内存中的x，然后线程B再读取主内存中的x，此时B的本地内存中x也成了新值。\n\n#### 内存模型的实现\n\n![jmmImplement](../img/jmmImplement.png)\n\n所有的原始类型变量都在stack中，一个线程中的本地变量对另一个线程是不可见的。而java创建的对象，以及原始类型的封装类则存在heap中（不管是成员变量还是方法中的本地变量）。heap中的变量是可以被共享的，只要另一个线程获得了这个对象，那它就可以访问这个对象的成员变量。\n\n#### Volatile和sychronized区别\n\n首先要理解线程安全的两个方面：\n\n- **执行控制：**目的是控制代码执行顺序及是否可以并发进行\n- **内存可见：**线程执行的结果在内存中的可见性\n\n特征：\n\nsynchronized：是解决执行控制的问题，即阻止其他线程获得当前对象的监控锁，使得被synchronized保护的代码块无法被其他线程所访问，也就无法并发执行。\n\nvolatile：解决的是内存可见性的问题。使得所有对volatile变量对读写都直接刷新到主存，即保证了变量的可见性，但只能保证对原始类型变量（除了double、long）的操作原子性。\n\n区别：\n\n1. volatile本质是告诉jvm这个变量值是不确定的，需要从主内存中取，而synchronized则是直接锁定当前变量。\n2. volatile只能针对变量，而synchronized可以用在变量、方法或类的级别。\n3. volatile只能实现对变量修改的可见性，而无法保证原子性。synchronized则可以保证原子性\n4. volatile不会造成线程阻塞，而synchronized会造成\n5. volatile标记变量不会被编译器优化，而synchronized则会被优化。\n\n#### happen-before原则：\n\n一个线程内，按照代码执行顺序，书写在前面的操作先行发生于书写在后面的操作。\n\n#### 内存栅栏：\n\n为了提高性能，cpu通常不按照顺序执行指令，而是通过重排指令来提高存储器的利用率，但这也意味着会有不可预测的问题发生。内存栅栏就是解决指令重排问题的方法，具体来说，是指处理器在重排时，不能采用先执行栅栏后的内存访问，在执行栅栏前内存访问的方式。这也是volatile的实现方式。\n\n### GC回收机制：\n\n### 类加载机制：\n\n### 其他：\n\n\n\n## 大数据相关\n\n### mysql数据库\n\n#### mysql主键和唯一索引\n\n之前不太了解唯一索引，这次才知道。唯一索引就是mysql里的unique，可以在创建表时或者建表后增加，如建表后可以用：\n\n```sql\nCREATE UNIQUE INDEX nm on PERSON(name);\n```\n\n执行成功后，如果在原表已经有一条类似 ('1','max','1995-12-12')类似的记录后，还要插入如下：\n\n```sq\nINSERT INTO person(name,birthday) values ('max','1995-12-13');\n```\n\n会直接报错，告诉用户name是唯一索引，无法再插入相同。这个有个使用场景，即在高并发下为了保证某一个键不会插入重复信息，需要给这个键增加唯一索引。\n\n主键和唯一索引的区别：\n\n- 唯一索引允许空值，但主键不能为空\n- 主键创建后一定包含一个唯一索引，但唯一索引不一定是主键\n- 主键可以被其他表引为外间，但唯一索引不能\n- 一个表只能有一个主键，但可以有多个外键\n\nmysql里key其实包含三种：PRI（主键）,UNQ（唯一索引）,MUL（普通key，可以重复）\n\n#### 外键：\n\n一个表的主键可以作为它从表的一个外键，外键本质是一种约束。外键要求和主键Type完全相同（int(10)和int(11)的差别都不可以），用如下语句可以创建外键约束：\n\n```sql\nalter table my_profile add CONSTRAINT id foreign key(id) references person(id) on delete cascade on update no action;\n```\n\n注意后面可以对这个键的delete、update进行约定，确定是否进行级联操作。\n\n注意：外键只有InnoDB支持，而MyISAM不支持。\n\n#### InnoDB和MyISAM的几点区别：\n\n- InnoDB支持事物和外键，而MyISAM不支持\n- InnoDB支持行及锁，MyISAM只支持表级锁\n- InnoDB不支持全文索引，而MyISAM支持\n- MyISAM相对简单，提供了告诉存储和检索，在效率上要优于InnoDB，适合管理非事物表，适合有大量的select的程序。\n- InnoDB则更安全，多用户的并发性能更好，适合有大量insert、update的事物处理的应用程序。\n\n#### MySQL的锁机制：\n\nmysql有几种类型的锁：\n\n- 排他锁（写锁，X锁）：如果事务T对A加上排他锁，那其他事务不能对A加任何类型的锁。获得排他锁既能读数据也能写数据。\n- 共享锁（读锁，S锁）：如果事务T对A加上共享锁，则其他事务不能对A加排他锁，获得共享锁的事务只能读数据，不能写数据。\n- 行级锁：行级锁氛围共享锁和排他锁，行级锁是mysql中锁定粒度最细的锁。行级锁开销大，加锁慢，锁定力度小，发生锁冲突的概率最低，并发度最高。\n- 表级锁：表级锁分为表共享锁和表独占锁。表级锁开销小，加锁快，锁定粒度大，发生冲突概率高，并发度低。\n- 悲观锁：即悲观并发控制，简称PCC。悲观锁是指在数据处理过程中加锁，使数据处于锁定状态，使用数据库锁机制实现。注意在mysql中使用悲观锁要关闭mysql的自动提交。——安全，但产生额外开销，增加产生死锁机会。\n- 乐观锁：通过记录数据版本的方式实现。为数据增加一个版本标识，读取数据时，将版本标识一起取出，数据每更新一次，就对版本标识进行更新。\n\n语句示例：\n\n```sql\n#共享锁\nSELECT * FROM person WHERE ... LOCK IN SHARE MODE;\n#排他锁\nSELECT * FROM person WHERE ... FOR UPDATE;\n```\n\n实现：\n\nInnoDB实现行级锁需要在表上增加索引，InnoDB会对索引项加锁，即当InnoDB没有索引项时，也只能进行表锁。\n\n#### InnoDB中MVCC的一些理解\n\nmvcc（multiversion concurrency control），即多版本并发控制技术，它使得行锁不再简单通过锁来进行并发控制。它本身希望同一个数据有多个版本，版本号是单向增长的。而读事务只读在该事务开始前的数据库的快照。实质是用来解决读写冲突的无锁并发控制。乐观锁就是这种思路。\n\n## 算法相关\n\n## 其他","tags":["career"]},{"title":"elasticsearch使用整理","url":"/2018/10/17/elasticsearch基本概念/","content":"\n最近项目要使用elasticsearch。。。这里总结一下看elasticsearch文档的一些总结吧。\n\n## 基本概念\n\nelasticsearch是一个全文本搜索的服务，比如github从几百亿行代码中搜索某个代码就用了elasticsearch。在mac下可以通过brew安装和管理，注意还要再安装一个kibana来进行可视化和管理（可以在kibana的console跑各种请求）。es默认跑在9200，kibana默认跑在5601端口。\n\n这里注意对于中文场景要安装[elasticsearch-analysis-ik](https://github.com/medcl/elasticsearch-analysis-ik)进行分词，ik还支持自定义词典，可以在es-path/config/analysis-ik/custom下自定义dic。\n\n概念对应关系如下表\n\n| 传统数据库 | ElasticSearch |\n| ---------- | ------------- |\n| Database   | Index         |\n| Table      | Type          |\n| Row        | Document      |\n| Column     | Field         |\n| schema     | mapping       |\n\nelasticsearch的所有内容都是通过json格式传递的。每个文档都是一个json的形式。一般文档包括下面三个metadata：\n\n- _index：文档存储的地方（类似于database）\n- _type：文档代表的对象的类（类似于table）\n- _id：文档的唯一标示（类似于主见，可以自定义，也可以让es帮你生成）\n\nes是支持分布式的，当然也有对应的shard和replica机制。\n\n## 单机操作api\n\n### 创建：\n\n```shell\nPUT /{index}/{type}/{id} \n{\n    \"field\":\"value\",\n    ...\n}\n```\n\n### 获取：\n\n```shell\nGET /{index}/{type}/{id}?_source=title,text#可以获取特定字段（包含metadata）\nGET /{index}/{type}/{id}/_source#只获取_source字段，而无metadata\n```\n\n### 存在：\n\n```shell\nHEAD /{index}/{type}/{id}#判断document是不是存在，只返回状态码而没有具体数据\n```\n\n### 更新：\n\n由于elasticsearch中的文档是不可变的，所以修改只能通过重建或者脚本的方式更新\n\n```shell\nPUT /{index}/{type}/{id}\n{\n    ...\n}\n#也可以用post，注意put和post的区别主要是post可以默认自动生成id\nPOST /{index}/{type}\n{\n    ...\n}\n```\n\n脚本的方式：\n\n```shell\nPOST {index}/{type}/{id}/_update#注意最后是一个_update\n{\n    \"script\" : {\n        \"source\":\"stx._source.tags.add(params.tag)\",\n        \"lang\":\"painless\",\n        \"params\" : {\n            \"tag\":\"blue\"\n        }\n    }\n}\n#想增加一个字段tags的内容\n```\n\n### 删除：\n\n```shell\nDELETE /{index}/{type}/{id}#删除某个文档\n```\n\n### 搜索：\n\nelasticsearch最重要的功能就是搜索，搜索的姿势也是很多的\n\n```shell\nGET /_search#在所有索引类型中搜索\nGET /{index1},{index2}/_search#在index1和index2两种索引类型中搜索\nGET /{index}/{type}/_search#在某个type中搜索\nGET /_all/{type},{type}/_search#在所有索引的某些type上搜索\n```\n\n对于搜索的结果，可以用size分页，用from决定从哪里出发\n\n当然平时比较多的是查找字符串\n\n```shell\nGET /_all/{type}/_search?q=tweet:elasticsearch#查找某个type中tweet：elasticsearch的文档\nGET /_all/_doc/_search?q=%2bdescribtion%3amax#查找某个类中describtion字段包含max的文档\n```\n\n第二个要重点解释一下，其实在url encode之前为：\n\n+describtion:max\n\n如果要放在查询中，就要进行urlencode（percent encode），然后放在q后，即：\n\nq=%2bdescribtion%3amax\n\n这里的查询不仅限于字符串查找，还可以是特定条件下的查找，如\n\n- `name`字段包含`\"mary\"`或`\"john\"`\n- `date`晚于`2014-09-10`\n- `_all`字段包含`\"aggregations\"`或`\"geo\"`\n\n这个可以用\n\n```shell\n+name:(mary john) +date:>2014-09-10 +(aggregations geo)\n```\n\n来表示，最后urlencode即可进行查询。\n\n## mapping和Analysis\n\nes的mapping对应了传统数据库的schema，其中es会对不同的字段类型进行猜测，得到对应的mapping，在这种情况下，对某个field的类型猜测可能会是date，但如果用_all搜索这个field则认为他是string，从而带来搜索结果的不同。\n\n对于一个搜索引擎，与数据库最本质的区别在于确切值和全文文本之间。\n\n传统数据库需要在where语句里的特定匹配，但es则希望如下：\n\n- 一个针对`\"UK\"`的查询将返回涉及`\"United Kingdom\"`的文档\n- 一个针对`\"jump\"`的查询同时能够匹配`\"jumped\"`， `\"jumps\"`， `\"jumping\"`甚至`\"leap\"\n\n等等等场景，为此引出了es最关键的技术——倒排索引。\n\n### Analysis\n\n对于大小写或者同源词等原本是不能匹配的，需要通过特定的分析过程才能被检索，具体过程如下：\n\n- 首先，标记化一个文本快为适用于倒排索引的term\n- 标准化这些term为标准行事，提高可搜索率和查全率\n\n一个完成的分析器（analyzer）包括三部分：\n\n1. 字符过滤器：去掉一些html标签，或者转化&为and等\n2. 分词器：通过空格或逗号分词（中文需要特定分词工具ik）\n3. 标记过滤：将词进行大小写转换、去掉一些停用词（a、the这些）或者增加词（同义词）\n\n可以通过如下方式测试analyzer\n\n```shell\nGET _analyze?pretty\n{\n  \"analyzer\": \"ik_smart\",\n  \"text\":\"王者荣耀真好玩\"\n}\n\n#output:\n{\n  \"tokens\": [\n    {\n      \"token\": \"王者荣耀\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"CN_WORD\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"真好玩\",\n      \"start_offset\": 4,\n      \"end_offset\": 7,\n      \"type\": \"CN_WORD\",\n      \"position\": 1\n    }\n  ]\n}\n```\n\n如果想使用某个特定analyzer，而不是系统自带的standard，就需要通过mapping。\n\n查看map需要用：\n\n```shell\nGET /{index}/_mapping/{type}\n```\n\n这样得到mappings的内容，注意现在一些field的类型是keyword（而不是text）是因为keyword不会自动分词并建立索引，但text会这么做。\n\n```shell\nPUT final\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\"\n        },\n        \"blob\": {\n          \"type\": \"text\",\n          \"analyzer\":\"ik_smart\"\n        }\n\n      }\n    }\n  }\n}\n```\n\n","tags":["big data"]},{"title":"最近写openrice爬虫遇到的一些问题（未完待续）","url":"/2018/10/12/基于pyhusky的爬虫）/","content":"\n最近项目中需要写爬虫，需要爬openrice，其中有一些tips需要记录一下。\n\n## tor and polipo\n\n### Tor\n\ntor是一个socks5代理工具，可以实现匿名访问网页，本质其实是利用了分布在世界各地的一些肉机做请求转发。\n\ntor有两部分，tor和ControlPort，他们都需要在tor/config下手动配置：\n\n- tor，一般绑定在9050端口，socks5代理服务器，如果在mac下，可以通过brew下载并通过brew services进行管理\n- ControlPort，一般绑定在9051端口，用来管理本地的tor，在python下用TorCtl包来操作。开启前要通过tor --keygen设置passphrase\n\n在linux下可以用lsof -i:port-num来查看有没有正确开启这两个服务。\n\n可以通过Control向tor发送不同的signal的方式来实现对tor的管理，例如获取新ip可以\n\n```python\ndef getNewIp():\n    conn = TorCtl.connect(controlAddr=\"127.0.0.1\", controlPort=9051, passphrase=\"my password\")\n    conn.send_signal(\"NEWNYM\")\n```\n\n### Polipo\n\n由于tor是通过sock5代理的，但实际使用的很多python库只支持http/https代理，例如requests、urllib3等，这样需要一个web服务器来进行转化。polipo就是这样的工具。\n\nmac端可以通过brew来下载和管理polipo，但polipo的config不会自动生成，需要我手动去/usr/local/etc下mkdir polipo文件并自己写config，我的config如下\n\n```shell\nproxyAddress = \"::0\"\nallowedClients = 127.0.0.1, 192.168.1.0/24\nproxyAddress = \"0.0.0.0\"\nsocksParentProxy = \"localhost:9050\"\nsocksProxyType = socks5\n```\n\n这里一个坑是我刚开始在polipo的config里没有写proxyaddress = “0.0.0.0”，会有很多问题\n\npolipo默认绑定端口是8123，如果成功开启，在python可以通过\n\n```python\n    proxy_manager = urllib3.ProxyManager(\n        'http://127.0.0.1:8123',\n        timeout=urllib3.Timeout(connect=10, read=10),\n        retries=urllib3.Retry(5),\n        maxsize=5\n    )\n```\n\nproxy_manager可以正常访问web页面，但是速度较慢。\n\n## Pyhusky\n\n可以通过husky的map来运行下载任务，提高分布式效率。\n\n首先需要在本地启动husky的master和daemon任务（目前遗留了一个问题：每个任务只能跑一次，第二次跑不了？），然后在python通过\n\n```python\n    env.pyhusky_start('127.0.0.1', 20000)\n    pages = env.parallelize(url_list)\n    tuple = pages.map(lambda url: mapper_process(url)).collect()\n```\n\n来运行的，注意这里有一个坑是parallelize分成几份会由master config的 [worker] 下的配置决定的，所以如果发现本机只跑了所有任务的一半或三分之一，就去check一下husky的master config有无问题。","tags":["crawler"]},{"title":"pyhusky的单机编译与运行遇到的坑","url":"/2018/10/10/pyhusky的编译与运行/","content":"\n由于项目要用到husky的python版，pyhusky。所以要在本机运行husky。不得不说，c++的项目的可移植性和兼容性真的差java好几条街，所以编译这个看起来简单，但实操坑非常多。\n\n### 坑1 —— zeromq\n\n我很早以前就在mac上通过brew装好了zeromq，这次在编译husky的时候还报了找不到。最后发现homebrew安装的zmq只是一个底层的c版本，而不是合适版本。这里我只好手动下载了libzmq和cppzmq。clone下来后大概流程就是：\n\n1. mkdir build & cd build\n2. make ..\n3. make -j4\n4. make install\n\n可以说是build一个项目最典型的操作了。刚开始不理解学长讲的要把hpp放在系统的include里，后来了解了一下原来cpp会把自己的一些hpp文件（类似快捷方式）放在/usr/local/include文件目录下，其他proj就可以去调用了。而make install里面集成了这一步。\n\n### 坑2 —— libhdfs3\n\n这个库是cpp用来连接hdfs的，但放弃维护了，所以只能找别人的，这里我找到了一个可用的\n\n```shell\ngit clone git@github.com:ContinuumIO/libhdfs3-downstream.git\nmkdir build & cd build\n../bootstrap --prefix=/usr/local --dependency=$(brew --prefix openssl)\nmake\nmake install\n```\n\n这个需要提前装好gtest和gmock，就要去下载gtest和gmock，还是一样的build\n\n### 坑3 —— build完Master依然不存在\n\n在make master之后，应该出现Master应用程序，但我的程序里没出现。后来才知道因为mac的默认文件格式是不支持大小写敏感的，所以会有问题。这样我需要去改master下build的CMakeList.txt，如下：\n\n```shell\n# MasterHusky\nadd_executable(MasterHusky master.cpp ${master_plugins})\ntarget_link_libraries(MasterHusky ${husky})\ntarget_link_libraries(MasterHusky ${EXTERNAL_LIB})\nset_property(TARGET MasterHusky PROPERTY CXX_STANDARD 14)\n```\n\n原来这里的MasterHusky叫Master，与build出的master文件冲突，所以要将它改成MasterHusky的名字，下面相应的地方都要改，然后再重新cmake就可以了。\n\n### 坑4 —— Daemon无法执行\n\n刚开始不知道要跑起来master以后再跑daemon，daemon也是build出来的，但刚开始不能跑，我需要装好libhdfs3以后，先运行master，再运行daemon（master不能停顿太久，因为可能挂掉了）\n\n### 坑5 —— bindings路径问题\n\n这个是最大的一个坑，我需要把husky目录下的bindings放在自己执行./Daemon和./Master的地方才可以！！！\n\n例如我是直接在release里执行的，就需要把bindings文件夹和crawler下的hcrawl都放在release目录下！","tags":["database"]},{"title":"降维的常用方法总结（未完待续）","url":"/2018/10/10/数据降维/","content":"\n课上讲了常见的数据降维的方法PCA，感觉比较有用，再结合一下网上看到的比较新的方法，总结在这里吧。\n\n### 常见的词汇\n\n最近都只看英文，所以相关的很多词汇中文不太熟悉，以后面试内地的话不太好聊，先把中文翻译放在这里\n\nrank of matrix：矩阵的秩\n\ndeterminant：行列式\n\nconjugate transpose：转置\n\northonormal matrix：正交矩阵\n\nsimilarity matrix：相似矩阵\n\ndiagonal matrix：对角矩阵\n\ndiagonal element：对角元素\n\nSingular Value Decomposition (SVD)：奇异值分解 ：）\n\nEigen-Decomposition：特征分解\n\neigenvector：特征向量\n\neigenpair：特征值\n\nPriciple Component Analysis (PCA) ：主成分分析\n\n### SVD (Singular Value Decomposition)\n\n奇异值分解是一种提取特征值的很好的方法。公式如下：\n\n![屏幕快照 2018-10-10 下午6.01.35](/Users/max/Desktop/屏幕快照 2018-10-10 下午6.01.35.png)\n\nA可以通过这样的方式分解，而当：rank（A）= r时，有如下：\n\n![屏幕快照 2018-10-10 下午8.30.45](/Users/max/Desktop/屏幕快照 2018-10-10 下午8.30.45.png)\n\nU：左奇异向量\n\nΣ：奇异值\n\nV：右奇异向量\n\n这个总是成立的如果U,V是正交矩阵并且Σ为对角矩阵并且所有元素降序非负\n\n\n\n","tags":["big data"]},{"title":"流数据挖掘算法","url":"/2018/09/29/数据流挖掘/","content":"\n流数据，顾名思义，就是针对流数据（例如google查询、twitter的状态更新这类数据）的一些挖掘。不同于普通数据集，流数据源源不断地产生，而我们的存储空间是有限的。所以要采取一些办法，：\n\n- Samping data from stream\n- Queries over sliding windows\n- Filtering a data streaming\n\n### Sampling data from stream\n\n由于我们不能存储整个stream，所以要存一些sample，具体有两种方式\n\n1. Sample一个stream的固定比例\n2. 维护一个固定大小的随机Sample —— 蓄水池采样\n\n#### 固定比例\n\n以用户的查询为例：假设有s个单次查询，有d个两次查询，即总共有s+2d次查询，sample率为p（0<p<=1），求用户两次查询的比例\n\n正确答案：d/(s+d)\n\n按照条件推倒：dp^2/(sp+dp^2+2p(1-p)d)\n\n这个准确率问题还是比较大的，为了改良，可以采用以取用户sample的方式。\n\n#### 固定大小的sample —— 蓄水池采样\n\n很好玩的问题：假如有一本非常厚的电话簿，你要随机挑1000个人打电话，并且保证每个人被选中的几率相等，你要怎么做？\n\n这里的电话簿就像数据流，你不能先数一遍总共有多少人，再产生一千个随机数这样，效率太低。\n\n**解决方法：**\n\n先选前1000人，然后后面的每一位以k/n的概率决定替换掉前面的任意一位。其中n是当前总共人数，k是蓄水池大小。例如第1001位就有1000/1001的概率替换掉前面1000个人中任意一个。\n\n**证明：**\n\n条件: sample size: k，total size: n，probability of each element: s/n\n\n求证: 对于n->n+1的情况，probability of each element: k/n+1，即无论n多大，每个元素被选中概率均等\n\n证明: \n\n每个元素被选中概率：k/n\n\nn->n+1后，每个被替换前面概率：k/n+1\n\n而假设A而原来在sample里的，仍然还在的概率为: (1-k/n+1) + (k/n+1)*(k-1/k) = n/n+1，这个公式前者是n+1这个元素没被选中，后者是没替换自己的概率\n\n那么A的概率就是：k/n * n/n+1 = k/n+1\n\n综上，概率相等～\n\n### Sliding Windows\n\n就是滑动窗口，简单来说：最多储存N bits,当又来一个新bit时，吐出第N+1个bits（也就是窗口里最老的那个）。\n\n问题：如果我们要统计N中有一个1，那必须全部存储N中的内容，但窗口的存储空间有限，因此这里用了DGIM方法->exponential Window。\n\n#### DGIM算法\n\n它是通过Bucket来对滑动窗口进行划分，每个桶包括：\n\n1. 桶最右边的timestamp（O(logN) bits）\n2. 在每个桶中1刀数量（O(log logN)bits）\n\n桶的限制：每个桶里的1的数量是2的次方。\n\n桶的特征：\n\n- 只有1个或者2个桶有相同数目的2的次方个1\n- 桶和桶的timestamps不会重叠\n- 桶是按照大小有序的\n- 当桶的end-time > N的时候（即整个桶已经从窗口中走了），桶会消失\n\n![DGIMbucket](/img/DGIMbucket.png)\n\n  DGIM算法中数据结构的更新（2048式更新）：\n\n1. 每一个新的位进入滑动窗口后，最左边一个位从窗口中移出（同时从桶中移出）；如果最左边的桶的时间戳是当前时间戳减去N（也就是说桶里已经没有处于窗口中的位），则放弃这个桶；\n2. 对于新加入的位，如果其为0，则无操作；否则建立一个包含新加入位的大小为1的桶；\n3. 由于新增一个大小为1的桶而出现3个桶大小为1，则合并最左边的两个桶为一个大小为2的桶；合并之后可能出现3个大小为2的桶，则合并最左边两个大小为2的桶得到一个大小为4的桶……依次类推直到到达最左边的桶。\n\n如何估计最新N窗口中1的数目呢？\n\n- 将除了最后一个的所有的bucket大小（这里大小都是指1的数目）加起来\n- 再加上最后一个bucket的一半大小\n\n准确率：>= 50%\n\n### Filtering Data Streams\n\n问题：垃圾邮件的过滤问题，我们知道有10亿个好的email address，如果邮件来自这些address，那么它就不是spam。即现在有一封邮件{\"address\":\"contents\"}，如果key可以匹配10亿email address就不是spam。\n\n#### Bloom Filter\n\n设S是刚刚好的email address集合，而B是大小为n，全部初始化为0的一个bit array\n\n![initBFbits](/img/initBFbits.png)\n\n使用k个相互独立的hash函数h1,h2……hk，他们可以将元素映射到[0,n]的bit array上。将S中元素通过k个hash函数映射，映射到的地方设置为1\n\n![findBFbits](/img/findBFbits.png)\n\n这里的x1，x2就是S的元素。\n\n当判断一个元素y1，y2是否在集合S中时，就通过相同的hash函数将其映射到B上，如果任何一个hash函数映射到的地方存在0，那y就不在S中，反之则证明在S上，或者是一个false positive（将错的分到对的里）。\n\n![hashedBFbits](/img/hashedBFbits.png)\n\n如图y1就不在S中，y2或是在S中，或是一个false positive。但这种方法不存在true negative（将对的分到错的里）。","tags":["data mining"]},{"title":"MongoDB使用整理","url":"/2018/09/25/MongoDB使用整理/","content":"\n最近project需要mongo做存储，因此在这里总结一些mongo的东西\n\n## 基本概念\n\nMongo是一个开源的文档型数据库，它提供了高性能、高可用和自动扩展的特性。\n\n概念对应关系如下表：\n\n| SQL术语/概念          | MongoDB术语/概念     | 解释/说明                           |\n| --------------------- | -------------------- | ----------------------------------- |\n| database              | database             | 数据库                              |\n| table                 | collection           | 数据库表/集合                       |\n| row                   | document             | 数据记录行/文档                     |\n| column                | field                | 数据字段/域                         |\n| index                 | index                | 索引                                |\n| table joins           |                      | 表连接,MongoDB不支持                |\n| primary key           | primary key          | 主键,MongoDB自动将_id字段设置为主键 |\n| aggregation(group by) | aggregation pipeline | 聚合函数                            |\n| transactions          | transactions         | 事物                                |\n\nmongo支持包括mongo shell、c++、java、python、php等几乎所有主流语言。只需要下好对应的driver即可，下面的操作我们使用pymongo来完成。\n\n## CURD操作\n\nCURD是数据库基本操作。这里用pymongo来创建mongo client。\n\n```python\nfrom pymongo import MongoClient\n\n#首先要创建一个mongo client来运行mongo instance\nclient = MongoClient('localhost', 27017)\n\n#选中需要操作的database（test）和collection（inventory）\ninventory = client['test']['inventory']\n```\n\n### Read操作\n\n```python\nresult = collection.find_one({filter})\n\nresults = collection.find({filter}).limit(5)\n\n#example\nvalues = inventory.find({\"type\": {\"$regex\": \"Event\"}})\n```\n\n###  Update操作\n\n```python\ncollection.update_one({filter},<{$set}>)\n\n#example\ninventory.update_one({\"id\": \"2614896652\"}, {\"$set\": {\"type\": \"CreateEvent\"}})\n```\n\n### Create操作\n\n```python\n#在这里我读取一个json文件，然后存入数据库中\ndata = []\nwith open(\"/data/test.json\", 'r', encoding=\"utf-8\") as f:\n    for content in f.readlines():\n        data.append(json.loads(content))\n\ninventory.insert_many(data)\n```\n\n### Delete操作\n\n```python\ncollection.delete_many({filter})\n\n#example\ninventory.delete_many({}) #删除全部记录\n```\n\n### Bulk Write操作\n\n```python\nfrom pymongo import InsertOne,DeleteMany,UpdateOne\n\n#第一个参数数组为operations数组，ordered表示是否顺序执行\ninventory.bulk_write([\n    InsertOne(data[0]),\n    UpdateOne({\"id\": \"2614896652\"}, {\"$set\": {\"type\": \"PushEvent\"}}),\n    DeleteMany({})\n], ordered=True)\n```\n\n### Text Search操作\n\n```python\n#mongo提供了针对文本内容search的方法，要想使用这个就要先对对应field创建text index\n\n#创建text index\ndb.inventory.createIndex({\"type\":\"text\"})\n\n#文本搜索\nvalues = inventory.find({\"$text\": {\"$search\": \"CreateEvent\"}})\n```\n\n\n\n## Replication\n\n![replica](/img/replica.png)\n\n一组replica只指存相同数据的节点集。其中只有一个被认为是primary节点，其他是secondary节点（类似于master-slave）。\n\n只有primary节点可以接受所有的写操作，并且通过{w:\"majority\"}来确认通知。同时primary会把所有数据操作放在oplog里。\n\n对于secondary节点，他们会备份primary节点的oplog，然后把log里的东西apply到自己的节点上，这样primary上的dataset也就映射到了secondary上。当primary不可用时，secondaries会通过选举决定谁是primary。\n\n![secondaySelection](/img/secondaySelection.png)\n\n你可以额外增加一个叫arbiter的节点，arbiter不维护任何数据，它通过应答其他节点发出的心跳和选举请求，维护了投票时需要的法定人数。因为它本身不维护dataset，所以它占用的资源很少，也不要单独硬件。所以当节点数为偶数时，可以增加一个arbiter来维护。\n\n![arbiter](/img/arbiter.png)\n\n### Asynchronous Replication\n\nsecondary本身异步地从primary复制。完成备份后，即使。\n\n#### Automatic Failover\n\n当primary在一定时间（configured by electionTimeoutMillis，10s default）不与secondary通信时，有资格的secondary会提名自己参加选举，cluster会试图完成选举并恢复正常功能。\n\n在选举阶段，这个replica set都无法进行写操作，个别secondary如果配置了允许读操作的话还是正常进行的。通常系统会在12s只能判定一个primary不可用并完成选举工作。\n\n## Sharding\n\nSharding是Mongo采用的一种在多台机器上分布数据的方法。对于在大量、高吞吐率的数据在单台服务器上无法承载，有两种方法来扩展机器：\n\n- Vertical Scaling：增加单机的容量、内存，使用更好的cpu等方式。但这种受限于目前可用的技术能力。\n- Horizontal Scaling：增加机器的数量，每个机器只要handle一部分数据。比较便宜，但维护和部署基础设施成本较高。\n\nSharding cluster包含一下组件：\n\n- Shard：每个shard包含总的一个子数据集。每个shard都可以被部署为备用数据集。\n- mongos：mongos作为query router，提供了client和sharded cluster之间的借口。\n- config servers：储存了metadata和cluster的配置设置。在mongo3.4，其必须被部署为一个备用数据集（CSRS）。\n\n关系组图如下：\n\n![sharded cluster](/img/sharded cluster.png)\n\nMongoDB从collection层面来分割数据。\n\n### Shard Keys\n\nMongoDB是根据shard key来分割一个collection的。shard key是由目标collection中每个document都有的fields或者不变的field组成的。\n\n在分割collection时要选择shard key，分割完后shard key就不能再改变了。\n\n对于非空collection，必须有一个index是以shard key开头的；而对于空collection，MongoDB则会创建一个index。\n\nshard keys的选择会影响sharded cluster的性能、效率、扩展性等问题，它和它背后的index也会影响你的集群可以使用的sharding策略。\n\n### Chunks\n\nMongoDB会数据分成一个个的chunks，其中包含了左闭右开的shard key范围。Mongo使用shared cluster balancer来实现在sharded cluster上的不同shards间移动chunks。\n\n### Sharded and Non-sharded Collections\n\n每个数据库都可以既有sharded collections又有un-sharded collections。unsharded collection存在数据库的primary shard上。每个数据库都有primary shard。\n\n### Connecting to a sharded Cluster\n\n![shaded connection](/img/shaded connection.png)\n\n用户通过mongos router来实现和sharded cluster（包含sharded和unsharded collections）的交互。client做读写操作时不能只连接一个shard。\n\n### Sharding Strategy\n\n具体来说sharding的方式有两种：\n\n#### Hashed Sharding\n\n通过计算shard key的hash值来分到不同的chunk中。在使用hashed indexes来处理查询时，client不需要计算hash，这个由MongoDB自动完成。\n\n![hashed sharding](/img/hashed sharding.png)\n\n对于那种很接近的shard key（例如单调递增的），hash通常会比较均匀地把他们分到不同的chunk里。这样存在个问题：当我要取一定范围内的数据时，需要从各个不同的chunk里取，造成较大范围的broadcast operations。\n\n#### Ranged Sharding\n\n把shard key的值分成一定范围区间，然后分配。\n\n![ranged sharding](/img/ranged sharding.png)\n\n这样相近的shard key实在同一个chunk里的，这样mongos只需要把operation route到包含这些数据的shards。\n\n这个方式很依赖shard key的选择，不理想的话可能造成数据分配不平衡。\n\n### Zones in Sharded clusters\n\n在sharded clusters里，可以把几个shard对应到一个zone中，一个shard也可以对应到多个zone里。chunks只在相同zone下的几个shards中转移。\n\n![zoneMongoDB](/img/zoneMongoDB.png)\n\nzones存在的目的是提高在大的sharded clusters里数据的本地化程度。\n\n\n\n","tags":["big data"]},{"title":"局部敏感哈希(LSH)——Locality Sensitive Hashing","url":"/2018/09/19/局部敏感哈希/","content":"\n课上学了一种大数据计算相似性的算法，觉得很巧妙，在这里总结下。\n\n## 使用场景\n\n求相似性的场景有很多，比如比较两图相似性从而做一些处理，或者搜索引擎比较文本相似性从而确定返回的搜索结果等等。这种场景常常面临的问题是要进行比较的数据特征量非常大，同时比较对象又非常多，不能直接比较。LSH正是解决了这个问题。\n\n## LSH流程\n\nLSH分为三步：\n\n1. Shingling: Convert documents to sets\n2. Min Hashing: Convert large sets to short signatures, while preserving similarity\n3. Locality Sensitive Hashing: Focus on pairs of signatures likely to b from similar documents - **Candidate pairs!**\n\n![LSHFlow](/img/LSHFlow.png)\n\n要测相似性就要确定距离/相似性函数，LSH使用的是jaccard距离/相似性，具体如下：\n\n- sim(C1,C2) =  | C1 ∩ C2 | / | C1 U C2 |\n- d(C1,C2) = 1 -  | C1 ∩ C2 | / | C1 U C2 |\n\n### 第一步：Shingling\n\nShingling是将整个文档转化为一个set，这里我们不重视文档顺序、重要词等信息，所以shingling是最好选择。\n\nK-shingle（k-gram）则是将文档分成以k个token为一个单位的序列，token可以是字符、单词或者其他。例如k=2，则对于文档D = abcab，2-shingle的set则是：C = S(D) = {ab, bc, ca}。k的取值：\n\n- k = 5，对于比较短的文章\n- k = 10，对于很长的文章\n\n随后可以将每一个shingle作为一个维度将所有文档联合起来构建一个0/1矩阵，这个 矩阵:\n\n- Rows = elements (shingles)\n- Columns = set(documents)\n\n例如：\n\n|           | C1   | C2   | C3   | C4   |\n| --------- | ---- | ---- | ---- | ---- |\n| Shingles1 | 1    | 1    | 1    | 0    |\n| Shingles2 | 1    | 1    | 0    | 1    |\n| ...       | 0    | 1    | 0    | 1    |\n| ...       | 0    | 0    | 0    | 1    |\n\n很显然这个矩阵非常稀疏的。这样计算量非常大，所以有第二步来“压缩矩阵”。\n\n### 第二步：Minhash\n\nminhash需要压缩矩阵，将原来100，000维的压缩到100维，采用了很聪明的方法：概率聚类大法！\n\n具体描述：将上述矩阵每一列随机排列，然后记录第一个1的对应位置（位置标注方式不一定是递增序列，而和产生随机的方式有关）\n\n![minhash](/img/minhash.png)\n\n每一次随机都会产生1个signature，100次随机就针对每个文档产生一个长度100的signature，文档之前只要互相比较一下这个signature的相似性就相当于比较文档的相似性。\n\n这个算法的可行性用公式表达则是：\n\n- Pr[ min( π (C1 )) = min ( π (C2 )) ] = sim(C1,C2)\n\n这个结论是可以证明的，此处略过。\n\n由于这里要随机100次，这个显然是比较麻烦的，所以随机用hash方法来进行。\n\n全局hash为：\n\n- h_a,b(x) = (( a*x + b )mod p ) mod N\n\n然后每次只要随机产生a，b两个值就好。之后按照产生的hash函数的大小顺序进行排列。hash产生重复的值也不要紧，signature就是产生的这个hash值即可。\n\n这里解决了维度太多的问题，但相互比较问题还是没解决，因为文档很多，所以要非常多次比较。第三步将减少比较流程，只要候选文档之间相互比较即可。\n\n### 第三步：Locality Sensitive Hashing\n\n这里先讲M矩阵分成了b个bands，每个bands里包含r行，如图：\n\n![LshBand](/img/LSHBand.png)\n\n之后再对每个内容进行hash，使其map到k个buckets中，如果C1,C2有大于等于一个band在同一buckets中，则对他们进行比较，如图：\n\n![hashBand](/img/hashBand.png)\n\n这样将每个bands都map到Buckets的不同块里，如果map到同一个buckets的块里，则称为candidate pair，他们不一定相等，要进行比较。但如果map到不同的块里，表明这两个band一定不同，不需要进行比较。\n\n下面验证一下这个算法的可行性。\n\n假设C1和C2的相似度为t，b为bands数目，r为每个band多少row，可以得知：\n\n- band中所有row相等（该band会映射到同一bucket块）概率： t^r\n- band中row存在不相等row概率：1-t^r\n- 没有一个band是相等的概率：(1 - t^r)^b\n- 至少有一个band相等的概率：1 - (1 - t^r)^b\n\n按照公式计算，当b取20，r取5，对于相似度30%的文档C1,C2（不希望有bands出现在同一buckets中），有4.7%的几率至少一个bands分在同一buckets中，我们花多余的代价去计算它概率比较小。而相对于相似度80%的文档，有99.96%的几率我们要去比较他们，我们会错过一对相似文档的概率不到0.04%，可以接受。\n\n综上改算法是可行的。","tags":["big data"]},{"title":"图说mysql的四种join","url":"/2018/09/14/图说mysql的四种join/","content":"\n![屏幕快照 2018-09-14 上午10.54.59](/img/rightJoin.png)\n\n![屏幕快照 2018-09-14 上午10.55.09](/img/leftJoin.png)\n\n![屏幕快照 2018-09-14 上午10.55.39](/img/innerJoin.png)\n\n![屏幕快照 2018-09-14 上午10.55.46](/img/crossJoin.png)\n\n","tags":["database"]},{"title":"MapReduce终极整理","url":"/2018/09/13/MapReduce终极整理/","content":"\n这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。\n\n首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。\n\n### 使用MapReduce的目的\n\n用一个工具只有知道他的目的和优点，才能找到最合适的使用场景。简单来说，mapreduce提供了一个自动并行和分布式计算的工具（接口），在大规模集群中性能出众。由于它隐藏了这些分布式系统的细节，所以很多不懂分布式的程序员也可以基于此搭建分布式系统。对于一些web服务器、爬虫、排序算法、机器学习、数据挖掘等都很有用。\n\n### MapReduce的架构和运行\n\n这是本文的重点，不多说，直接上图。\n\n![mapreduce](/img/mapreduce.png)\n\n这幅图是直接从论文截下来的，生动的讲述了mapreduce的整个流程：\n\n1. 将用户input文件主动分割成成16-64MB大小的块，然后在集群中做一些备份（一般备份3份，其中两份在同一个机架上）\n2. master发挥作用，将选择一些idle的worker给其分配map或者reduce任务。\n3. 这时候worker就会去读那些被分割的文件，将一个key/value对读取到用户定义的map函数中。worker在读取这些文件时，会优先读取本地存着的，如果本地没有，选取离它最近的文件，减少网络传输代价。\n4. map函数会产生一些中间过程数据，然后周期性的通过buffer将它存在本地磁盘上，然后将地址、文件名等信息通知master，这些中间文件通过hash(key) mod R的方式被分成R份（R是reduce worker的数量），master就会将这些块分配给对应的reducer。\n5. reducer会通过RPC方式读取到这些中间过程数据，然后进行一个排序（shuffle），这个shuffle会让相同value聚在一起。这个很必要，因为不同key会映射到同一个reduce任务上。\n6. 只有在map全部结束后，reduce才会开始。在reducer中处理这个有序数据时，遇到相同key就会把value放在用户定义的reduce函数中。最终reduce函数会把结果文件输出到这个reduce partition中。\n\n最终得到的结果其实就是part-r-00000这样形式的不同文件。用户没必要把output文件最后聚在一起，如果需要的话，这个结果还可以作为下一轮mapreduce的输入。\n\n### 在有Combiner后过程的改良\n\n上面的过程是论文中解释的，但现在的程序都用了Combiner，Combiner其实和reduce的代码是一样的，那他是做什么用的呢？\n\nCombiner其实是针对本地的map后的结果进行pre-reduce（或者叫mini-reduce），例如wordcount在map之后大概是('a':1),('b',1),('a',1),('c',1)这样，combiner将local的这些先做一次reduce，变成('a',2),('b',1),('c',1)，之后再去shuffle和reduce。\n\n综上mapreduce整个过程分为四步：\n\n​\t\t\t\t  \t\t\t**Map -> Combiner -> Shuffle -> Reduce**\n\n### Master节点的任务和结构\n\n对于每个map任务或者reduce任务，都分成三种状态：idel, in-progress, completed。每个任务的这些状态都存在master节点上。\n\nMaster节点是作为map任务和reduce任务通信的管道的。master要存储并更新M个map任务产生的M * R个块(每个map任务产生R个块)的位置信息、文件名等。即，master承担 O(M+R)个scheduling决策，存储 O(M * R)个状态信息。\n\nM：map是的m块数据，R: reduce时的r块数据，W: worker机器的数量；三者关系为：\n\n​\t\t\t\t\t\t\t**M > R > W**\n\n### Worker的容错问题\n\n首先，master出错的话，没啥好说的，直接告诉用户就成。\n\nworker一旦出错，master应该要感知到，有两种策略感知：\n\n- Pull模型 ：worker通过发送heartbeat给master，master在感知到heartbeat之后在heartbeat response里给worker分配任务。\n\n- Push模型：master一直ping worker，当一段时间ping不通后说明worker失败了。\n\n这个时候master会将worker的任务分配给其他的worker去执行。对于执行完的worker，状态会重新标定为idle，表示有资格接受任务。失败的worker也会被标定为idle，同样可以接受任务。\n\n对于在失败的worker上completed的map任务，在其他worker上需要重新执行，因为他们存在本地的中间数据访问不到了。但对于在失败的worker上completed的reduce任务则不需要重新执行，因为他们的结果文件存在了global的文件系统下。\n\n在执行mapreduce时对于一些**straggler**（落伍者），有两种处理方式：\n\n- Job stealing: 对这个job进行分片，将没完成的部分交给其他的worker完成。\n- Speculative execution: master，这时候两个类似竞争的关系，当其中一个结束时，这个task会被标定为completed，同时另一个task将会被放弃。\n\n\n\n","tags":["big data"]},{"title":"课程信息整理","url":"/2018/09/13/课程信息整理/","content":"\n之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。\n\n---\n\n### [CMSC5741 - Big Data Technology and Applications]()\n\ntextbook: Mining of Massive Dataset（已借）\n\nInstructor: [Prof. Michael R. Lyu](http://www.cse.cuhk.edu.hk/lyu/)\n\nTutor: Zeng Jichuan\n\nexam: **Nov.6 Midterm exam** \n\nAssessment Scheme and Deadlines:\n\n- 20% Assignment\n- 40% Midterm examination\n- 40% Project : Proposal, Presentation, Report\n\nBackgroud Knowledge: Tensorflow, Amazon EC2\n\n---\n\n### [CSCI5570 - Large Scale Data Processing Systems](http://www.cse.cuhk.edu.hk/~jcheng/5570/)\n\nwebsite Account:\n\n- Username : csci5570\n\n- Password : huskydatalab\n\nInstructor: [Prof. James CHENG](http://www.cse.cuhk.edu.hk/~jcheng) \n\nTutor: Tatiana Jin\n\nLecture/Lab: \n\n- Tuesday 13:30 Lecture && Lab\n- Wednesday 14:30 Lecture\n\nAssessment Criteria:\n\n- 30% Survey paper : select one topics (DDL: Dec 10, 2018)\n- 70% project : deadline: DEC 20\n\n---\n\n### [CMSC5724Data Mining and Knowledge Discovery](http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/18-fall/)\n\nInstructor: [Yufei Tao](http://www.cse.cuhk.edu.hk/~taoyf/)\n\nTutor: Shangqi Lu\n\nAssessment Criteria:\n\n- 30% Project\n- 30% Short Tests (three times in class)\n- 40% Final (Open-book)\n\n---\n\n### CMSC 5720 - Project I\n\nInstructor: [Prof. James CHENG](http://www.cse.cuhk.edu.hk/~jcheng) \n\nOptions:\n\n1. NN-descent （kn-graph的近似算法）\n2. search with fa2ss（facebook的相似性检索库）\n3. multiprobe with tree（基于树的哈希方法）\n4. LSH for MZPS （lsh）\n5. 数据收集->存储->分析 系统\n6. topic modeling on ps archetective -> （LDA FlexPS->parameter server拓展）\n7. 调度算法，同步/异步 任务，在不同集群下测试算法，分布式，任务的表现\n8. 矩阵分解 Distributed MF（矩阵分解） on Actor Framework （nomad,lftf acf或者akka -> cpu to gpu to scheduling）\n9. Clustering-aware query (database query optimizer)\n\n","tags":["class note"]},{"title":"地道的口语表达积累","url":"/2018/09/01/地道的口语表达/","content":"\n### 1. 给别人加油\n\n不要再用fighting啦～\n\n**Go get‘em.**\n\n完整说法：Go get'em tiger.\n\n追女生：go get her\n\n\n\n### 2. 问别人想不想要\n\n不要再用do you want啦～\n\n**be up for something**\n\ne.g: \n\nAre you up for going clubing?\n\n你想去蹦迪吗？\n\nbe up三个意思：\n\n- 醒着 Are you up?\n\n- 下一个 who's up next?\n\n- 怎么了 what's up？\n\n\n\n### 3. 和朋友约出去玩\n\nplay with不行，一般是小朋友或者人和宠物，不适合成人一起玩\n\nhang out\n\n还可以用chill out，自己一个人出去玩\n\ne.g.\n\n Do you wanna hang out with us?\n\n\n\n###4. 如何约一场“夜生活”\n\n**night out～(名词)**\n\ne.g. \n\nIt's been a while since we had a girls' night out.\n\n姐妹们我们已经很久没有出去玩啦\n\nhave(need) a night out.\n\ne.g.\n\nIf you are up for a boys' night out~\n\n如果想约不会太晚的夜生活用：evening out\n\nNight owl 夜猫子"},{"title":"daily life in HK 2","url":"/2018/08/30/daily life in HK 2/","content":"\nSigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can't enjoy your life. While, I come here not for enjoy the postgraduate career but for open my mind and learn something useful.\n\nAnother feeling deeps me most is if I can't speak english well, the embarrassment will rise upon my face. In the afternoon, I met with my project teacher about the future work. From what he said, I could speculate that the former students may make him disappointed. As for project, he gave us much freedom to do what we interested in based on our foundation.\n\n\n\nAt night, I hung out with Doc.Lin, and it is my first time to meet with old friends after I came to HK. We walked around 中环 and 香港岛, where we smell at the taste of dollars. What a fame of capitalism! Then we returned to 九龙 by ship and ate dinner there. Life of Doc.Lin seemed as relaxed and funny as the time in SAP. It's admired by everyone, right?","tags":["life"]},{"title":"daily life in HK  1","url":"/2018/08/27/daily life in HK  1/","content":"\nFinally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog unregularly.\n\n\n\nThe journey from SZ to HK is tremendously hard for my. I took all my packages (more than 30kg, wooo~~~) comming in HK custom. At that time, I thought the most difficulty time had been gone though. I was wrong, cause it was just the start of my suffering. The long lines of bus station made me crying, at the same time, I realized that I forgot to take some change. Of course, as a freshman of HK, I have no HK card. So I had to beg all around. After two times of tranfer, I got to Sha Tin station.\n\n\n\nFrom the introduction of my room, I knew the building I live is not far from the station. While, I made the mistake for twice. Unfortunately, It lies on a unknown mountain, so I was supposed to climb mountain with huge bags.\n\n\n\nFinally, I got in my rooms and met with my new roommate, who is a programmer, too. He is a nice guy and I was looking forward to make more friends here.\n\n\n\nWhat was worth to mention is we have a free dinner organized by our college, other guys in my table feel embarrassed to pack the left-overs so I took it all back. It solved my two-day meal problems.\n\n\n\nNever be shamed with yourself, keep on  doing what you believe in and you will make it sooner or later!","tags":["life"]},{"title":"20180811日记","url":"/2018/08/11/title 20180811日记/","content":"\n估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！\n\nFlag～ X\n\n\n\nFlag~\n\n\n\nFlag~\n\n\n\nFlag~","tags":["life"]},{"title":"mac在nginx下部署php遇到的坑","url":"/2018/08/09/mac在nginx下部署php遇到的坑/","content":"\n受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php不太熟悉（之前都用xampp这类软件），所以花了一点时间才搞定。\n\n首先要明白的是，nginx本身不能处理php，它只是一个web服务器，当前端请求php时，nginx需要把界面发给php解释器处理，然后把结果返回给前端。一般地，nginx是把请求发给fastcgi管理进程处理。如nginx中配置：\n\n```xml\n        location ~ \\.php$ {\n            root           html;\n            fastcgi_pass   127.0.0.1:9000;\n            fastcgi_index  index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;#这里原来不是$document_root，搞得我很蒙，还好网上查到改好了，不然会报file not found\n            include        fastcgi_params;\n        }\n```\n\n所以要启动一个fastcgi，这里就用到了php-fpm，它是一个php fastcgi管理器，只用于php语言（旧版php的要单独下php-fpm，我用的php-fpm已经集成了这个）。\n\n这里有很多奇怪的问题。\n\n**第一次运行php-fpm**\n\nfailed: 找不到/private/etc/php-fpm.conf文件，\n\nSolution:但这个目录下有个php-fpm.conf.default的文件，所以cp了正确名字的新文件\n\n**第二次远行php-fpm**\n\nFailed: 找不到/usr/var/log/php-fpm.log \n\nSolution：根本没有这个目录，到conf文件里改了但是没有效果，没办法我就通过下面的命令执行php-fpm(后面都用这个命令执行)\n\n```shell\nphp-fpm --fpm-config /private/etc/php-fpm.conf  --prefix /usr/local/var\n```\n\n**第三次运行php-fpm**\n\nFailed: No pool defined. at least one pool section must be specified in config file\n\nSolution：到/etc/php-fpm.d/ 目录下有文件www\\.conf.default，cp一份名为www.conf的文件\n\n**第四次运行php-fpm**\n\nFailed：端口被占用\n\nSolution：杀掉这个进程\n\n```shell\nsudo lsof -i tcp:9000#找到占用9000端口的进程号\nkill -9 port#杀！\n```\n\n**第五次运行php-fpm**\n\n成功！\n\n\n\n##补充：\n\n在nginx上配的时候又有所一点不同，在mac上php-fpm直接listen了9000端口，但在服务器上它listen了php7.0-fpm.sock但socket文件，这种方式可能快一点，所以要在nginx上php的配置那边将\n\n```shell\nfastcgi_pass 127.0.0.1:9000;\n```\n\n改成：\n\n```shell\nfastcgi_pass unix:/run/php/php7.0-fpm.sock;\n```\n\n才能成功运行php\n\n### 继续补充\n\n很有意思的一个东西，要上传27m的一个视频，nginx直接报了413 Request Entity Too Large，是我没设置...\n\n到nginx的配置（set-enabled/default）里面添加\n\n```shell\nserver {\n    ...\n    client_max_body_size 80m;\n    ...\n}\n```\n\n重读配置、重启服务器\n\n```shell\nnginx -s reload\nservice nginx restart\n```\n\n然后还要去修改php.ini，在其中修改两条配置\n\n```shell\nupload_max_filesize = 80M\npost_max_size = 80M\n```\n\n然后关掉php-fpm的进程，再重启即可～\n\nps：贺老师真的完全不研究的...mp4传不上去只是在系统里没添加这种类型，这种事都要我自己去找...难受 :(\n\n**note：**在ubuntu下现在比较推荐用apt而不是apt-get...so，是时候改变了！","tags":["mac"]},{"title":"看Husky的一点整理","url":"/2018/08/08/Husky文档整理/","content":"\nUsername : csci5570\n\nPassword : huskydatalab\n\nhusky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受...）\n\n## 配置\n\n```shell\n# Required\nmaster_host=master#master跑的地方\nmaster_port=10086#master绑定的端口\ncomm_port=12306#worker绑定的端口\n\n# Worker information\n[worker]\ninfo=worker1:4#worker1有4个线程\ninfo=worker2:4#worker2有4个线程\n\n#如果用了hdfs，配置hdfs路径\nhdfs_namenode=master\nhdfs_namenode_port=9000\n```\n\n运行的时候用\n\n```shell\n./program --conf=/path/to/config.ini\n```\n\n写入配置。\n\n## 组件\n\n### Object List\n\nObject List（objList）是husky中最主要的对象，可以把任何对象都存在objlist中，两个objlist通过channel传递消息。\n\n```c++\nclass Obj {\n   public:\n    using KeyT = int;\n    KeyT key;\n    const KeyT& id() const { return key; }\n    Obj() = default;\n    explicit Obj(const KeyT& k) : key(k) {}\n};\n```\n\n创建一个obj只要3步：\n\n1. 定义一个key的类型（keyT），一般都用int\n2. 写一个id（）函数来返回该对象对应的key\n3. 需要一个默认构造函数，还需要一个能够接收key参数的构造函数\n\n接下来就可以创建、使用、删除Object List\n\n```c++\n//创建名叫my_objlist的objlist\nauto& objlist = ObjListStore::create_objlist<Obj>(\"my_objlist\");\n\n//将obj传入创建好的objlist中\nObj obj(3);\nobjlist.add_object(obj);\n\n//通过名字拿到对应的objlist，注意这里的auto关键字，自动判定类型，很舒服！\nauto& objlist2 = ObjListStore::get_objlist<Obj>(\"my_objlist\");  \n\n//通过名字删除objlist\nObjListStore::drop_objlist(\"my_objlist\");\n```\n\n为了让添加在objlist中的obj被其他线程感知并利用，在多线程情况下需要将objlist全局化一下，husky已经封装好该方法\n\n```c++\nglobalize(objlist);\n```\n\n接下来就是**最重要**的一个函数list_execute（），它规定了list里的每个object需要做的事，这个函数是用户自己定义的。它有两个参数：\n\n-  第一个是要操作的objlist\n- 第二个是这个objlist中每个obj要做的事，例如下面函数就是obj在log中打印id，包括之后用channel发送或接收消息也都是在这个函数里\n\n```c++\nlist_execute(objlist, [](Obj& obj) {\n    base::log_msg(\"My id is: \" + obj.id());\n});\n```\n\n### Channel\n\nchannel就是object和object互相通信的工具，他们的关系类似于城市和公路。husky中有四种channel：\n\n- Push Channel：最常见的点对点通信\n- Push Combined Channel：在push channel基础上增加了合并发给同个obj的\n- Broadcast Channel：将一个key-value广播出去，任何地方都可以通过key拿到值\n- Migrate Channel：用来migrate对象，将一个对象发送到另一个线程上\n\nchannel的创建、使用和drop（一定要主动销毁）\n\n```c++\n// create PushChannel\ntemplate <typename MsgT, typename DstObjT> \nstatic PushChannel<MsgT, DstObjT>& \ncreate_push_channel(ChannelSource& src_list,\n                    ObjList<DstObjT>& dst_list,\n                    const std::string& name = \"\");\n\n// Get PushChannel through name\ntemplate <typename MsgT, typename DstObjT>\nstatic PushChannel<MsgT, DstObjT>& \nget_push_channel(const std::string& name = \"\");\n\n// Drop channel through name\nstatic void drop_channel(const std::string& name);\n```\n\n下面通过例子来说明：\n\n首先，要想创建channel，就要确定发消息的源objlist和目的objlist，当然，参数里的目的objlist必须是全局化的\n\n```c++\n//创建一个push_channel\nauto& ch = ChannelStore::create_push_channel<int>(src_list, dst_list);\n```\n\n一般来说，channel是放在list_execute（）函数里用的，要想清楚从哪个obj发，发什么，哪个obj接收（通过key来标注）\n\n```c++\n//push channel\n//发送端代码\nlist_execute(src_list, [&ch](Obj& obj) {\n    ch.push(msg, key);  // send msg to key\n});\n\n//接收端代码\nlist_execute(dst_list, [&ch](Obj& obj) {\n    auto& msgs = ch.get(obj); // The msgs is of type std::vector<MsgT>, MsgT is int in this case\n});\n\n//broadcast channel\nauto& ch4 = ChannelStore::create_broadcast_channel<int, std::string>(src_list);\nlist_execute(src_list, [&ch4](Obj& obj) {\n    ch4.broadcast(key, value);  // broadcast key, value pair\n});\nlist_execute(src_list, [&ch4](Obj& obj) {\n    auto msg = ch4.get(key);   // get the broadcasted value through key.\n});\n```\n\n\n\n###Aggregator\n\n用来执行一些聚合操作的类，可以用来做求前k大值，统计数量，计算机器学习梯度总数等。他的构造函数需要两个参数（或以上），一个是init值，另外是lambda函数\n\n```c++\nAggregator<int> agg(0, [](int& a, const int& b){ a += b; });\n```\n\n这个lambda函数就是aggregate的规则。\n\n在创建完agregator后，就要使用它了。可以用update函数或者update_any函数（比update可接受参数类型多）来进行aggregator，例如\n\n```c++\nagg.update(1);//aggregator值加1\n```\n\n在聚合完之后，更新的值其实只在本地，为了让这个值在全局响应要用HuskyAggregatorFactory::sync()函数。另一种方式是通过HuskyAggregatorFactory::get_channel()来拿到通道，然后在list_execute中通过这个channel把消息传播出去，这种方法最后也会去调用sync()函数\n\n```c++\n//两种方式\nAggregatorFactory::sync();\n\n// or using aggregator channel\nauto& ac = AggregatorFactory::get_channel();\nlist_execute(obj_list, {}, {&ac}, [&](OBJ& obj) { \n  ...  // here we can give updates to some aggregators\n});\n```\n\n当全局划这个聚合之后，就可以用get_value()函数得到值了\n\n```c++\nint sum = agg.get_value()\n```\n\n这个值是被全局共享的，所以对他的修改会影响其他executor，并可能有线程安全问题","tags":["CUHK"]},{"title":"Hadoop了解一下","url":"/2018/08/05/hadoop了解一下/","content":"\n搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。\n\n### map reduce\n\n- automatic parallezation\n- Fault tolerance\n- a clean abstraction for programmers\n\nBSP model : Bulk sychronous parallel\n\nidentity reducer？re\n\n## 基本概念\n\n### Apache Hadoop与HDFS\n\nHadoop是一个大的生态系统，最主要是hdfs和一个基于mapreduce的分布式计算引擎。hdfs就是一个文件系统，在mapreduce的时候（包括用spark的时候）都需要对应文件在hdfs里。\n\n**block块：**HDFS在物理上是以block存储的，block大小可以通过配置参数（dfs.blocksize）来规定，默认128M，可以减少寻址开销。大文件会被切分成很多block来存，而小文件存储则不会占用整个块的空间。\n\n**NameNode：**是master。负责管理文件系统的namespace（可以理解是指向具体数据的文件名、路径名这种）和客户端对文件的访问。data path？\n\n**(非Yarn)JobTracker：**在NameNode上，协调在集群运行的所有作业，分配要在tasktracker上运行的map和reduce任务。\n\n**DataNode：**是slave。datanode则负责数据的存储。\n\n**(非Yarn)TaskTracker：**在datanode上，运行分配的任务并定期向jobtracker报告进度。\n\n**流式访问：**指hdfs访问时像流水一样一点一点过。这样也决定了hdfs是一次写入、多次读取的特性，同时只能有一个wirhter。这样访问方式适合做数据分析，而不是网盘这种。\n\n**rack-aware（机架感知）：**这是hdfs的复制策略。hdfs为了数据可靠一般会将数据复制几份（默认三份）。同一个机架的机器传输速度快，不需要通过交换机。为了提高效率，一台机器的数据会把一个备份放在同一机架（相同rack id）的机器里，另一个备份放在其他机架的机器上。机架的错误率很小，所以不影响可靠性。\n\n**hdfs的特点：**\n\n- 面对构成系统的组件数目很大，所以对硬件的快速检测错误并自动回复非常重要\n- hdfs需要流式访问他们的数据集\n- 运行的数据集非常大，一个典型文件大小一般在几G到几T\n- 文件访问模型是“一次写入、多次访问”\n- 将计算移动到数据附近闭将数据移动到计算更好\n\n\n\n### Yarn\n\nyarn其实是解决了经典mapreduce中一些问题（例如：jobtracker太累导致的可扩展性问题）的新一代hadoop计算平台。\n\n**ResourceManager：**代替jobtracker，以后台进程的形式运行。追踪有哪些可用的活动节点和资源，指出哪些程序应该何时或者这些资源。\n\n**ApplicationMaster：**代替一个专用而短暂的JobTracker。用户提交一个应用程序时，会启动applicationmaster这个轻量级进程实例来协调程序内任务（监视进度、定时向resourcemanager发送心跳数据、负责容错等），计算数据需要的资源并向resourcemanager申请。它本身也是在一个container里运行的，且可能与它管理的任务运行在同一节点上。\n\n**Container：**是yarn中资源的抽象，封装了某个节点上一定量的资源（如cpu和内存等资源）。它的分配是由applicationmaster向resourcemanager申请的；而它的运行则是applicationmaster向资源所在的nodemanager发起的。\n\n**NodeManager：**代替tasktracker。拥有很多动态创建的资源Container。容器大小取决于它所包含资源量，而一个节点上的容器数量由配置参数和除用于后台进程和操作系统以外资源总量决定。\n\n## 基本架构\n\nHdfs采用了master/slave架构。一个hdfs集群由一个namenode和一群datanodes组成。简单来说，就是hdfs通过namenode暴露出了文件系统命名空间的操作，包括打卡、关闭、重命名文件等等。在这个文件系统对一块数据的操作会映射到具体的datanodes上。\n\n所以一般是一台机器上搭namenode，然后datanode在其他各个机器上。\n\n![hdfsarchitecture](/img/hdfsarchitecture.jpg)\n\n### Hadoop Streaming\n\njava这么规范的东西有人就是不喜欢，非得用python啥的写hadoop，所以就有了hadoop streaming，支持其他语言的hadoop操作。\n\n### Haddop Resource Management\n\nhadoop的资源管理进化\n\n## 基本操作\n\n### 单节点测试\n\n比较无聊，只是测一下能不能跑，不需要运行什么\n\n```shell\n$ cd Cellar/hadoop/3.1.0/libexec\n$ mkdir input#不能是别的名字\n$ cp etc/hadoop/*.xml input\n$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar  grep input output 'dfs[a-z.]+'\n$ cat output/*\n```\n\n### 伪分布式测试（pseudo-distributed）\n\n1. 保证本机已经装好hadoop，java1.8（java9有些函数被废了会报错）\n\n2. 配置本机ssh，确保\n\n   ```shell\n   ssh localhost#mac默认不允许，需要手动去打开\n   ```\n\n   可以运行。注意：mac默认不允许任何机器远程登录，需要到  系统偏好设置 -> 共享 去勾选远程登录。\n\n3. 配置HDFS，包括core-site.xml文件和hdfs-site.xml文件。前者配置用于存储HDFS的临时文件目录和hdsf访问端口，后者确定复制份数\n\n   ```xml\n   <!-- core-site.xml -->\n   <configuration>\n       <property>\n          <name>hadoop.tmp.dir</name>\n   \t   <value>[$HADOOP_HOME]/tmp</value>\n           <!--如果无此目录则去mkdir一个-->\n          <description>A base for other temporary directories.</description>\n       </property> \n       <property>\n           <name>fs.default.name</name>\n           <value>hdfs://localhost:9000</value>\n       </property>\n   </configuration>\n   ```\n\n   ```xml\n   <!-- hdfs-site.xml -->\n   <configuration>\n       <property>\n           <name>dfs.replication</name>\n           <value>1</value>\n       </property>\n       <property>\n           <name>dfs.namenode.name.dir</name>\n           <value>[$HADOOP_HOME]/hdfs/namenode</value>\n       </property>\n       <property>\n           <name>dfs.datanode.data.dir</name>\n           <value>[$HADOOP_HOME]/hdfs/datanode</value>\n       </property>\n   </configuration>\n   ```\n\n4. 格式化HDFS\n\n   ```shell\n   hdfs namenode -format\n   ```\n\n   成功的话在tmp目录下可以看到dfs文件\n\n5. 启动各个节点\n\n   ```shell\n   #直接启动全部（包括hdfs和yarn）\n   sbin/start-all.sh\n   \n   #分别启动\n   hdfs --daemon start NameNode#启动namenode\n   hdfs --daemon start DataNode#启动datanode\n   hdfs --daemon start SecondaryNameNode#它是namenode的快照，保证了namenode的更新\n   jps#用来查看这些节点是否真的启动了\n   ```\n\n6. 在HDFS上创建文件夹及文件\n\n   ```shell\n   hdfs dfs -mkdir /demo#在hdfs上创建demo文件夹\n   hdfs dfs -ls /demo\n   hdfs dfs -put test.input /demo#将本地的test.input文件发到hdfs上\n   ```\n\n7. 配置Yarn的mapred-site.xml和yarn-site.xml\n\n   ```xml\n   <!-- mapred-sited.xml -->\n   <configuration>\n       <property>\n           <name>mapreduce.framework.name</name>\n           <value>yarn</value>\n       </property>\n       <property>\n          <name>mapreduce.application.classpath</name>\n          <value>share/hadoop/mapreduce/*</value>\n   \t</property>\n   \t<!-- 如果不加这个property，在后面运行mapreduce任务时会报找不到包 -->\n   </configuration>\n   ```\n\n   ```xml\n   <!-- yarn-site.xml -->\n   <configuration>\n   <!-- Site specific YARN configuration properties -->\n       <property>\n           <name>yarn.nodemanager.aux-services</name>\n           <value>mapreduce_shuffle</value>\n       </property>\n       <property>\n           <name>yarn.resourcemanager.hostname</name>\n           <value>localhost</value>\n       </property>\n      \n   </configuration\n   ```\n\n8. 运行mapreduce任务\n\n   ```shell\n   hadoop jar hadoop-mapreduce-examples-3.1.0.jar wordcount /demo/test.input /demo-output/\n   ```\n\n   可以到对应文件里查看运行结果","tags":["big data"]},{"title":"Spark使用整理","url":"/2018/08/05/spark使用整理/","content":"\nRecently I read the book <\\<spark in action>>, the summation about this book will be wrote down here.\n\n### the notion of RDD(resilient distributed dataset)\n\nThe RDD is the fundamental abstraction in spark it represents a collection of elements that is\n\n- Immutable ( read-only )\n- Resilient ( fault-tolerant )\n- Distributed\n\nImmutable : allow xspark to provide important fault-tolerance guarantees in a straightforward manner.\n\nDistributed : machines are transparents to users, so working with RDDs is not much differents from working with a lists, maps and so on.\n\nResilient: whereas other systems facilitate fault-tolerance by replicating data to multiple machines, RDDs provide falut-tolerant by logging the transformations used to build dataset rather than itself. When fault happens, it just need to repair a subset of dataset.\n\n### Basic RDD Operation\n\nthere are two types of operations\n\n- Transformations\n- actions\n\nTransformations : like *filter* and *map*, perform some useful data manipulation and it will produce a new RDD\n\nactions : like *count* and *foreach*, trigger a computations to return a result.\n\n### Spark SQL\n\nDataframe is a basic elements of Spark, similarily with other Dataframe in python or R, it represent a table-like data with named columns and declared column types. The different among them is it's distributed nature and spark's catalyst.\n\nFundamental concepts:\n\n- Spark Sql: Consult from Table catalog; Query from Relational DB, Read data from HDFS. Spark Application can using DataFrame DSL to submit spark job, as for non-spark client could connect though JDBC.\n- Table catalog: It contains information about registered DataFrames and how to access their data.\n- DataFrame: user reads its data from a table in a relational databases.\n\nSpark sql is supported by Hive. Hive is a distributed warehouse built as a layer of abstraction on the top of Hadoop's MapReduce. It has its own dialect named HiveQL.\n\n#### Create DataFrame\n\nWe can create DataFrame in these three ways:\n\n- Converting from RDDs\n  - Using RDDs containing row data as tuples\n  - Using case classes\n  - Specifying a schema\n- Running SQL queries\n- Loading external data\n\nUsing RDDs containing row data as tuples : use toDF() method to convert RDD to DataFrame. However, all columns are of type String and nullable, which obviously is a bad solution.\n\nUsing case classes : the case class like this:\n\n```scala\ncase class Post(\n\ncommentCount:Option[Int],\n\nbody:String,\n\n......)\n\n```\n\nNullable fields are declared to be of type Option[T].\n\nSpecify a schema: This format of schema like this:\n\n```scala\nval postSchema = StructType(Seq(\n    StructField(\"commentCount\", IntegerType, true),\n    StructField(\"id\",LongType,false))\n)\n```\n\nThen invoke spark.createDataFrame(rowRDD, postSchema) to convert.\n\n#### DataFrame API Basics\n\nSelect:\n\n```scala\nval postIdBody = postsDF.select(\"id\",\"body\")\nval postIds = postsIdBody.drop(\"body\")//drop this column\n```\n\nFiltering:\n\n```scala\nval noAnswer = postsDf.filter(('postTypeId === 1) and ('acceptedAnswerId isNull))\n```\n\n\n\n\n\n","tags":["big data"]},{"title":"homebrew使用整理","url":"/2018/08/02/homebrew使用整理/","content":"\n现在新的mac基本都内置homebrew了吧，brew可以说是mac神器之一了。上手简单，但还是用法需要整理一下：\n\n###brew常用命令\n\n```shell\nbrew search 包名 #搜索包\nbrew info 包名#包信息\nbrew list #查看有哪些包\nbrew install 包名#安装包\nbrew uninstall 包名#删除包\n```\n\n### brew管理服务\n\nbrew还有个重要的任务就是管理服务，在我本机的：\n\n- Kafka\n- mysql\n- nginx\n- Redis\n- zookeeper\n\n都是用了brew进行管理，管理他们用\n\n```shell\nbrew services start 服务名#开启一个service\nbrew services stop 服务名#关闭一个service\n```\n\n每次开启一个服务，就会在～/Library/LaunchAgents里面增加一个plist文件，用来存储这个服务的一些版本信息，同时，本机所有其他服务可以通过\n\n```shell\nlaunchctl load *.plist #加载 \nlaunchctl unload *.plist #取消\nlaunchctl list#查看服务\n```\n\n来完成\n\n### brew其他命令\n\n```shell\nbrew link 包名\n```\n\n这里的link是指symbollink（有点类似于windows里的创建快捷方式）。以hadoop为例，在brew刚下载的hadoop只是存在/usr/local/Cellar目录下的，在全局环境下不能用hadoop命令。只有将其link到bin里（hadoop产生了27个symbolink），才能全局使用hadoop命令。在用brew install时会默认完成link的操作，除非出现意外。\n\n意外：在安装hadoop时出现了\n\n```shell\nError: The `brew link` step did not complete successfully\nThe formula built, but is not symlinked into /usr/local\nCould not symlink sbin/FederationStateStore\n/usr/local/sbin is not writable.\n```\n\n是因为我本机根本没有这个目录，同时权限也不够，所以我建了这个目录，然后用\n\n```shell\nsudo chown -r $(whoami) $(brew --prefix)/*\n```\n\n修改了对应权限，成功安装。这里引出了\n\n```shell\nbrew --prefix\n```\n\n这个是指brew存在的目录，其他brew操作都是在这个目录下搞的（例如cellar就是在这个目录下）。\n\n","tags":["mac"]},{"title":"hip-hop中一些slang积累","url":"/2018/07/27/hip-hop中一些slang积累/","content":"\n听了很久trap，很多都不太懂，之后不懂的就写在这里吧～\n\n###In my feelings - Drake （KIKI在b榜第一呆了一个月，我满耳朵都是kiki）\n\nHenny -> Hennessy：轩尼诗（酒）\n\nwraith：幽灵、幻影（劳斯莱斯品牌）\n\ncode to the safe：保险箱密码\n\nNeck work：类似的表达有give me some neck，指吹喇叭，等同于blow job或者sucking或者blowing of one's dick~\n\nNetflix and chill：一个internet meme，指约pao\n\nnet worth；资产净值（身价）\n\n\n\n### Alright - Kendrick Lamar\n\nMac-11：机械手枪的型号\n\nPussy&Benjamin：指代女人和金钱\n\nChevy -> chevrolet 雪弗兰\n\nGet reaping everything I sow：收获我所播种的（种豆得豆）\n\nMy karma：我的命运\n\nPreliminary hearing：法庭的初审\n\nfight my vice：和恶习斗争（vice除了副的还有恶习的意思）\n\nPopo：police，条子\n\nPreacher：牧师，传道人\n\nRegal：君主的，这里指别克君威车\n\nResentment：愤恨，不满\n\nSelf destruct：自我毁灭\n\nLucy -> Lucifer：是指撒旦，Satan, the devil（貌似只有lamar称lucifer为lucy～）\n\n\n\n### Bed - Nicki Minaj/Ariana Grande （沉迷黄歌，无法自拔～）\n\nwit'(with) your name on it：属于某人（不是真的指文字那种）\n\nSheet：床单\n\nCarter III：指Lil Wayne备受赞誉的专辑\n\nA Milli：Carter III专辑中的一首歌\n\nGOAT：greatest of all time 史上最佳\n\nturn down：拒绝（don't turn me down不要拒绝我）\n\nLingerie：内衣如图，不解释\n\n![img](/img/lingerie.png)\n\nblow it like a feather on you：像一片羽毛xx你（撩的不行啊～）\n\nStarting five：指nba那种首发五人，也可以用 Starting Line-up（首发阵容）\n\nThick skin：脸皮厚，不怕被骂～","tags":["hiphop"]},{"title":"pandas使用整理","url":"/2018/07/24/pandas使用整理/","content":"\n之前看了numpy，这两天看了pandas，也在这里整理一下。\n\n### 新建\n\n```python\nimport\tpandas as pd\n\ndata = {\n    'key1':['value11,value12'],\n    'key2':['value21','value22']   \n       }\ndf = pd.DataFrame(data)\n\ndf.index#描述dataframe的index的【开始/结束）和步长，注意这个是指df的index而非数据的id\ndf.columns#行名\ndf.dtypes#类型\ndf.size#数据总数\ndf.shape#数据形式（行，列）\ndf.ndim#维度\ndf.T#转置\n```\n\n### 从数据库中得到数据 && 将数据写入数据库\n\n```python\nfrom sqlalchemy import create_engine\n\n#连接mysql数据库\nengine = create_engine('mysql+pymysql://root:qh129512@127.0.0.1:3306/testdb?charset=utf8')\n\nsql = 'select * from person'\n\n#下面有三种获取数据的方式，返回的formlist就是一个dataframe\nformlist = pd.read_sql_query(sql,con=engine)#参数为sql语句\nformlist = pd.read_sql_table(table,con=engine)#参数为表名\nformlist = pd.read_sql(sql,con=engine)#参数可以是sql语句，也可以表名\n\n```\n\n下面是将数据写入数据库\n\n```python\ndf = pd.DataFrame(data,columns=['name,birthday'])#这里的columns就是指dataframe里有哪些comlumn，这里没有的df里也不会有，如果不添加columns这个参数就默认data中全部数据\n\ndf.to_sql(name='person',con=engine, if_exists='append',index=False)\n#if_exists:有三种fail->表存在就不写入；replace->表存在就删掉原来的表重新创建；append->在原表基础上追加数据。默认为fail\n#index：决定是否将行索引作为数据传入数据库，注意：如果数据库没有专门来存这个的就false，因为表里没有这里用true会有问题\n```\n\n### 使用dataframe\n\n直接取某个、某些数据\n\n```python\ndf['name'][0]\ndf.name[:5]\ndf[['name','birthday']][:2]\ndf.head()\n```\n\n取数据的切片\n\n```python\ndf.loc[:5,'name']#前一个参数为行索引，后一个是列索引名称\ndf.iloc[0:2,1]\n\n#有条件的切片\ndf.loc[(df['name'] == 'max'),:]#取name为max的切片\n\ndf.iloc[(df['name'] == 'max').values,:]\n```\n\n这里有两个函数，loc和iloc，区别有\n\n- loc第一个参数可以为series，例如我传入一个条件，其实相当于一个Series([True,True,False...])这种形式；iloc不可以穿series，但能传一个array，所以可以通过.values的形式传入条件\n- 行索引时loc是前后闭区间，而iloc是前闭后开区间（python中这种更常见）\n\n删除数据\n\n```python\ndf.drop(labels=rang(9,10),axis=0,inplace=True)\n'''\nlabels接收string、array，表示删除行/列的标签\naxis接收0、1，表示操作轴向，0为横，1为纵\ninplace接收boolean，代表操作是否对原数据生效\n'''\n```\n\n对dataframe中数据修改\n\n```python\n#直接声明就可以添加一列，如\ndf['prefix_name'] = 'MAC_' + df['name']\n\n#修改某个数据可以将其找出来然后直接赋值\ndf.loc[(df.name == 'max'),'name'] = 'maxhh'\n```\n\n随机数的使用\n\n```python\nseries = pd.Series(np.random.randint(high=10000,low=1000,size=8))#产生一个随机series\n\n\ndf.loc[:,'net-worth'] = series#将series值付给df\n```\n\ndataframe算数统计\n\n```python\ndf['net-worth'].mean()#平均值\nnp.mean(df['net-worth'])#另一种计算平均值的方法\ndf['net-worth'].min()#最小值\ndf['net-worth'].describe()#描述，包含很多数据可以用！\nnullNum = df.shape[0] - df['name'].count()#统计有多少空值\n```\n\n###  Category类型的使用\n\n可以将某一列转化成category类型，这样相当于做了一次分类处理，这样得到的描述性信息会很多\n\n```python\ndf['name'] = df['name'].astype('category')#注意这里是赋值而不是调用\n\ndf['name'].describe()\n```\n\n### 时间类型\n\npandas里有很多时间类型，不同类型用处不同。如timestamp主要用来记录时间，而timedelta用来做时间运算\n\n```python\n#有很多方法创建或转化出一个timestamp\ntoday_date = pd.to_datetime('2018-8/5')#随意的一个string都可以识别\n```\n\n\n\n注意一个概念，从数据库一个datatime拿出来的时间如果调用dtype的话发现是('<M8[ns]')类似的类型，展开来说：\n\n这个是属于机器的比较特别的类型:\n小端机器的类型：datatime[ns] == <M8[ns]\n大端机器的类型：datatime[ns] == >M8[ns]\n这个可以通过\n\n```python\nnp.dtype('datetime64[ns]') == np.dtype('<M8[ns]')\n#out: True\n```\n\n证明。\n所以当数据库中取出就是这种类型时，不需要再进行处理，但如果取出是个object，则用pd.to_datatime处理\n\n```python\ndf['birthday'] = pd.to_datatime(df['birthday'])\n```\n\n这里要熟悉的操作有：\n\n```python\nuser_birthday = [i.year for i in df['birthday']]#返回所有年份的list\n\nbirthday = df[df['birthday']<=pd.datetime(1991,1,1)]#按条件取时间时，一定要那拿timestamp与对应的pdf.datetime()相比\n```\n\n还有时间做加减法也是支持的,timestamp可以加一个tmiedelta来做时间的计算。注意timedelta的参数为weeks,days,hours以及更小的时间\n直接用+，-符号就可以做计算了。相反的，想算两个时间差，只要用两个datetime做减法，即可得到timedelta类型的时间差距\n\n```python\naweek_after = date + pd.Timedelta(weeks = 1)\n\ntoday_date = pd.to_datetime('2018-8-5')\n\ntime_delta = aweek_after - today_date#两个timestamp做减法，得到了一个timedelta类型的时间差\n```\n\n### 分组和聚合\n\n分组顾名思义，就是将数据按照一定条件进行分组，得到数据整体情况\n\n```python\ndata_group = df.groupby(by='birthday')\ndata_group.count()\n```\n\n这里的count()得到如下表结果\n\n|            | id   | name | net-worth |\n| ---------- | ---- | ---- | --------- |\n| birthday   |      |      |           |\n| 1987-01-10 | 1    | 1    | 1         |\n| 1989-03-10 | 6    | 0    | 4         |\n| 1993-08-05 | 1    | 1    | 1         |\n| 1995-12-12 | 1    | 1    | 1         |\n| 1996-10-12 | 1    | 1    | 1         |\n\n分组的直接结果并不能直接看，因为它返回的只是一个地址，但可以方便的查分组后的一些属性，如：count，head，max等等\n\n聚合，就是将一组数据做aggretate，聚合的函数自己定\n\n```python\ndf['net-worth'].agg([np.sum,np.mean])#针对net-worth数据计算两种agg，分别以sum和mean\n\ndf.agg({'net-worth':[np.sum,sp.mean]})#针对不同数据要进行不同的agg，可以用key-value的方式\n```\n\nagg函数的参数是计算函数，这个函数可以用numpy中一些简单的统计函数，如果复杂也可以自定义，如：\n\n```python\ndef Trinum(data):\n    return data.sum()*2\n    \ndf['net-worth'].agg(Trinum)#agg函数的参数可以是函数，该函数将data传入做处理，然后返回即可\n#稍微注意下sum后面的括号，没有括号是函数，有括号的是执行，但必须声明参数\n\ndf['net-worth'].agg(lambda x:x*2)#agg的参数可以是lambda函数\n```\n\n这里的一个例子：\n\n```python\ndf.groupby(by='birthday').agg(np.mean)#完全等价于data_group.mean()，都是求每组的平均值\n```\n\n当然，apply函数、transform函数和agg函数也大部分相同，但apply方法不能用key-value类型来特定的处理，transform方法只有一个参数function\n\n### 创建透视表\n\n可以通过pandas创建透视表\n\n```python\npd.pivot_table(df[['id','name','net-worth']],index=['id'],columns='name',fill_value=0)#不显示无index的值\n```\n\n得到\n\n|      | net-worth |      |        |      |\n| ---- | --------- | ---- | ------ | ---- |\n| name | hape      | john | johnny | max  |\n| id   |           |      |        |      |\n| 1    | 0         | 0    | 0      | 6355 |\n| 2    | 0         | 6567 | 0      | 0    |\n| 4    | 0         | 0    | 8491   | 0    |\n| 8    | 6872      | 0    | 0      | 0    |\n\n但这不是pandas的重点，略～","tags":["data science"]},{"title":"看了一点zookeeper的心得","url":"/2018/07/20/zookeeper整理/","content":"\n###Fast Paxos算法：\n\n这个还没看，先留个坑把。\n\n###文件结构：\n\nzookeeper的文件结构大概是这个样子的：\n\n!å¾ 1 Zookeeper æ°æ®ç\"æ](https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/image001.gif)\n\n### znode（data node）：\n\n1. 其中的每个子目录就是一个znode，他们也可以有子的znode（临时znode除外）。\n2. 这些znode必须是绝对路径，不允许相对路径。\n3. 每个znode都维护一个stat structure（linux系统文件的结构），其中包括版本号，acl改变等等。每次更新数据会让版本号自增。\n4. 每个znode可以设置一个watches，当watch触发后，zookeeper将发给client一个提醒。\n5. 在znode中数据读写是原子性的。每个znode都有一个ACL（access control list）来控制其读写权限。zookeeper不是用来做数据库的，其中存储的数据可能都是几kb的配置/状态信息。大块数据都存在hdfs中。\n6. Ephemeral node就是临时节点，每次会话结束这些节点都会清空，不允许有子节点。\n\n###Zookeeper Session：\n\n![state_dia](/img/state_dia.jpg)\n\n###一致性的保证：\n\nzookeeper是一个非常高效、可扩展的服务。其一致性靠以下几点保证：\n\n1. 顺序的一致性。一个client的更新会依序发送给其他。\n2. 原子性。更新是原子操作，只有成功和失败，没有中间过程。\n3. 服务器会看到完全相同的服务，不论其连接了哪个服务器。\n4. 可靠性。一旦一个更新被部署，他会一直存在，直到被另一个更新覆盖。\n5. 及时性。client对系统的观察在一个时间段内将是最新的。也即系统的改变在这个时间段内client是看见的，或者可以探测到它失败。","tags":["big data"]},{"title":"numpy使用整理","url":"/2018/07/18/numpy使用整理/","content":"\n最近看了一些numpy的基础，在这里整理一下。\n\n### 创建：\n\n```python\nimport numpy as np\n\n#数组\narray1 = np.array([[1,2,3],[4,5,6]])\nnp.eye(3)#单位多维数组\nnp.diag([1,2,3,4])#对角多维数组\nnp.arange(1,4)#[1,2,3]数组\n\n#矩阵\nnp.mat(\"1 2 3;4 5 6;7 8 9\")#矩阵运算与多维数组运算结果不同，所以要用mat建矩阵，用分号隔开数据\nmatrix = np.mat(array1)#可以用多维数组初始化矩阵\nmatrix1 = np.bmat(\"array1 array2;array1 array2\")#创建分块矩阵\n```\n\n### 随机数：\n\n```python\nnp.random.random(100)#完全随机\nnp.random.rand(5,5)#5*5均匀分布\nnp.random.randn(5,5)#5*5正态分布\nnp.random.randint(2,50,size=(2,3),dtype='l')#大于等于2小于50的2*3的int64型整数\n```\n\n### 变换数组形态：\n\n```python\narr.reshape(3,3)#转变成3*3的数组，但要求原数组必须9个元素，否则不能reshape\nnp.hstack((arr1,arr2))#横向组合\nnp.vsplit(arr4,3)#横向切割，即把横向由1列的变成3列（相当于横着切）\n```\n\n### 文件存储与读取\n\n```python\n#二进制存储与读取\n#存储\nfile = \"./temp/save_arr.npy\"\nfilez = \"./temp/save_arr.npz\"\nnp.save(file,arr)#用save存，文件扩展名.npy，只能存一个数组\nnp.savez(filez,arr1[,arr2...])#用savez存，文件扩展名.npz，可以存多个数组。注意：不按照要求扩展名，则系统自己添加对应扩展名；二进制存储的数组打开文件看不到真实数据\n#读取\nloaded_data = np.load(file)#存储可以省略扩展名，读取一定不可以\nloaded_dataz = np.load(filez)\nloaded_dataz[\"arr_0\"]#对于多个文件读取，这种方式可以得到单独数组\n```\n\n```python\t\n#文件存储与读取\n#存储\nnp.savetxt(fname,x,fmt='%d',delimiter=',',newline='\\n',header='',footer='',comments='# ')#x为要存的数组，fmt='%d'表示整数方式存，delimiter表示存储时的分隔符，存储和读取时默认为空格\n#读取\nloaded_data = np.loadtxt(fname,delimiter=\",\")#一定也要带上啊delimiter且与文件中的分隔符一致\n```\n\n### 利用numpy做简单的统计分析\n\n```python\nnp.random.seed(10)#种子是伪随机数的开头，相同种子对应随机数都相同，一般种子会设为当前时间，确保得到真随机数（numpy默认也是这样）\narr = np.random.randint(1,10,size=10)\n#排序\narr.sort()#这个会直接将arr排序\narr.sort(axis=0)#二维数组的sort参数axis可以为0、1，分别对应数组纵向和横向的排序\n#去重\nnp.unique(arr)\n#重复\nnp.tile(arr,3)#arr是重复哪个，3是重复次数\nnp.repeat(arr,3,axis=0)#axis是重复的方向（tile没有这个参数）\n#注意：tile是对数组进行重复，repeat则是对每一个数组的每一个元素进行重复，打破了原来的数组。\n#常用统计函数\nnp.sum(arr)#求和\narr.sum(axis=0)#沿纵轴求和\nnp.mean(arr)#计算数组均值\narr.mean(axis = 0)#沿着纵轴计算数组均值\nnp.std(arr)#计算标准差\nnp.var(arr)#计算方差\nnp.min(arr)#计算最小值\nnp.max(arr)#计算最大值\n```\n\n\n\n","tags":["data science"]},{"title":"最近在看的东西","url":"/2018/07/16/20180716日记/","content":"\n最近都没有写博客，想更一篇了。\n\n前几天在刷算法，这几天看了一些springboot的东西。之前对spring了解的也比较多，而且这次看的也比较浅，就这样吧。\n\n感觉很慌。马上要找工作了，我却还没开始工作...\n\nspring-boot用的时候再看吧，\n\n算法还是要接着刷，\n\n接下来就做我的python了！\n","tags":["life"]},{"title":"anaconda基础整理","url":"/2018/07/04/anaconda基础整理/","content":"\n这是一个python环境、包管理工具，这玩意很厉害。\n\n### 插一个其他东西：\n\n在搞conda环境变量的时候在.zshrc里没有注意语句的顺序，变量使用在前，声明在后，导致path里没有这个。。。。。。以后要注意了！\n\n###使用原因：\n\n1. 和以前用的virtualenv有点像，可以创建一个独立的python环境，python版本，包都是独立于外部的。\n2. 自带很多数据科学的包，省的下。\n3. 可以将环境与远程同步，也可以clone别人的环境，开发效率高。\n4. 可以与pycharm等工具结合，通用性强。\n5. Anaconda navigator是一个桌面应用，使用非常简单。\n\n###常用到的操作：\n\n1. 在命令行可以用conda来操作一些东西：\n\n```bash\nconda create -n <env-name> <package-name>#创建conda环境\n\nconda remove -n <env-name>#删除conda环境\n\nconda env list#查看所有环境，其中带*的为当前环境，在当前环境下，用的python版本、包等都是anaconda的，而不是本机环境\n\nsource activate <env-name>#激活某个环境，之后zsh前面会加上这个环境的名称\n\nsource deactivate#退出某个环境\n\nconda install <package-name>[=versionInfo]#在当前环境下安装包，可以选定版本\n\nconda install -n <env-name> <package-name>#在特定环境中\n\nconda list#列出当前环境所有的包\n\nconda search <package-name>#查找某个包（模糊匹配）\n```\n\n###conda和pycharm的结合：\n\npycharm可以直接用conda的environment来做，只要在选择interpreter的时候选conda环境对应的那个即可。","tags":["data science"]},{"title":"mac&linux命令整理","url":"/2018/07/04/mac&linux命令整理/","content":"今天看了一些oh-my-zsh的东西，感觉还是要整理在博客中，不然太容易忘掉了。\n\n### 命令：\n\n以后一些好用但是不熟悉的命令就都放在这里了，方便回忆：\n\n**jq** ： 命令行处理json的命令，，支持管道\n\n用法：\n\n针对一个json，直接 \n\n```shell\nhead -n 1 xxx.json | jq '.'\n```\n\n就可以获得format之后的形式，如果想获得json某个key，只要\n\n```shell\nhead -n 1 xxx.json | jq '.key'\n```\n\n即可获得\n\n**wc** ： 用于统计指定文件的字节数、字数、行数\n\n**df** ： 查看磁盘大小和占用情况\n\n**du**：查看磁盘大小，用du -sh可以查看当前文件大小，用du -lh --max-depth=1查看当前和下一层文件大小。l表示硬连接，h表示用人类可以看懂的方式\n\n**ranger** ：文件查看和管理工具，界面舒服\n\n**nohup &**：这个以前经常用，最近不用差点忘了，在命令钱加nohup，后面加&，才能让命令后台运行，而且和终端没关系\n\n### Docker:\n\n我有一句mmp一定要说出来！\n\nmac下由于docker的实现原因，所以宿主机不会有docker0网卡，更无法ping通container的，即使我用了bridge模式。当然container之间是可以互相ping通的。。。\n\n### 插件：\n\nzsh自带很多插件，可以在.zshrc的plugin里写入，就可以用这些插件了，我用的插件包括：\n\n- z。可以直接跳转。它记录（统计）了一些常用的跳转，只要z+destination就可以\n- extract。可以直接解压，忽略tar后各种参数。与unzip类似。\n- zsh-autosuggestions。这个神器，之前输入的命令可以再提示出来，很方便用。\n- Web-search。可以在命令行直接用 google+要查的内容 即可打开搜索页面。\n\n### 命令行快捷键：\n\n- ctrl+q。可以直接删除整行命令。\n- ctrl+w。可以删除每一分段的命令。\n- ctrl+e。直接跳到命令最后。\n- ctrl+a。直接跳到命令最前面。\n- command+d 在iterm中分屏\n- comand+[ or ] 在iterm的分屏中切换\n\n### 文件：\n\n**/etc/motd** : 改命令行打开的提示语\n\n### 遇到的问题：\n\nssh-key生成忘记有什么问题了。。。整理不及时呐～\n\nscala2.11和autosuggestions配色问题有冲突，导致每次scala都会报错\n\n### Dockerfile的基础应用\n\nDockerfile里有个from，就是指从哪个images拿过来的，可以是本地的，所以增量修改images就是新建一个dockerfile然后from原来的images。再加上自己的RUN，最后执行docker build .就可以～","tags":["mac"]},{"title":"Cryptocurrency Technologies","url":"/2018/04/30/Cryptocurrency Technologies/","content":"\n# Cryptocurrency Technologies\n\nCoursera course from Princeton University, here are some information and notes from the course.\n\n## Cryptographic Hash Functions\n\n### basic term:\n\n- takes any string of any size as input\n- fixed-size output (We'll use 256 bits )\n- efficiently computable\n\n### Security properties:\n\n- collision-free\n- hiding\n- puzzle-friendly \n\n### collision-free\n#### Definition\n\nNobody can find x and y such that:\n>  **x!=y and H(x) = H(y)**  \n\nSo that is what we call collision-free.\n\nActually **collision do exist. But it merely be found** ——that is guaranteed the work.\n\n#### Application\nIf we know H(x) = H(y), it's safe to assume that x = y. Hash function provide us with a efficient way to recognize the same things. The hash is small, which has only 256 bits, while the whole things might be really big.\n\n### Hiding\n#### Definition\nWe want something like this:\n> Given H(x), it is infeasible to find x.\n\nhigh min-entropy : the distribution is \"very spread out\". If r is chosen from a probability distribution that has high min-entropy, then given H(r | x), it is infeasible to find x.\n\n#### Application : Commitment\n\nCommit to a value, reveal it later.\n> **com = H(key | msg)**\n##### Hidding:\nGiven H(key | msg), infeasible to find msg.\n##### Binding:\nInfeasible to find msg != msg' such that H(key | msg) == H(key | msg')\n\n### Puzzle-friendly\n\nfor every possible output value y, if k is chosen from a distribution with high min-entropy, then it is infeasible to find x such that H(k | x) = y.\n\n#### Application: Search puzzle\n\n1. given a \"puzzle ID\" id (from high min-entropy distrib), and a target set Y;\n2. try to find a \"solution\" x such that H(id | x) = y.\n\n\n\n## Add:\n\ncommon hash function in shell\n\n```shell\n#md5\nmd5 [-pqrtx] [-s string] [files ...]\n#more ways\necho \"max\"|md5\nmd5 <<< \"max\"\n\n#sha-1 && sha-256\nshasum -a 1 -t test\nshasum -a 256 <<< \"max\"\n\n#encode and decode function -> base64\nbase64 -D <<< \"max\" #decode\nbase64 <<< \"max\" #encode\n```\n\n\n\nbitcoin use **sha256** as its hash function\n\n\n\n## Topic from Prof.Yuen \n\n### some keywords\n\nZero-knoledge proff approach\n\nLinkable ring signature\n\nLighting network\n\nIOTA\n\nByteball\n\nHdera/Hashgraph\n\nmonero's limitation\n\nRingCT 2.0\n\nBulletRingCT","tags":["block chain"]}]
[{"title":"图说mysql的四种join","url":"/2018/09/14/图说mysql的四种join/","content":"\n![屏幕快照 2018-09-14 上午10.54.59](/img/rightJoin.png)\n\n![屏幕快照 2018-09-14 上午10.55.09](/img/leftJoin.png)\n\n![屏幕快照 2018-09-14 上午10.55.39](/img/innerJoin.png)\n\n![屏幕快照 2018-09-14 上午10.55.46](/img/crossJoin.png)\n\n","tags":["database"]},{"title":"MapReduce终极整理","url":"/2018/09/13/MapReduce终极整理/","content":"\n这个礼拜的课都在讲mapreduce，自己也看了google那篇文章，感觉理解了很多，趁着刚学的知识正热乎着，赶紧在这里总结一下。\n\n首先，关于HDFS的内容在另一篇博客中有了比较详细的介绍，这里直接谈MapReduce。\n\n### 使用MapReduce的目的\n\n用一个工具只有知道他的目的和优点，才能找到最合适的使用场景。简单来说，mapreduce提供了一个自动并行和分布式计算的工具（接口），在大规模集群中性能出众。由于它隐藏了这些分布式系统的细节，所以很多不懂分布式的程序员也可以基于此搭建分布式系统。对于一些web服务器、爬虫、排序算法、机器学习、数据挖掘等都很有用。\n\n### MapReduce的架构和运行\n\n这是本文的重点，不多说，直接上图。\n\n![mapreduce](/img/mapreduce.png)\n\n这幅图是直接从论文截下来的，生动的讲述了mapreduce的整个流程：\n\n1. 将用户input文件主动分割成成16-64MB大小的块，然后在集群中做一些备份（一般备份3份，其中两份在同一个机架上）\n2. master发挥作用，将选择一些idle的worker给其分配map或者reduce任务。\n3. 这时候worker就会去读那些被分割的文件，将一个key/value对读取到用户定义的map函数中。worker在读取这些文件时，会优先读取本地存着的，如果本地没有，选取离它最近的文件，减少网络传输代价。\n4. map函数会产生一些中间过程数据，然后周期性的通过buffer将它存在本地磁盘上，然后将地址、文件名等信息通知master，这些中间文件通过hash(key) mod R的方式被分成R份（R是reduce worker的数量），master就会将这些块分配给对应的reducer。\n5. reducer会通过RPC方式读取到这些中间过程数据，然后进行一个排序（shuffle），这个shuffle会让相同value聚在一起。这个很必要，因为不同key会映射到同一个reduce任务上。\n6. 只有在map全部结束后，reduce才会开始。在reducer中处理这个有序数据时，遇到相同key就会把value放在用户定义的reduce函数中。最终reduce函数会把结果文件输出到这个reduce partition中。\n\n最终得到的结果其实就是part-r-00000这样形式的不同文件。用户没必要把output文件最后聚在一起，如果需要的话，这个结果还可以作为下一轮mapreduce的输入。\n\n### 在有Combiner后过程的改良\n\n上面的过程是论文中解释的，但现在的程序都用了Combiner，Combiner其实和reduce的代码是一样的，那他是做什么用的呢？\n\nCombiner其实是针对本地的map后的结果进行pre-reduce（或者叫mini-reduce），例如wordcount在map之后大概是('a':1),('b',1),('a',1),('c',1)这样，combiner将local的这些先做一次reduce，变成('a',2),('b',1),('c',1)，之后再去shuffle和reduce。\n\n综上mapreduce整个过程分为四步：\n\n​\t\t\t\t  \t\t\t**Map -> Combiner -> Shuffle -> Reduce**\n\n### Master节点的任务和结构\n\n对于每个map任务或者reduce任务，都分成三种状态：idel, in-progress, completed。每个任务的这些状态都存在master节点上。\n\nMaster节点是作为map任务和reduce任务通信的管道的。master要存储并更新M个map任务产生的M * R个块(每个map任务产生R个块)的位置信息、文件名等。即，master承担 O(M+R)个scheduling决策，存储 O(M * R)个状态信息。\n\nM：map是的m块数据，R: reduce时的r块数据，W: worker机器的数量；三者关系为：\n\n​\t\t\t\t\t\t\t**M > R > W**\n\n### Worker的容错问题\n\n首先，master出错的话，没啥好说的，直接告诉用户就成。\n\nworker一旦出错，master应该要感知到，有两种策略感知：\n\n- Pull模型 ：worker通过发送heartbeat给master，master在感知到heartbeat之后在heartbeat response里给worker分配任务。\n\n- Push模型：master一直ping worker，当一段时间ping不通后说明worker失败了。\n\n这个时候master会将worker的任务分配给其他的worker去执行。对于执行完的worker，状态会重新标定为idle，表示有资格接受任务。失败的worker也会被标定为idle，同样可以接受任务。\n\n对于在失败的worker上completed的map任务，在其他worker上需要重新执行，因为他们存在本地的中间数据访问不到了。但对于在失败的worker上completed的reduce任务则不需要重新执行，因为他们的结果文件存在了global的文件系统下。\n\n在执行mapreduce时对于一些**straggler**（落伍者），有两种处理方式：\n\n- Job stealing: 对这个job进行分片，将没完成的部分交给其他的worker完成。\n- Speculative execution: master，这时候两个类似竞争的关系，当其中一个结束时，这个task会被标定为completed，同时另一个task将会被放弃。\n\n\n\n","tags":["big data"]},{"title":"课程信息整理","url":"/2018/09/13/课程信息整理/","content":"\n之前好多课上记的乱七八糟的东西都丢在了博客上，显得很乱，以后笔记可能还是写在ppt上多一些，关于课程的信息和一些总结会丢在博客上，保证博客的质量。\n\n---\n\n### [CMSC5741 - Big Data Technology and Applications]()\n\ntextbook: Mining of Massive Dataset（已借）\n\nInstructor: [Prof. Michael R. Lyu](http://www.cse.cuhk.edu.hk/lyu/)\n\nTutor: Zeng Jichuan\n\nexam: **Nov.6 Midterm exam** \n\nAssessment Scheme and Deadlines:\n\n- 20% Assignment\n- 40% Midterm examination\n- 40% Project : Proposal, Presentation, Report\n\nBackgroud Knowledge: Tensorflow, Amazon EC2\n\n---\n\n### [CSCI5570 - Large Scale Data Processing Systems](http://www.cse.cuhk.edu.hk/~jcheng/5570/)\n\nwebsite Account:\n\n- Username : csci5570\n\n- Password : huskydatalab\n\nInstructor: [Prof. James CHENG](http://www.cse.cuhk.edu.hk/~jcheng) \n\nTutor: Tatiana Jin\n\nLecture/Lab: \n\n- Tuesday 13:30 Lecture && Lab\n- Wednesday 14:30 Lecture\n\nAssessment Criteria:\n\n- 30% Survey paper : select one topics (DDL: Dec 10, 2018)\n- 70% project : deadline: DEC 20\n\n---\n\n### [CMSC5724Data Mining and Knowledge Discovery](http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/18-fall/)\n\nInstructor: [Yufei Tao](http://www.cse.cuhk.edu.hk/~taoyf/)\n\nTutor: Shangqi Lu\n\nAssessment Criteria:\n\n- 30% Project\n- 30% Short Tests (three times in class)\n- 40% Final (Open-book)\n\n---\n\n### CMSC 5720 - Project I\n\nInstructor: [Prof. James CHENG](http://www.cse.cuhk.edu.hk/~jcheng) \n\nOptions:\n\n1. NN-descent （kn-graph的近似算法）\n2. search with fa2ss（facebook的相似性检索库）\n3. multiprobe with tree（基于树的哈希方法）\n4. LSH for MZPS （lsh）\n5. 数据收集->存储->分析 系统\n6. topic modeling on ps archetective -> （LDA FlexPS->parameter server拓展）\n7. 调度算法，同步/异步 任务，在不同集群下测试算法，分布式，任务的表现\n8. 矩阵分解 Distributed MF（矩阵分解） on Actor Framework （nomad,lftf acf或者akka -> cpu to gpu to scheduling）\n9. Clustering-aware query (database query optimizer)\n\n","tags":["class note"]},{"title":"地道的口语表达积累","url":"/2018/09/01/地道的口语表达/","content":"\n### 1. 给别人加油\n\n不要再用fighting啦～\n\n**Go get‘em.**\n\n完整说法：Go get'em tiger.\n\n追女生：go get her\n\n\n\n### 2. 问别人想不想要\n\n不要再用do you want啦～\n\n**be up for something**\n\ne.g: \n\nAre you up for going clubing?\n\n你想去蹦迪吗？\n\nbe up三个意思：\n\n- 醒着 Are you up?\n\n- 下一个 who's up next?\n\n- 怎么了 what's up？\n\n\n\n### 3. 和朋友约出去玩\n\nplay with不行，一般是小朋友或者人和宠物，不适合成人一起玩\n\nhang out\n\n还可以用chill out，自己一个人出去玩\n\ne.g.\n\n Do you wanna hang out with us?\n\n\n\n###4. 如何约一场“夜生活”\n\n**night out～(名词)**\n\ne.g. \n\nIt's been a while since we had a girls' night out.\n\n姐妹们我们已经很久没有出去玩啦\n\nhave(need) a night out.\n\ne.g.\n\nIf you are up for a boys' night out~\n\n如果想约不会太晚的夜生活用：evening out\n\nNight owl 夜猫子"},{"title":"daily life in HK 2","url":"/2018/08/30/daily life in HK 2/","content":"\nSigh~When you have to try your best to save money in all aspects of your life, it is obvious that you can't enjoy your life. While, I come here not for enjoy the postgraduate career but for open my mind and learn something useful.\n\nAnother feeling deeps me most is if I can't speak english well, the embarrassment will rise upon my face. In the afternoon, I met with my project teacher about the future work. From what he said, I could speculate that the former students may make him disappointed. As for project, he gave us much freedom to do what we interested in based on our foundation.\n\n\n\nAt night, I hung out with Doc.Lin, and it is my first time to meet with old friends after I came to HK. We walked around 中环 and 香港岛, where we smell at the taste of dollars. What a fame of capitalism! Then we returned to 九龙 by ship and ate dinner there. Life of Doc.Lin seemed as relaxed and funny as the time in SAP. It's admired by everyone, right?","tags":["life"]},{"title":"daily life in HK  1","url":"/2018/08/27/daily life in HK  1/","content":"\nFinally, I arrive at CUHK and now I sit in SINO building writing this words. After that, I will upgrade my daily life in CUHK in my blog unregularly.\n\n\n\nThe journey from SZ to HK is tremendously hard for my. I took all my packages (more than 30kg, wooo~~~) comming in HK custom. At that time, I thought the most difficulty time had been gone though. I was wrong, cause it was just the start of my suffering. The long lines of bus station made me crying, at the same time, I realized that I forgot to take some change. Of course, as a freshman of HK, I have no HK card. So I had to beg all around. After two times of tranfer, I got to Sha Tin station.\n\n\n\nFrom the introduction of my room, I knew the building I live is not far from the station. While, I made the mistake for twice. Unfortunately, It lies on a unknown mountain, so I was supposed to climb mountain with huge bags.\n\n\n\nFinally, I got in my rooms and met with my new roommate, who is a programmer, too. He is a nice guy and I was looking forward to make more friends here.\n\n\n\nWhat was worth to mention is we have a free dinner organized by our college, other guys in my table feel embarrassed to pack the left-overs so I took it all back. It solved my two-day meal problems.\n\n\n\nNever be shamed with yourself, keep on  doing what you believe in and you will make it sooner or later!","tags":["life"]},{"title":"20180811日记","url":"/2018/08/11/title 20180811日记/","content":"\n估计要暂别电脑一段时间，转型去线下学数学了～刚买了统计学习方法，之前数学底子太差了，要看点基础的东西，感触什么的就写在纸质笔记本里了，等看差不多了就来这篇日记打卡，把下面的flag挨个叉掉！\n\nFlag～ X\n\n\n\nFlag~\n\n\n\nFlag~\n\n\n\nFlag~","tags":["life"]},{"title":"mac在nginx下部署php遇到的坑","url":"/2018/08/09/mac在nginx下部署php遇到的坑/","content":"\n受人之托，帮人部署一个网站，然后我想在本地的nginx里先调试一下。一开始，打开页面显示403，这个之前见过，nginx的权限问题，改了这个权限之后，发现访问php页面都是直接下载而没有解析，我想起来电脑可能没有php环境，就下了php，然后还是同样的问题。总之因为对php不太熟悉（之前都用xampp这类软件），所以花了一点时间才搞定。\n\n首先要明白的是，nginx本身不能处理php，它只是一个web服务器，当前端请求php时，nginx需要把界面发给php解释器处理，然后把结果返回给前端。一般地，nginx是把请求发给fastcgi管理进程处理。如nginx中配置：\n\n```xml\n        location ~ \\.php$ {\n            root           html;\n            fastcgi_pass   127.0.0.1:9000;\n            fastcgi_index  index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;#这里原来不是$document_root，搞得我很蒙，还好网上查到改好了，不然会报file not found\n            include        fastcgi_params;\n        }\n```\n\n所以要启动一个fastcgi，这里就用到了php-fpm，它是一个php fastcgi管理器，只用于php语言（旧版php的要单独下php-fpm，我用的php-fpm已经集成了这个）。\n\n这里有很多奇怪的问题。\n\n**第一次运行php-fpm**\n\nfailed: 找不到/private/etc/php-fpm.conf文件，\n\nSolution:但这个目录下有个php-fpm.conf.default的文件，所以cp了正确名字的新文件\n\n**第二次远行php-fpm**\n\nFailed: 找不到/usr/var/log/php-fpm.log \n\nSolution：根本没有这个目录，到conf文件里改了但是没有效果，没办法我就通过下面的命令执行php-fpm(后面都用这个命令执行)\n\n```shell\nphp-fpm --fpm-config /private/etc/php-fpm.conf  --prefix /usr/local/var\n```\n\n**第三次运行php-fpm**\n\nFailed: No pool defined. at least one pool section must be specified in config file\n\nSolution：到/etc/php-fpm.d/ 目录下有文件www\\.conf.default，cp一份名为www.conf的文件\n\n**第四次运行php-fpm**\n\nFailed：端口被占用\n\nSolution：杀掉这个进程\n\n```shell\nsudo lsof -i tcp:9000#找到占用9000端口的进程号\nkill -9 port#杀！\n```\n\n**第五次运行php-fpm**\n\n成功！\n\n\n\n##补充：\n\n在nginx上配的时候又有所一点不同，在mac上php-fpm直接listen了9000端口，但在服务器上它listen了php7.0-fpm.sock但socket文件，这种方式可能快一点，所以要在nginx上php的配置那边将\n\n```shell\nfastcgi_pass 127.0.0.1:9000;\n```\n\n改成：\n\n```shell\nfastcgi_pass unix:/run/php/php7.0-fpm.sock;\n```\n\n才能成功运行php\n\n### 继续补充\n\n很有意思的一个东西，要上传27m的一个视频，nginx直接报了413 Request Entity Too Large，是我没设置...\n\n到nginx的配置（set-enabled/default）里面添加\n\n```shell\nserver {\n    ...\n    client_max_body_size 80m;\n    ...\n}\n```\n\n重读配置、重启服务器\n\n```shell\nnginx -s reload\nservice nginx restart\n```\n\n然后还要去修改php.ini，在其中修改两条配置\n\n```shell\nupload_max_filesize = 80M\npost_max_size = 80M\n```\n\n然后关掉php-fpm的进程，再重启即可～\n\nps：贺老师真的完全不研究的...mp4传不上去只是在系统里没添加这种类型，这种事都要我自己去找...难受 :(\n\n**note：**在ubuntu下现在比较推荐用apt而不是apt-get...so，是时候改变了！","tags":["mac"]},{"title":"看Husky的一点整理","url":"/2018/08/08/Husky文档整理/","content":"\nUsername : csci5570\n\nPassword : huskydatalab\n\nhusky是一个通用的分布式的计算平台，就像mapreduce、spark这种，它是用c++写的（难受...）\n\n## 配置\n\n```shell\n# Required\nmaster_host=master#master跑的地方\nmaster_port=10086#master绑定的端口\ncomm_port=12306#worker绑定的端口\n\n# Worker information\n[worker]\ninfo=worker1:4#worker1有4个线程\ninfo=worker2:4#worker2有4个线程\n\n#如果用了hdfs，配置hdfs路径\nhdfs_namenode=master\nhdfs_namenode_port=9000\n```\n\n运行的时候用\n\n```shell\n./program --conf=/path/to/config.ini\n```\n\n写入配置。\n\n## 组件\n\n### Object List\n\nObject List（objList）是husky中最主要的对象，可以把任何对象都存在objlist中，两个objlist通过channel传递消息。\n\n```c++\nclass Obj {\n   public:\n    using KeyT = int;\n    KeyT key;\n    const KeyT& id() const { return key; }\n    Obj() = default;\n    explicit Obj(const KeyT& k) : key(k) {}\n};\n```\n\n创建一个obj只要3步：\n\n1. 定义一个key的类型（keyT），一般都用int\n2. 写一个id（）函数来返回该对象对应的key\n3. 需要一个默认构造函数，还需要一个能够接收key参数的构造函数\n\n接下来就可以创建、使用、删除Object List\n\n```c++\n//创建名叫my_objlist的objlist\nauto& objlist = ObjListStore::create_objlist<Obj>(\"my_objlist\");\n\n//将obj传入创建好的objlist中\nObj obj(3);\nobjlist.add_object(obj);\n\n//通过名字拿到对应的objlist，注意这里的auto关键字，自动判定类型，很舒服！\nauto& objlist2 = ObjListStore::get_objlist<Obj>(\"my_objlist\");  \n\n//通过名字删除objlist\nObjListStore::drop_objlist(\"my_objlist\");\n```\n\n为了让添加在objlist中的obj被其他线程感知并利用，在多线程情况下需要将objlist全局化一下，husky已经封装好该方法\n\n```c++\nglobalize(objlist);\n```\n\n接下来就是**最重要**的一个函数list_execute（），它规定了list里的每个object需要做的事，这个函数是用户自己定义的。它有两个参数：\n\n-  第一个是要操作的objlist\n- 第二个是这个objlist中每个obj要做的事，例如下面函数就是obj在log中打印id，包括之后用channel发送或接收消息也都是在这个函数里\n\n```c++\nlist_execute(objlist, [](Obj& obj) {\n    base::log_msg(\"My id is: \" + obj.id());\n});\n```\n\n### Channel\n\nchannel就是object和object互相通信的工具，他们的关系类似于城市和公路。husky中有四种channel：\n\n- Push Channel：最常见的点对点通信\n- Push Combined Channel：在push channel基础上增加了合并发给同个obj的\n- Broadcast Channel：将一个key-value广播出去，任何地方都可以通过key拿到值\n- Migrate Channel：用来migrate对象，将一个对象发送到另一个线程上\n\nchannel的创建、使用和drop（一定要主动销毁）\n\n```c++\n// create PushChannel\ntemplate <typename MsgT, typename DstObjT> \nstatic PushChannel<MsgT, DstObjT>& \ncreate_push_channel(ChannelSource& src_list,\n                    ObjList<DstObjT>& dst_list,\n                    const std::string& name = \"\");\n\n// Get PushChannel through name\ntemplate <typename MsgT, typename DstObjT>\nstatic PushChannel<MsgT, DstObjT>& \nget_push_channel(const std::string& name = \"\");\n\n// Drop channel through name\nstatic void drop_channel(const std::string& name);\n```\n\n下面通过例子来说明：\n\n首先，要想创建channel，就要确定发消息的源objlist和目的objlist，当然，参数里的目的objlist必须是全局化的\n\n```c++\n//创建一个push_channel\nauto& ch = ChannelStore::create_push_channel<int>(src_list, dst_list);\n```\n\n一般来说，channel是放在list_execute（）函数里用的，要想清楚从哪个obj发，发什么，哪个obj接收（通过key来标注）\n\n```c++\n//push channel\n//发送端代码\nlist_execute(src_list, [&ch](Obj& obj) {\n    ch.push(msg, key);  // send msg to key\n});\n\n//接收端代码\nlist_execute(dst_list, [&ch](Obj& obj) {\n    auto& msgs = ch.get(obj); // The msgs is of type std::vector<MsgT>, MsgT is int in this case\n});\n\n//broadcast channel\nauto& ch4 = ChannelStore::create_broadcast_channel<int, std::string>(src_list);\nlist_execute(src_list, [&ch4](Obj& obj) {\n    ch4.broadcast(key, value);  // broadcast key, value pair\n});\nlist_execute(src_list, [&ch4](Obj& obj) {\n    auto msg = ch4.get(key);   // get the broadcasted value through key.\n});\n```\n\n\n\n###Aggregator\n\n用来执行一些聚合操作的类，可以用来做求前k大值，统计数量，计算机器学习梯度总数等。他的构造函数需要两个参数（或以上），一个是init值，另外是lambda函数\n\n```c++\nAggregator<int> agg(0, [](int& a, const int& b){ a += b; });\n```\n\n这个lambda函数就是aggregate的规则。\n\n在创建完agregator后，就要使用它了。可以用update函数或者update_any函数（比update可接受参数类型多）来进行aggregator，例如\n\n```c++\nagg.update(1);//aggregator值加1\n```\n\n在聚合完之后，更新的值其实只在本地，为了让这个值在全局响应要用HuskyAggregatorFactory::sync()函数。另一种方式是通过HuskyAggregatorFactory::get_channel()来拿到通道，然后在list_execute中通过这个channel把消息传播出去，这种方法最后也会去调用sync()函数\n\n```c++\n//两种方式\nAggregatorFactory::sync();\n\n// or using aggregator channel\nauto& ac = AggregatorFactory::get_channel();\nlist_execute(obj_list, {}, {&ac}, [&](OBJ& obj) { \n  ...  // here we can give updates to some aggregators\n});\n```\n\n当全局划这个聚合之后，就可以用get_value()函数得到值了\n\n```c++\nint sum = agg.get_value()\n```\n\n这个值是被全局共享的，所以对他的修改会影响其他executor，并可能有线程安全问题","tags":["CUHK"]},{"title":"Hadoop了解一下","url":"/2018/08/05/hadoop了解一下/","content":"\n搞分布式数据分析系统，hadoop绝对是不可绕过的一关，所以简单玩了一下，以下是总结。\n\n### map reduce\n\n- automatic parallezation\n- Fault tolerance\n- a clean abstraction for programmers\n\nBSP model : Bulk sychronous parallel\n\nidentity reducer？re\n\n## 基本概念\n\n###Apache Hadoop与HDFS\n\nHadoop是一个大的生态系统，最主要是hdfs和一个基于mapreduce的分布式计算引擎。hdfs就是一个文件系统，在mapreduce的时候（包括用spark的时候）都需要对应文件在hdfs里。\n\n**block块：**HDFS在物理上是以block存储的，block大小可以通过配置参数（dfs.blocksize）来规定，默认128M，可以减少寻址开销。大文件会被切分成很多block来存，而小文件存储则不会占用整个块的空间。\n\n**NameNode：**是master。负责管理文件系统的namespace（可以理解是指向具体数据的文件名、路径名这种）和客户端对文件的访问。data path？\n\n**(非Yarn)JobTracker：**在NameNode上，协调在集群运行的所有作业，分配要在tasktracker上运行的map和reduce任务。\n\n**DataNode：**是slave。datanode则负责数据的存储。\n\n**(非Yarn)TaskTracker：**在datanode上，运行分配的任务并定期向jobtracker报告进度。\n\n**流式访问：**指hdfs访问时像流水一样一点一点过。这样也决定了hdfs是一次写入、多次读取的特性，同时只能有一个wirhter。这样访问方式适合做数据分析，而不是网盘这种。\n\n**rack-aware（机架感知）：**这是hdfs的复制策略。hdfs为了数据可靠一般会将数据复制几份（默认三份）。同一个机架的机器传输速度快，不需要通过交换机。为了提高效率，一台机器的数据会把一个备份放在同一机架（相同rack id）的机器里，另一个备份放在其他机架的机器上。机架的错误率很小，所以不影响可靠性。\n\n**hdfs的特点：**\n\n- 面对构成系统的组件数目很大，所以对硬件的快速检测错误并自动回复非常重要\n- hdfs需要流式访问他们的数据集\n-  运行的数据集非常大，一个典型文件大小一般在几G到几T\n- 文件访问模型是“一次写入、多次访问”\n- 将计算移动到数据附近闭将数据移动到计算更好\n\n###Yarn：\n\nyarn其实是解决了经典mapreduce中一些问题（例如：jobtracker太累导致的可扩展性问题）的新一代hadoop计算平台。\n\n**ResourceManager：**代替jobtracker，以后台进程的形式运行。追踪有哪些可用的活动节点和资源，指出哪些程序应该何时或者这些资源。\n\n**ApplicationMaster：**代替一个专用而短暂的JobTracker。用户提交一个应用程序时，会启动applicationmaster这个轻量级进程实例来协调程序内任务（监视进度、定时向resourcemanager发送心跳数据、负责容错等），计算数据需要的资源并向resourcemanager申请。它本身也是在一个container里运行的，且可能与它管理的任务运行在同一节点上。\n\n**Container：**是yarn中资源的抽象，封装了某个节点上一定量的资源（如cpu和内存等资源）。它的分配是由applicationmaster向resourcemanager申请的；而它的运行则是applicationmaster向资源所在的nodemanager发起的。\n\n**NodeManager：**代替tasktracker。拥有很多动态创建的资源Container。容器大小取决于它所包含资源量，而一个节点上的容器数量由配置参数和除用于后台进程和操作系统以外资源总量决定。\n\n## 基本架构\n\nHdfs采用了master/slave架构。一个hdfs集群由一个namenode和一群datanodes组成。简单来说，就是hdfs通过namenode暴露出了文件系统命名空间的操作，包括打卡、关闭、重命名文件等等。在这个文件系统对一块数据的操作会映射到具体的datanodes上。\n\n所以一般是一台机器上搭namenode，然后datanode在其他各个机器上。\n\n![hdfsarchitecture](/img/hdfsarchitecture.jpg)\n\n### Hadoop Streaming\n\njava这么规范的东西有人就是不喜欢，非得用python啥的写hadoop，所以就有了hadoop streaming，支持其他语言的hadoop操作。\n\n##基本操作\n\n###单节点测试\n\n比较无聊，只是测一下能不能跑，不需要运行什么\n\n```shell\n$ cd Cellar/hadoop/3.1.0/libexec\n$ mkdir input#不能是别的名字\n$ cp etc/hadoop/*.xml input\n$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar  grep input output 'dfs[a-z.]+'\n$ cat output/*\n```\n\n###伪分布式测试（pseudo-distributed）\n\n\n\n1. 保证本机已经装好hadoop，java1.8（java9有些函数被废了会报错）\n\n2. 配置本机ssh，确保\n\n   ```shell\n   ssh localhost#mac默认不允许，需要手动去打开\n   ```\n\n   可以运行。注意：mac默认不允许任何机器远程登录，需要到  系统偏好设置 -> 共享 去勾选远程登录。\n\n3. 配置HDFS，包括core-site.xml文件和hdfs-site.xml文件。前者配置用于存储HDFS的临时文件目录和hdsf访问端口，后者确定复制份数\n\n   ```xml\n   <!-- core-site.xml -->\n   <configuration>\n       <property>\n          <name>hadoop.tmp.dir</name>\n   \t   <value>hadoop/libexec/tmp/hadoop- ${user.name}</value>\n           <!--如果无此目录则去mkdir一个-->\n          <description>A base for other temporary directories.</description>\n       </property> \n       <property>\n           <name>fs.default.name</name>\n           <value>hdfs://localhost:9000</value>\n       </property>\n   </configuration>\n   ```\n\n   ```xml\n   <!-- hdfs-site.xml -->\n   <configuration>\n       <property>\n           <name>dfs.replication</name>\n           <value>1</value>\n       </property>\n   </configuration>\n   ```\n\n4. 格式化HDFS\n\n   ```shell\n   hdfs namenode -format\n   ```\n\n   成功的话在tmp目录下可以看到dfs文件\n\n5. 启动各个节点\n\n   ```shell\n   hdfs --daemon start NameNode#启动namenode\n   hdfs --daemon start DataNode#启动datanode\n   hdfs --daemon start SecondaryNameNode#它是namenode的快照，保证了namenode的更新\n   jps#用来查看这些节点是否真的启动了\n   ```\n\n6. 在HDFS上创建文件夹及文件\n\n   ```shell\n   hdfs dfs -mkdir /demo#在hdfs上创建demo文件夹\n   hdfs dfs -ls /demo\n   hdfs dfs -put test.input /demo#将本地的test.input文件发到hdfs上\n   ```\n\n7. 配置Yarn的mapred-site.xml和yarn-site.xml\n\n   ```xml\n   <!-- mapred-sited.xml -->\n   <configuration>\n       <property>\n           <name>mapreduce.framework.name</name>\n           <value>yarn</value>\n       </property>\n       </property>\n          <name>mapreduce.application.classpath</name>\n          <value>share/hadoop/mapreduce/*</value>\n   \t</property>\n   \t<!-- 如果不加这个property，在后面运行mapreduce任务时会报找不到包 -->\n   </configuration>\n   ```\n\n   ```xml\n   <!-- yarn-site.xml -->\n   <configuration>\n   <!-- Site specific YARN configuration properties -->\n       <property>\n           <name>yarn.nodemanager.aux-services</name>\n           <value>mapreduce_shuffle</value>\n       </property>\n       <property>\n           <name>yarn.resourcemanager.hostname</name>\n           <value>localhost</value>\n       </property>\n   </configuration>\n   ```\n\n8. 启动resourcemanager和nodemanager\n\n   ```shell\n   yarn --daemon start nodemanager\n   yarn --daemon start resourcemanager\n   ```\n\n   yarn端口是8088，可以去localhost:8088看页面\n\n9. 运行mapreduce任务\n\n   ```shell\n   yarn jar hadoop-mapreduce-examples-3.1.0.jar wordcount /demo/test.input /demo-output/\n   ```\n\n   可以到对应文件里查看运行结果","tags":["big data"]},{"title":"Spark使用整理","url":"/2018/08/05/spark使用整理/","content":"\nRecently I read the book <\\<spark in action>>, the summation about this book will be wrote down here.\n\n### the notion of RDD(resilient distributed dataset)\n\nThe RDD is the fundamental abstraction in spark it represents a collection of elements that is\n\n- Immutable ( read-only )\n- Resilient ( fault-tolerant )\n- Distributed\n\nImmutable : allow xspark to provide important fault-tolerance guarantees in a straightforward manner.\n\nDistributed : machines are transparents to users, so working with RDDs is not much differents from working with a lists, maps and so on.\n\nResilient: whereas other systems facilitate fault-tolerance by replicating data to multiple machines, RDDs provide falut-tolerant by logging the transformations used to build dataset rather than itself. When fault happens, it just need to repair a subset of dataset.\n\n### Basic RDD Operation\n\nthere are two types of operations\n\n- Transformations\n- actions\n\nTransformations : like *filter* and *map*, perform some useful data manipulation and it will produce a new RDD\n\nactions : like *count* and *foreach*, trigger a computations to return a result.","tags":["big data"]},{"title":"homebrew使用整理","url":"/2018/08/02/homebrew使用整理/","content":"\n现在新的mac基本都内置homebrew了吧，brew可以说是mac神器之一了。上手简单，但还是用法需要整理一下：\n\n###brew常用命令\n\n```shell\nbrew search 包名 #搜索包\nbrew info 包名#包信息\nbrew list #查看有哪些包\nbrew install 包名#安装包\nbrew uninstall 包名#删除包\n```\n\n### brew管理服务\n\nbrew还有个重要的任务就是管理服务，在我本机的：\n\n- Kafka\n- mysql\n- nginx\n- Redis\n- zookeeper\n\n都是用了brew进行管理，管理他们用\n\n```shell\nbrew services start 服务名#开启一个service\nbrew services stop 服务名#关闭一个service\n```\n\n每次开启一个服务，就会在～/Library/LaunchAgents里面增加一个plist文件，用来存储这个服务的一些版本信息，同时，本机所有其他服务可以通过\n\n```shell\nlaunchctl load *.plist #加载 \nlaunchctl unload *.plist #取消\nlaunchctl list#查看服务\n```\n\n来完成\n\n### brew其他命令\n\n```shell\nbrew link 包名\n```\n\n这里的link是指symbollink（有点类似于windows里的创建快捷方式）。以hadoop为例，在brew刚下载的hadoop只是存在/usr/local/Cellar目录下的，在全局环境下不能用hadoop命令。只有将其link到bin里（hadoop产生了27个symbolink），才能全局使用hadoop命令。在用brew install时会默认完成link的操作，除非出现意外。\n\n意外：在安装hadoop时出现了\n\n```shell\nError: The `brew link` step did not complete successfully\nThe formula built, but is not symlinked into /usr/local\nCould not symlink sbin/FederationStateStore\n/usr/local/sbin is not writable.\n```\n\n是因为我本机根本没有这个目录，同时权限也不够，所以我建了这个目录，然后用\n\n```shell\nsudo chown -r $(whoami) $(brew --prefix)/*\n```\n\n修改了对应权限，成功安装。这里引出了\n\n```shell\nbrew --prefix\n```\n\n这个是指brew存在的目录，其他brew操作都是在这个目录下搞的（例如cellar就是在这个目录下）。\n\n","tags":["mac"]},{"title":"hip-hop中一些slang积累","url":"/2018/07/27/hip-hop中一些slang积累/","content":"\n听了很久trap，很多都不太懂，之后不懂的就写在这里吧～\n\n###In my feelings - Drake （KIKI在b榜第一呆了一个月，我满耳朵都是kiki）\n\nHenny -> Hennessy：轩尼诗（酒）\n\nwraith：幽灵、幻影（劳斯莱斯品牌）\n\ncode to the safe：保险箱密码\n\nNeck work：类似的表达有give me some neck，指吹喇叭，等同于blow job或者sucking或者blowing of one's dick~\n\nNetflix and chill：一个internet meme，指约pao\n\nnet worth；资产净值（身价）\n\n\n\n### Alright - Kendrick Lamar\n\nMac-11：机械手枪的型号\n\nPussy&Benjamin：指代女人和金钱\n\nChevy -> chevrolet 雪弗兰\n\nGet reaping everything I sow：收获我所播种的（种豆得豆）\n\nMy karma：我的命运\n\nPreliminary hearing：法庭的初审\n\nfight my vice：和恶习斗争（vice除了副的还有恶习的意思）\n\nPopo：police，条子\n\nPreacher：牧师，传道人\n\nRegal：君主的，这里指别克君威车\n\nResentment：愤恨，不满\n\nSelf destruct：自我毁灭\n\nLucy -> Lucifer：是指撒旦，Satan, the devil（貌似只有lamar称lucifer为lucy～）\n\n\n\n### Bed - Nicki Minaj/Ariana Grande （沉迷黄歌，无法自拔～）\n\nwit'(with) your name on it：属于某人（不是真的指文字那种）\n\nSheet：床单\n\nCarter III：指Lil Wayne备受赞誉的专辑\n\nA Milli：Carter III专辑中的一首歌\n\nGOAT：greatest of all time 史上最佳\n\nturn down：拒绝（don't turn me down不要拒绝我）\n\nLingerie：内衣如图，不解释\n\n![img](/img/lingerie.png)\n\nblow it like a feather on you：像一片羽毛xx你（撩的不行啊～）\n\nStarting five：指nba那种首发五人，也可以用 Starting Line-up（首发阵容）\n\nThick skin：脸皮厚，不怕被骂～","tags":["hiphop"]},{"title":"pandas使用整理","url":"/2018/07/24/pandas使用整理/","content":"\n之前看了numpy，这两天看了pandas，也在这里整理一下。\n\n### 新建\n\n```python\nimport\tpandas as pd\n\ndata = {\n    'key1':['value11,value12'],\n    'key2':['value21','value22']   \n       }\ndf = pd.DataFrame(data)\n\ndf.index#描述dataframe的index的【开始/结束）和步长，注意这个是指df的index而非数据的id\ndf.columns#行名\ndf.dtypes#类型\ndf.size#数据总数\ndf.shape#数据形式（行，列）\ndf.ndim#维度\ndf.T#转置\n```\n\n### 从数据库中得到数据 && 将数据写入数据库\n\n```python\nfrom sqlalchemy import create_engine\n\n#连接mysql数据库\nengine = create_engine('mysql+pymysql://root:qh129512@127.0.0.1:3306/testdb?charset=utf8')\n\nsql = 'select * from person'\n\n#下面有三种获取数据的方式，返回的formlist就是一个dataframe\nformlist = pd.read_sql_query(sql,con=engine)#参数为sql语句\nformlist = pd.read_sql_table(table,con=engine)#参数为表名\nformlist = pd.read_sql(sql,con=engine)#参数可以是sql语句，也可以表名\n\n```\n\n下面是将数据写入数据库\n\n```python\ndf = pd.DataFrame(data,columns=['name,birthday'])#这里的columns就是指dataframe里有哪些comlumn，这里没有的df里也不会有，如果不添加columns这个参数就默认data中全部数据\n\ndf.to_sql(name='person',con=engine, if_exists='append',index=False)\n#if_exists:有三种fail->表存在就不写入；replace->表存在就删掉原来的表重新创建；append->在原表基础上追加数据。默认为fail\n#index：决定是否将行索引作为数据传入数据库，注意：如果数据库没有专门来存这个的就false，因为表里没有这里用true会有问题\n```\n\n### 使用dataframe\n\n直接取某个、某些数据\n\n```python\ndf['name'][0]\ndf.name[:5]\ndf[['name','birthday']][:2]\ndf.head()\n```\n\n取数据的切片\n\n```python\ndf.loc[:5,'name']#前一个参数为行索引，后一个是列索引名称\ndf.iloc[0:2,1]\n\n#有条件的切片\ndf.loc[(df['name'] == 'max'),:]#取name为max的切片\n\ndf.iloc[(df['name'] == 'max').values,:]\n```\n\n这里有两个函数，loc和iloc，区别有\n\n- loc第一个参数可以为series，例如我传入一个条件，其实相当于一个Series([True,True,False...])这种形式；iloc不可以穿series，但能传一个array，所以可以通过.values的形式传入条件\n- 行索引时loc是前后闭区间，而iloc是前闭后开区间（python中这种更常见）\n\n删除数据\n\n```python\ndf.drop(labels=rang(9,10),axis=0,inplace=True)\n'''\nlabels接收string、array，表示删除行/列的标签\naxis接收0、1，表示操作轴向，0为横，1为纵\ninplace接收boolean，代表操作是否对原数据生效\n'''\n```\n\n对dataframe中数据修改\n\n```python\n#直接声明就可以添加一列，如\ndf['prefix_name'] = 'MAC_' + df['name']\n\n#修改某个数据可以将其找出来然后直接赋值\ndf.loc[(df.name == 'max'),'name'] = 'maxhh'\n```\n\n随机数的使用\n\n```python\nseries = pd.Series(np.random.randint(high=10000,low=1000,size=8))#产生一个随机series\n\n\ndf.loc[:,'net-worth'] = series#将series值付给df\n```\n\ndataframe算数统计\n\n```python\ndf['net-worth'].mean()#平均值\nnp.mean(df['net-worth'])#另一种计算平均值的方法\ndf['net-worth'].min()#最小值\ndf['net-worth'].describe()#描述，包含很多数据可以用！\nnullNum = df.shape[0] - df['name'].count()#统计有多少空值\n```\n\n###  Category类型的使用\n\n可以将某一列转化成category类型，这样相当于做了一次分类处理，这样得到的描述性信息会很多\n\n```python\ndf['name'] = df['name'].astype('category')#注意这里是赋值而不是调用\n\ndf['name'].describe()\n```\n\n### 时间类型\n\npandas里有很多时间类型，不同类型用处不同。如timestamp主要用来记录时间，而timedelta用来做时间运算\n\n```python\n#有很多方法创建或转化出一个timestamp\ntoday_date = pd.to_datetime('2018-8/5')#随意的一个string都可以识别\n```\n\n\n\n注意一个概念，从数据库一个datatime拿出来的时间如果调用dtype的话发现是('<M8[ns]')类似的类型，展开来说：\n\n这个是属于机器的比较特别的类型:\n小端机器的类型：datatime[ns] == <M8[ns]\n大端机器的类型：datatime[ns] == >M8[ns]\n这个可以通过\n\n```python\nnp.dtype('datetime64[ns]') == np.dtype('<M8[ns]')\n#out: True\n```\n\n证明。\n所以当数据库中取出就是这种类型时，不需要再进行处理，但如果取出是个object，则用pd.to_datatime处理\n\n```python\ndf['birthday'] = pd.to_datatime(df['birthday'])\n```\n\n这里要熟悉的操作有：\n\n```python\nuser_birthday = [i.year for i in df['birthday']]#返回所有年份的list\n\nbirthday = df[df['birthday']<=pd.datetime(1991,1,1)]#按条件取时间时，一定要那拿timestamp与对应的pdf.datetime()相比\n```\n\n还有时间做加减法也是支持的,timestamp可以加一个tmiedelta来做时间的计算。注意timedelta的参数为weeks,days,hours以及更小的时间\n直接用+，-符号就可以做计算了。相反的，想算两个时间差，只要用两个datetime做减法，即可得到timedelta类型的时间差距\n\n```python\naweek_after = date + pd.Timedelta(weeks = 1)\n\ntoday_date = pd.to_datetime('2018-8-5')\n\ntime_delta = aweek_after - today_date#两个timestamp做减法，得到了一个timedelta类型的时间差\n```\n\n### 分组和聚合\n\n分组顾名思义，就是将数据按照一定条件进行分组，得到数据整体情况\n\n```python\ndata_group = df.groupby(by='birthday')\ndata_group.count()\n```\n\n这里的count()得到如下表结果\n\n|            | id   | name | net-worth |\n| ---------- | ---- | ---- | --------- |\n| birthday   |      |      |           |\n| 1987-01-10 | 1    | 1    | 1         |\n| 1989-03-10 | 6    | 0    | 4         |\n| 1993-08-05 | 1    | 1    | 1         |\n| 1995-12-12 | 1    | 1    | 1         |\n| 1996-10-12 | 1    | 1    | 1         |\n\n分组的直接结果并不能直接看，因为它返回的只是一个地址，但可以方便的查分组后的一些属性，如：count，head，max等等\n\n聚合，就是将一组数据做aggretate，聚合的函数自己定\n\n```python\ndf['net-worth'].agg([np.sum,np.mean])#针对net-worth数据计算两种agg，分别以sum和mean\n\ndf.agg({'net-worth':[np.sum,sp.mean]})#针对不同数据要进行不同的agg，可以用key-value的方式\n```\n\nagg函数的参数是计算函数，这个函数可以用numpy中一些简单的统计函数，如果复杂也可以自定义，如：\n\n```python\ndef Trinum(data):\n    return data.sum()*2\n    \ndf['net-worth'].agg(Trinum)#agg函数的参数可以是函数，该函数将data传入做处理，然后返回即可\n#稍微注意下sum后面的括号，没有括号是函数，有括号的是执行，但必须声明参数\n\ndf['net-worth'].agg(lambda x:x*2)#agg的参数可以是lambda函数\n```\n\n这里的一个例子：\n\n```python\ndf.groupby(by='birthday').agg(np.mean)#完全等价于data_group.mean()，都是求每组的平均值\n```\n\n当然，apply函数、transform函数和agg函数也大部分相同，但apply方法不能用key-value类型来特定的处理，transform方法只有一个参数function\n\n### 创建透视表\n\n可以通过pandas创建透视表\n\n```python\npd.pivot_table(df[['id','name','net-worth']],index=['id'],columns='name',fill_value=0)#不显示无index的值\n```\n\n得到\n\n|      | net-worth |      |        |      |\n| ---- | --------- | ---- | ------ | ---- |\n| name | hape      | john | johnny | max  |\n| id   |           |      |        |      |\n| 1    | 0         | 0    | 0      | 6355 |\n| 2    | 0         | 6567 | 0      | 0    |\n| 4    | 0         | 0    | 8491   | 0    |\n| 8    | 6872      | 0    | 0      | 0    |\n\n但这不是pandas的重点，略～","tags":["data science"]},{"title":"看了一点zookeeper的心得","url":"/2018/07/20/zookeeper整理/","content":"\n###Fast Paxos算法：\n\n这个还没看，先留个坑把。\n\n###文件结构：\n\nzookeeper的文件结构大概是这个样子的：\n\n!å¾ 1 Zookeeper æ°æ®ç\"æ](https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/image001.gif)\n\n### znode（data node）：\n\n1. 其中的每个子目录就是一个znode，他们也可以有子的znode（临时znode除外）。\n2. 这些znode必须是绝对路径，不允许相对路径。\n3. 每个znode都维护一个stat structure（linux系统文件的结构），其中包括版本号，acl改变等等。每次更新数据会让版本号自增。\n4. 每个znode可以设置一个watches，当watch触发后，zookeeper将发给client一个提醒。\n5. 在znode中数据读写是原子性的。每个znode都有一个ACL（access control list）来控制其读写权限。zookeeper不是用来做数据库的，其中存储的数据可能都是几kb的配置/状态信息。大块数据都存在hdfs中。\n6. Ephemeral node就是临时节点，每次会话结束这些节点都会清空，不允许有子节点。\n\n###Zookeeper Session：\n\n![state_dia](/img/state_dia.jpg)\n\n###一致性的保证：\n\nzookeeper是一个非常高效、可扩展的服务。其一致性靠以下几点保证：\n\n1. 顺序的一致性。一个client的更新会依序发送给其他。\n2. 原子性。更新是原子操作，只有成功和失败，没有中间过程。\n3. 服务器会看到完全相同的服务，不论其连接了哪个服务器。\n4. 可靠性。一旦一个更新被部署，他会一直存在，直到被另一个更新覆盖。\n5. 及时性。client对系统的观察在一个时间段内将是最新的。也即系统的改变在这个时间段内client是看见的，或者可以探测到它失败。","tags":["big data"]},{"title":"numpy使用整理","url":"/2018/07/18/numpy使用整理/","content":"\n最近看了一些numpy的基础，在这里整理一下。\n\n### 创建：\n\n```python\nimport numpy as np\n\n#数组\narray1 = np.array([[1,2,3],[4,5,6]])\nnp.eye(3)#单位多维数组\nnp.diag([1,2,3,4])#对角多维数组\nnp.arange(1,4)#[1,2,3]数组\n\n#矩阵\nnp.mat(\"1 2 3;4 5 6;7 8 9\")#矩阵运算与多维数组运算结果不同，所以要用mat建矩阵，用分号隔开数据\nmatrix = np.mat(array1)#可以用多维数组初始化矩阵\nmatrix1 = np.bmat(\"array1 array2;array1 array2\")#创建分块矩阵\n```\n\n### 随机数：\n\n```python\nnp.random.random(100)#完全随机\nnp.random.rand(5,5)#5*5均匀分布\nnp.random.randn(5,5)#5*5正态分布\nnp.random.randint(2,50,size=(2,3),dtype='l')#大于等于2小于50的2*3的int64型整数\n```\n\n### 变换数组形态：\n\n```python\narr.reshape(3,3)#转变成3*3的数组，但要求原数组必须9个元素，否则不能reshape\nnp.hstack((arr1,arr2))#横向组合\nnp.vsplit(arr4,3)#横向切割，即把横向由1列的变成3列（相当于横着切）\n```\n\n### 文件存储与读取\n\n```python\n#二进制存储与读取\n#存储\nfile = \"./temp/save_arr.npy\"\nfilez = \"./temp/save_arr.npz\"\nnp.save(file,arr)#用save存，文件扩展名.npy，只能存一个数组\nnp.savez(filez,arr1[,arr2...])#用savez存，文件扩展名.npz，可以存多个数组。注意：不按照要求扩展名，则系统自己添加对应扩展名；二进制存储的数组打开文件看不到真实数据\n#读取\nloaded_data = np.load(file)#存储可以省略扩展名，读取一定不可以\nloaded_dataz = np.load(filez)\nloaded_dataz[\"arr_0\"]#对于多个文件读取，这种方式可以得到单独数组\n```\n\n```python\t\n#文件存储与读取\n#存储\nnp.savetxt(fname,x,fmt='%d',delimiter=',',newline='\\n',header='',footer='',comments='# ')#x为要存的数组，fmt='%d'表示整数方式存，delimiter表示存储时的分隔符，存储和读取时默认为空格\n#读取\nloaded_data = np.loadtxt(fname,delimiter=\",\")#一定也要带上啊delimiter且与文件中的分隔符一致\n```\n\n### 利用numpy做简单的统计分析\n\n```python\nnp.random.seed(10)#种子是伪随机数的开头，相同种子对应随机数都相同，一般种子会设为当前时间，确保得到真随机数（numpy默认也是这样）\narr = np.random.randint(1,10,size=10)\n#排序\narr.sort()#这个会直接将arr排序\narr.sort(axis=0)#二维数组的sort参数axis可以为0、1，分别对应数组纵向和横向的排序\n#去重\nnp.unique(arr)\n#重复\nnp.tile(arr,3)#arr是重复哪个，3是重复次数\nnp.repeat(arr,3,axis=0)#axis是重复的方向（tile没有这个参数）\n#注意：tile是对数组进行重复，repeat则是对每一个数组的每一个元素进行重复，打破了原来的数组。\n#常用统计函数\nnp.sum(arr)#求和\narr.sum(axis=0)#沿纵轴求和\nnp.mean(arr)#计算数组均值\narr.mean(axis = 0)#沿着纵轴计算数组均值\nnp.std(arr)#计算标准差\nnp.var(arr)#计算方差\nnp.min(arr)#计算最小值\nnp.max(arr)#计算最大值\n```\n\n\n\n","tags":["data science"]},{"title":"最近在看的东西","url":"/2018/07/16/20180716日记/","content":"\n最近都没有写博客，想更一篇了。\n\n前几天在刷算法，这几天看了一些springboot的东西。之前对spring了解的也比较多，而且这次看的也比较浅，就这样吧。\n\n感觉很慌。马上要找工作了，我却还没开始工作...\n\nspring-boot用的时候再看吧，\n\n算法还是要接着刷，\n\n接下来就做我的python了！\n","tags":["life"]},{"title":"anaconda基础整理","url":"/2018/07/04/anaconda基础整理/","content":"\n这是一个python环境、包管理工具，这玩意很厉害。\n\n### 插一个其他东西：\n\n在搞conda环境变量的时候在.zshrc里没有注意语句的顺序，变量使用在前，声明在后，导致path里没有这个。。。。。。以后要注意了！\n\n###使用原因：\n\n1. 和以前用的virtualenv有点像，可以创建一个独立的python环境，python版本，包都是独立于外部的。\n2. 自带很多数据科学的包，省的下。\n3. 可以将环境与远程同步，也可以clone别人的环境，开发效率高。\n4. 可以与pycharm等工具结合，通用性强。\n5. Anaconda navigator是一个桌面应用，使用非常简单。\n\n###常用到的操作：\n\n1. 在命令行可以用conda来操作一些东西：\n\n```bash\nconda create -n <env-name> <package-name>#创建conda环境\n\nconda remove -n <env-name>#删除conda环境\n\nconda env list#查看所有环境，其中带*的为当前环境，在当前环境下，用的python版本、包等都是anaconda的，而不是本机环境\n\nsource activate <env-name>#激活某个环境，之后zsh前面会加上这个环境的名称\n\nsource deactivate#退出某个环境\n\nconda install <package-name>[=versionInfo]#在当前环境下安装包，可以选定版本\n\nconda install -n <env-name> <package-name>#在特定环境中\n\nconda list#列出当前环境所有的包\n\nconda search <package-name>#查找某个包（模糊匹配）\n```\n\n###conda和pycharm的结合：\n\npycharm可以直接用conda的environment来做，只要在选择interpreter的时候选conda环境对应的那个即可。","tags":["data science"]},{"title":"mac&linux命令整理","url":"/2018/07/04/mac&linux命令整理/","content":"今天看了一些oh-my-zsh的东西，感觉还是要整理在博客中，不然太容易忘掉了。\n\n### 命令：\n\n以后一些好用但是不熟悉的命令就都放在这里了，方便回忆：\n\n**jq** ： 命令行处理json的命令，，支持管道\n\n用法：\n\n针对一个json，直接 \n\n```shell\nhead -n 1 xxx.json | jq '.'\n```\n\n就可以获得format之后的形式，如果想获得json某个key，只要\n\n```shell\nhead -n 1 xxx.json | jq '.key'\n```\n\n即可获得\n\n**wc** ： 用于统计指定文件的字节数、字数、行数\n\n### 插件：\n\nzsh自带很多插件，可以在.zshrc的plugin里写入，就可以用这些插件了，我用的插件包括：\n\n- z。可以直接跳转。它记录（统计）了一些常用的跳转，只要z+destination就可以\n- extract。可以直接解压，忽略tar后各种参数。与unzip类似。\n- zsh-autosuggestions。这个神器，之前输入的命令可以再提示出来，很方便用。\n- Web-search。可以在命令行直接用 google+要查的内容 即可打开搜索页面。\n\n### 命令行快捷键：\n\n- ctrl+q。可以直接删除整行命令。\n- ctrl+w。可以删除每一分段的命令。\n- ctrl+e。直接跳到命令最后。\n- ctrl+a。直接跳到命令最前面。\n- command+d 在iterm中分屏\n- comand+[ or ] 在iterm的分屏中切换\n\n### 文件：\n\n**/etc/motd** : 改命令行打开的提示语\n\n### 遇到的问题：\n\nssh-key生成忘记有什么问题了。。。整理不及时呐～\n\nscala2.11和autosuggestions配色问题有冲突，导致每次scala都会报错\n\n### Dockerfile的基础应用\n\nDockerfile里有个from，就是指从哪个images拿过来的，可以是本地的，所以增量修改images就是新建一个dockerfile然后from原来的images。再加上自己的RUN，最后执行docker build .就可以～","tags":["mac"]},{"title":"Cryptocurrency Technologies","url":"/2018/04/30/Cryptocurrency Technologies/","content":"\n# Cryptocurrency Technologies\n\nCoursera course from Princeton University, here are some information and notes from the course.\n\n## Cryptographic Hash Functions\n\n### basic term:\n\n- takes any string of any size as input\n- fixed-size output (We'll use 256 bits )\n- efficiently computable\n\n### Security properties:\n\n- collision-free\n- hiding\n- puzzle-friendly \n\n### collision-free\n#### Definition\n\nNobody can find x and y such that:\n>  **x!=y and H(x) = H(y)**  \n\nSo that is what we call collision-free.\n\nActually **collision do exist. But it merely be found** ——that is guaranteed the work.\n\n#### Application\nIf we know H(x) = H(y), it's safe to assume that x = y. Hash function provide us with a efficient way to recognize the same things. The hash is small, which has only 256 bits, while the whole things might be really big.\n\n### Hiding\n#### Definition\nWe want something like this:\n> Given H(x), it is infeasible to find x.\n\nhigh min-entropy : the distribution is \"very spread out\". If r is chosen from a probability distribution that has high min-entropy, then given H(r | x), it is infeasible to find x.\n\n#### Application : Commitment\n\nCommit to a value, reveal it later.\n> **com = H(key | msg)**\n##### Hidding:\nGiven H(key | msg), infeasible to find msg.\n##### Binding:\nInfeasible to find msg != msg' such that H(key | msg) == H(key | msg')\n\n### Puzzle-friendly\n\nfor every possible output value y, if k is chosen from a distribution with high min-entropy, then it is infeasible to find x such that H(k | x) = y.\n\n#### Application: Search puzzle\n\n1. given a \"puzzle ID\" id (from high min-entropy distrib), and a target set Y;\n2. try to find a \"solution\" x such that H(id | x) = y.\n\n\n\n## Add:\n\ncommon hash function in shell\n\n```shell\n#md5\nmd5 [-pqrtx] [-s string] [files ...]\n#more ways\necho \"max\"|md5\nmd5 <<< \"max\"\n\n#sha-1 && sha-256\nshasum -a 1 -t test\nshasum -a 256 <<< \"max\"\n\n#encode and decode function -> base64\nbase64 -D <<< \"max\" #decode\nbase64 <<< \"max\" #encode\n```\n\n\n\nbitcoin use **sha256** as its hash function\n\n\n\n## Topic from Prof.Yuen \n\n### some keywords\n\nZero-knoledge proff approach\n\nLinkable ring signature\n\nLighting network\n\nIOTA\n\nByteball\n\nHdera/Hashgraph\n\nmonero's limitation\n\nRingCT 2.0\n\nBulletRingCT","tags":["block chain"]}]